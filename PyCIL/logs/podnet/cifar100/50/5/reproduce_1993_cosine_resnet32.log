2023-10-17 19:18:16,101 [trainer.py] => config: ./exps/podnet.json
2023-10-17 19:18:16,106 [trainer.py] => prefix: reproduce
2023-10-17 19:18:16,106 [trainer.py] => dataset: cifar100
2023-10-17 19:18:16,106 [trainer.py] => memory_size: 2000
2023-10-17 19:18:16,106 [trainer.py] => memory_per_class: 20
2023-10-17 19:18:16,106 [trainer.py] => fixed_memory: True
2023-10-17 19:18:16,106 [trainer.py] => shuffle: True
2023-10-17 19:18:16,106 [trainer.py] => init_cls: 50
2023-10-17 19:18:16,106 [trainer.py] => increment: 5
2023-10-17 19:18:16,107 [trainer.py] => model_name: podnet
2023-10-17 19:18:16,107 [trainer.py] => convnet_type: cosine_resnet32
2023-10-17 19:18:16,107 [trainer.py] => device: [device(type='cuda', index=0)]
2023-10-17 19:18:16,107 [trainer.py] => seed: 1993
2023-10-17 19:18:18,370 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-10-17 19:18:24,831 [trainer.py] => All params: 466256
2023-10-17 19:18:24,831 [trainer.py] => Trainable params: 466256
2023-10-17 19:18:24,832 [podnet.py] => Learning on 0-50
2023-10-17 19:18:24,889 [podnet.py] => Adaptive factor: 0
2023-10-17 19:18:34,195 [podnet.py] => Task 0, Epoch 1/160 (LR 0.09999) => LSC_loss 3.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 5.80, Test_acc 5.94
2023-10-17 19:18:39,671 [podnet.py] => Task 0, Epoch 2/160 (LR 0.09996) => LSC_loss 3.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 14.38, Test_acc 16.40
2023-10-17 19:18:45,097 [podnet.py] => Task 0, Epoch 3/160 (LR 0.09991) => LSC_loss 2.95, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 21.41, Test_acc 24.70
2023-10-17 19:18:50,639 [podnet.py] => Task 0, Epoch 4/160 (LR 0.09985) => LSC_loss 2.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 27.63, Test_acc 24.78
2023-10-17 19:18:56,195 [podnet.py] => Task 0, Epoch 5/160 (LR 0.09976) => LSC_loss 2.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 33.52, Test_acc 26.42
2023-10-17 19:19:01,806 [podnet.py] => Task 0, Epoch 6/160 (LR 0.09965) => LSC_loss 2.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 39.37, Test_acc 40.18
2023-10-17 19:19:07,392 [podnet.py] => Task 0, Epoch 7/160 (LR 0.09953) => LSC_loss 2.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 44.68, Test_acc 40.06
2023-10-17 19:19:12,994 [podnet.py] => Task 0, Epoch 8/160 (LR 0.09938) => LSC_loss 1.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 47.98, Test_acc 45.92
2023-10-17 19:19:18,512 [podnet.py] => Task 0, Epoch 9/160 (LR 0.09922) => LSC_loss 1.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 51.09, Test_acc 43.40
2023-10-17 19:19:24,067 [podnet.py] => Task 0, Epoch 10/160 (LR 0.09904) => LSC_loss 1.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 53.87, Test_acc 50.50
2023-10-17 19:19:29,666 [podnet.py] => Task 0, Epoch 11/160 (LR 0.09884) => LSC_loss 1.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 55.47, Test_acc 50.70
2023-10-17 19:19:35,058 [podnet.py] => Task 0, Epoch 12/160 (LR 0.09862) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 57.16, Test_acc 48.34
2023-10-17 19:19:40,292 [podnet.py] => Task 0, Epoch 13/160 (LR 0.09838) => LSC_loss 1.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 58.43, Test_acc 52.92
2023-10-17 19:19:45,649 [podnet.py] => Task 0, Epoch 14/160 (LR 0.09812) => LSC_loss 1.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 59.72, Test_acc 54.48
2023-10-17 19:19:51,096 [podnet.py] => Task 0, Epoch 15/160 (LR 0.09785) => LSC_loss 1.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.96, Test_acc 51.06
2023-10-17 19:19:56,475 [podnet.py] => Task 0, Epoch 16/160 (LR 0.09755) => LSC_loss 1.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 61.79, Test_acc 53.44
2023-10-17 19:20:01,925 [podnet.py] => Task 0, Epoch 17/160 (LR 0.09724) => LSC_loss 1.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.04, Test_acc 54.56
2023-10-17 19:20:07,283 [podnet.py] => Task 0, Epoch 18/160 (LR 0.09691) => LSC_loss 1.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.90, Test_acc 55.50
2023-10-17 19:20:12,698 [podnet.py] => Task 0, Epoch 19/160 (LR 0.09656) => LSC_loss 1.29, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 64.37, Test_acc 55.64
2023-10-17 19:20:18,066 [podnet.py] => Task 0, Epoch 20/160 (LR 0.09619) => LSC_loss 1.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.22, Test_acc 55.50
2023-10-17 19:20:23,469 [podnet.py] => Task 0, Epoch 21/160 (LR 0.09581) => LSC_loss 1.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.21, Test_acc 58.10
2023-10-17 19:20:28,856 [podnet.py] => Task 0, Epoch 22/160 (LR 0.09541) => LSC_loss 1.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.86, Test_acc 56.44
2023-10-17 19:20:34,215 [podnet.py] => Task 0, Epoch 23/160 (LR 0.09499) => LSC_loss 1.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 66.24, Test_acc 52.04
2023-10-17 19:20:39,624 [podnet.py] => Task 0, Epoch 24/160 (LR 0.09455) => LSC_loss 1.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.23, Test_acc 53.84
2023-10-17 19:20:45,031 [podnet.py] => Task 0, Epoch 25/160 (LR 0.09410) => LSC_loss 1.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.48, Test_acc 57.28
2023-10-17 19:20:50,399 [podnet.py] => Task 0, Epoch 26/160 (LR 0.09362) => LSC_loss 1.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.84, Test_acc 56.94
2023-10-17 19:20:55,789 [podnet.py] => Task 0, Epoch 27/160 (LR 0.09314) => LSC_loss 1.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.98, Test_acc 50.48
2023-10-17 19:21:01,177 [podnet.py] => Task 0, Epoch 28/160 (LR 0.09263) => LSC_loss 1.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 68.64, Test_acc 59.20
2023-10-17 19:21:06,611 [podnet.py] => Task 0, Epoch 29/160 (LR 0.09211) => LSC_loss 1.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.24, Test_acc 61.20
2023-10-17 19:21:11,988 [podnet.py] => Task 0, Epoch 30/160 (LR 0.09157) => LSC_loss 1.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.00, Test_acc 57.22
2023-10-17 19:21:17,348 [podnet.py] => Task 0, Epoch 31/160 (LR 0.09102) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.35, Test_acc 60.94
2023-10-17 19:21:22,716 [podnet.py] => Task 0, Epoch 32/160 (LR 0.09045) => LSC_loss 1.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.85, Test_acc 59.24
2023-10-17 19:21:28,044 [podnet.py] => Task 0, Epoch 33/160 (LR 0.08987) => LSC_loss 1.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.77, Test_acc 57.96
2023-10-17 19:21:33,429 [podnet.py] => Task 0, Epoch 34/160 (LR 0.08927) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.48, Test_acc 60.34
2023-10-17 19:21:38,862 [podnet.py] => Task 0, Epoch 35/160 (LR 0.08865) => LSC_loss 1.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.84, Test_acc 54.54
2023-10-17 19:21:44,280 [podnet.py] => Task 0, Epoch 36/160 (LR 0.08802) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.92, Test_acc 57.32
2023-10-17 19:21:49,686 [podnet.py] => Task 0, Epoch 37/160 (LR 0.08738) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.68, Test_acc 60.76
2023-10-17 19:21:55,160 [podnet.py] => Task 0, Epoch 38/160 (LR 0.08672) => LSC_loss 1.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.56, Test_acc 59.00
2023-10-17 19:22:00,556 [podnet.py] => Task 0, Epoch 39/160 (LR 0.08604) => LSC_loss 1.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.10, Test_acc 63.64
2023-10-17 19:22:05,996 [podnet.py] => Task 0, Epoch 40/160 (LR 0.08536) => LSC_loss 1.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.57, Test_acc 61.18
2023-10-17 19:22:11,393 [podnet.py] => Task 0, Epoch 41/160 (LR 0.08465) => LSC_loss 1.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.65, Test_acc 63.02
2023-10-17 19:22:16,745 [podnet.py] => Task 0, Epoch 42/160 (LR 0.08394) => LSC_loss 1.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.81, Test_acc 63.68
2023-10-17 19:22:22,123 [podnet.py] => Task 0, Epoch 43/160 (LR 0.08321) => LSC_loss 1.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.06, Test_acc 60.52
2023-10-17 19:22:27,505 [podnet.py] => Task 0, Epoch 44/160 (LR 0.08247) => LSC_loss 0.98, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.61, Test_acc 64.00
2023-10-17 19:22:32,933 [podnet.py] => Task 0, Epoch 45/160 (LR 0.08172) => LSC_loss 0.98, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.84, Test_acc 57.18
2023-10-17 19:22:38,309 [podnet.py] => Task 0, Epoch 46/160 (LR 0.08095) => LSC_loss 0.96, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.14, Test_acc 61.06
2023-10-17 19:22:43,696 [podnet.py] => Task 0, Epoch 47/160 (LR 0.08018) => LSC_loss 0.97, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.99, Test_acc 62.66
2023-10-17 19:22:49,069 [podnet.py] => Task 0, Epoch 48/160 (LR 0.07939) => LSC_loss 0.96, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.11, Test_acc 60.18
2023-10-17 19:22:54,444 [podnet.py] => Task 0, Epoch 49/160 (LR 0.07859) => LSC_loss 0.95, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.14, Test_acc 64.28
2023-10-17 19:22:59,804 [podnet.py] => Task 0, Epoch 50/160 (LR 0.07778) => LSC_loss 0.94, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.23, Test_acc 65.30
2023-10-17 19:23:05,152 [podnet.py] => Task 0, Epoch 51/160 (LR 0.07696) => LSC_loss 0.94, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.60, Test_acc 61.48
2023-10-17 19:23:10,535 [podnet.py] => Task 0, Epoch 52/160 (LR 0.07612) => LSC_loss 0.92, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.31, Test_acc 62.56
2023-10-17 19:23:15,861 [podnet.py] => Task 0, Epoch 53/160 (LR 0.07528) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.40, Test_acc 61.44
2023-10-17 19:23:21,227 [podnet.py] => Task 0, Epoch 54/160 (LR 0.07443) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.24, Test_acc 63.26
2023-10-17 19:23:26,565 [podnet.py] => Task 0, Epoch 55/160 (LR 0.07357) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.45, Test_acc 65.12
2023-10-17 19:23:31,922 [podnet.py] => Task 0, Epoch 56/160 (LR 0.07270) => LSC_loss 0.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.14, Test_acc 60.20
2023-10-17 19:23:37,285 [podnet.py] => Task 0, Epoch 57/160 (LR 0.07182) => LSC_loss 0.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.02, Test_acc 62.46
2023-10-17 19:23:42,695 [podnet.py] => Task 0, Epoch 58/160 (LR 0.07093) => LSC_loss 0.88, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.20, Test_acc 59.80
2023-10-17 19:23:48,141 [podnet.py] => Task 0, Epoch 59/160 (LR 0.07004) => LSC_loss 0.86, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.93, Test_acc 64.40
2023-10-17 19:23:53,519 [podnet.py] => Task 0, Epoch 60/160 (LR 0.06913) => LSC_loss 0.87, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.52, Test_acc 65.06
2023-10-17 19:23:58,923 [podnet.py] => Task 0, Epoch 61/160 (LR 0.06822) => LSC_loss 0.86, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.01, Test_acc 62.32
2023-10-17 19:24:04,303 [podnet.py] => Task 0, Epoch 62/160 (LR 0.06731) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.80, Test_acc 63.02
2023-10-17 19:24:09,634 [podnet.py] => Task 0, Epoch 63/160 (LR 0.06638) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.33, Test_acc 58.36
2023-10-17 19:24:15,027 [podnet.py] => Task 0, Epoch 64/160 (LR 0.06545) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.60, Test_acc 60.96
2023-10-17 19:24:20,358 [podnet.py] => Task 0, Epoch 65/160 (LR 0.06451) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.85, Test_acc 63.92
2023-10-17 19:24:25,733 [podnet.py] => Task 0, Epoch 66/160 (LR 0.06357) => LSC_loss 0.83, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.75, Test_acc 67.14
2023-10-17 19:24:31,156 [podnet.py] => Task 0, Epoch 67/160 (LR 0.06262) => LSC_loss 0.81, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.34, Test_acc 63.74
2023-10-17 19:24:36,521 [podnet.py] => Task 0, Epoch 68/160 (LR 0.06167) => LSC_loss 0.81, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.00, Test_acc 62.90
2023-10-17 19:24:41,908 [podnet.py] => Task 0, Epoch 69/160 (LR 0.06072) => LSC_loss 0.80, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.66, Test_acc 65.02
2023-10-17 19:24:47,326 [podnet.py] => Task 0, Epoch 70/160 (LR 0.05975) => LSC_loss 0.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.88, Test_acc 61.90
2023-10-17 19:24:52,693 [podnet.py] => Task 0, Epoch 71/160 (LR 0.05879) => LSC_loss 0.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.01, Test_acc 60.48
2023-10-17 19:24:58,071 [podnet.py] => Task 0, Epoch 72/160 (LR 0.05782) => LSC_loss 0.77, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.58, Test_acc 63.62
2023-10-17 19:25:03,422 [podnet.py] => Task 0, Epoch 73/160 (LR 0.05685) => LSC_loss 0.76, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.43, Test_acc 64.56
2023-10-17 19:25:08,791 [podnet.py] => Task 0, Epoch 74/160 (LR 0.05588) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.78, Test_acc 64.86
2023-10-17 19:25:14,130 [podnet.py] => Task 0, Epoch 75/160 (LR 0.05490) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.96, Test_acc 63.08
2023-10-17 19:25:19,462 [podnet.py] => Task 0, Epoch 76/160 (LR 0.05392) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.56, Test_acc 66.34
2023-10-17 19:25:24,903 [podnet.py] => Task 0, Epoch 77/160 (LR 0.05294) => LSC_loss 0.72, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.67, Test_acc 63.52
2023-10-17 19:25:30,243 [podnet.py] => Task 0, Epoch 78/160 (LR 0.05196) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.22, Test_acc 69.38
2023-10-17 19:25:35,649 [podnet.py] => Task 0, Epoch 79/160 (LR 0.05098) => LSC_loss 0.71, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.85, Test_acc 66.28
2023-10-17 19:25:40,971 [podnet.py] => Task 0, Epoch 80/160 (LR 0.05000) => LSC_loss 0.71, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.90, Test_acc 63.62
2023-10-17 19:25:46,328 [podnet.py] => Task 0, Epoch 81/160 (LR 0.04902) => LSC_loss 0.68, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.05, Test_acc 65.04
2023-10-17 19:25:51,726 [podnet.py] => Task 0, Epoch 82/160 (LR 0.04804) => LSC_loss 0.68, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.95, Test_acc 63.56
2023-10-17 19:25:57,120 [podnet.py] => Task 0, Epoch 83/160 (LR 0.04706) => LSC_loss 0.67, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.12, Test_acc 69.22
2023-10-17 19:26:02,548 [podnet.py] => Task 0, Epoch 84/160 (LR 0.04608) => LSC_loss 0.67, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.92, Test_acc 63.06
2023-10-17 19:26:07,881 [podnet.py] => Task 0, Epoch 85/160 (LR 0.04510) => LSC_loss 0.66, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.31, Test_acc 68.78
2023-10-17 19:26:13,284 [podnet.py] => Task 0, Epoch 86/160 (LR 0.04412) => LSC_loss 0.65, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.51, Test_acc 67.62
2023-10-17 19:26:18,708 [podnet.py] => Task 0, Epoch 87/160 (LR 0.04315) => LSC_loss 0.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.12, Test_acc 68.04
2023-10-17 19:26:24,132 [podnet.py] => Task 0, Epoch 88/160 (LR 0.04218) => LSC_loss 0.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.14, Test_acc 66.42
2023-10-17 19:26:29,532 [podnet.py] => Task 0, Epoch 89/160 (LR 0.04121) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.83, Test_acc 64.18
2023-10-17 19:26:34,852 [podnet.py] => Task 0, Epoch 90/160 (LR 0.04025) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.68, Test_acc 66.62
2023-10-17 19:26:40,247 [podnet.py] => Task 0, Epoch 91/160 (LR 0.03928) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.06, Test_acc 67.34
2023-10-17 19:26:45,564 [podnet.py] => Task 0, Epoch 92/160 (LR 0.03833) => LSC_loss 0.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.31, Test_acc 67.00
2023-10-17 19:26:50,922 [podnet.py] => Task 0, Epoch 93/160 (LR 0.03738) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.66, Test_acc 67.96
2023-10-17 19:26:56,262 [podnet.py] => Task 0, Epoch 94/160 (LR 0.03643) => LSC_loss 0.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.20, Test_acc 61.16
2023-10-17 19:27:01,652 [podnet.py] => Task 0, Epoch 95/160 (LR 0.03549) => LSC_loss 0.57, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.76, Test_acc 66.52
2023-10-17 19:27:07,003 [podnet.py] => Task 0, Epoch 96/160 (LR 0.03455) => LSC_loss 0.54, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.78, Test_acc 71.20
2023-10-17 19:27:12,326 [podnet.py] => Task 0, Epoch 97/160 (LR 0.03362) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.04, Test_acc 67.96
2023-10-17 19:27:17,702 [podnet.py] => Task 0, Epoch 98/160 (LR 0.03269) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.84, Test_acc 67.58
2023-10-17 19:27:23,118 [podnet.py] => Task 0, Epoch 99/160 (LR 0.03178) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.19, Test_acc 66.90
2023-10-17 19:27:28,498 [podnet.py] => Task 0, Epoch 100/160 (LR 0.03087) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.75, Test_acc 70.46
2023-10-17 19:27:33,920 [podnet.py] => Task 0, Epoch 101/160 (LR 0.02996) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.04, Test_acc 68.36
2023-10-17 19:27:39,310 [podnet.py] => Task 0, Epoch 102/160 (LR 0.02907) => LSC_loss 0.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.57, Test_acc 66.80
2023-10-17 19:27:44,708 [podnet.py] => Task 0, Epoch 103/160 (LR 0.02818) => LSC_loss 0.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.50, Test_acc 70.78
2023-10-17 19:27:50,046 [podnet.py] => Task 0, Epoch 104/160 (LR 0.02730) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.70, Test_acc 68.20
2023-10-17 19:27:55,364 [podnet.py] => Task 0, Epoch 105/160 (LR 0.02643) => LSC_loss 0.45, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.21, Test_acc 70.92
2023-10-17 19:28:00,694 [podnet.py] => Task 0, Epoch 106/160 (LR 0.02557) => LSC_loss 0.43, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.83, Test_acc 68.58
2023-10-17 19:28:05,986 [podnet.py] => Task 0, Epoch 107/160 (LR 0.02472) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.31, Test_acc 71.20
2023-10-17 19:28:11,283 [podnet.py] => Task 0, Epoch 108/160 (LR 0.02388) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.45, Test_acc 71.44
2023-10-17 19:28:16,528 [podnet.py] => Task 0, Epoch 109/160 (LR 0.02304) => LSC_loss 0.39, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.88, Test_acc 68.30
2023-10-17 19:28:21,772 [podnet.py] => Task 0, Epoch 110/160 (LR 0.02222) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.38, Test_acc 71.32
2023-10-17 19:28:27,067 [podnet.py] => Task 0, Epoch 111/160 (LR 0.02141) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.64, Test_acc 72.72
2023-10-17 19:28:32,553 [podnet.py] => Task 0, Epoch 112/160 (LR 0.02061) => LSC_loss 0.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.97, Test_acc 72.24
2023-10-17 19:28:37,918 [podnet.py] => Task 0, Epoch 113/160 (LR 0.01982) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.01, Test_acc 70.94
2023-10-17 19:28:43,411 [podnet.py] => Task 0, Epoch 114/160 (LR 0.01905) => LSC_loss 0.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.27, Test_acc 72.68
2023-10-17 19:28:48,840 [podnet.py] => Task 0, Epoch 115/160 (LR 0.01828) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.02, Test_acc 72.28
2023-10-17 19:28:54,273 [podnet.py] => Task 0, Epoch 116/160 (LR 0.01753) => LSC_loss 0.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.63, Test_acc 70.98
2023-10-17 19:28:59,681 [podnet.py] => Task 0, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.96, Test_acc 71.24
2023-10-17 19:29:05,121 [podnet.py] => Task 0, Epoch 118/160 (LR 0.01606) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.69, Test_acc 71.94
2023-10-17 19:29:10,585 [podnet.py] => Task 0, Epoch 119/160 (LR 0.01535) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.83, Test_acc 72.42
2023-10-17 19:29:16,087 [podnet.py] => Task 0, Epoch 120/160 (LR 0.01464) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.40, Test_acc 74.48
2023-10-17 19:29:21,540 [podnet.py] => Task 0, Epoch 121/160 (LR 0.01396) => LSC_loss 0.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.76, Test_acc 74.18
2023-10-17 19:29:27,028 [podnet.py] => Task 0, Epoch 122/160 (LR 0.01328) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.42, Test_acc 73.64
2023-10-17 19:29:32,387 [podnet.py] => Task 0, Epoch 123/160 (LR 0.01262) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.74, Test_acc 72.58
2023-10-17 19:29:37,658 [podnet.py] => Task 0, Epoch 124/160 (LR 0.01198) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.69, Test_acc 75.54
2023-10-17 19:29:43,006 [podnet.py] => Task 0, Epoch 125/160 (LR 0.01135) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.36, Test_acc 75.46
2023-10-17 19:29:48,373 [podnet.py] => Task 0, Epoch 126/160 (LR 0.01073) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.63, Test_acc 74.86
2023-10-17 19:29:53,756 [podnet.py] => Task 0, Epoch 127/160 (LR 0.01013) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.15, Test_acc 74.60
2023-10-17 19:29:59,105 [podnet.py] => Task 0, Epoch 128/160 (LR 0.00955) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.58, Test_acc 74.94
2023-10-17 19:30:04,410 [podnet.py] => Task 0, Epoch 129/160 (LR 0.00898) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.92, Test_acc 75.24
2023-10-17 19:30:09,839 [podnet.py] => Task 0, Epoch 130/160 (LR 0.00843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.27, Test_acc 76.26
2023-10-17 19:30:15,140 [podnet.py] => Task 0, Epoch 131/160 (LR 0.00789) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 75.82
2023-10-17 19:30:20,477 [podnet.py] => Task 0, Epoch 132/160 (LR 0.00737) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.66, Test_acc 75.96
2023-10-17 19:30:25,850 [podnet.py] => Task 0, Epoch 133/160 (LR 0.00686) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 76.62
2023-10-17 19:30:31,232 [podnet.py] => Task 0, Epoch 134/160 (LR 0.00638) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 76.36
2023-10-17 19:30:36,635 [podnet.py] => Task 0, Epoch 135/160 (LR 0.00590) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.21, Test_acc 77.00
2023-10-17 19:30:42,018 [podnet.py] => Task 0, Epoch 136/160 (LR 0.00545) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.40, Test_acc 76.66
2023-10-17 19:30:47,426 [podnet.py] => Task 0, Epoch 137/160 (LR 0.00501) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 77.08
2023-10-17 19:30:52,834 [podnet.py] => Task 0, Epoch 138/160 (LR 0.00459) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 77.14
2023-10-17 19:30:58,175 [podnet.py] => Task 0, Epoch 139/160 (LR 0.00419) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 77.02
2023-10-17 19:31:03,556 [podnet.py] => Task 0, Epoch 140/160 (LR 0.00381) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 77.10
2023-10-17 19:31:08,864 [podnet.py] => Task 0, Epoch 141/160 (LR 0.00344) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 77.54
2023-10-17 19:31:14,185 [podnet.py] => Task 0, Epoch 142/160 (LR 0.00309) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 77.72
2023-10-17 19:31:19,527 [podnet.py] => Task 0, Epoch 143/160 (LR 0.00276) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 77.38
2023-10-17 19:31:24,861 [podnet.py] => Task 0, Epoch 144/160 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 77.46
2023-10-17 19:31:30,204 [podnet.py] => Task 0, Epoch 145/160 (LR 0.00215) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 77.58
2023-10-17 19:31:35,627 [podnet.py] => Task 0, Epoch 146/160 (LR 0.00188) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 77.48
2023-10-17 19:31:40,969 [podnet.py] => Task 0, Epoch 147/160 (LR 0.00162) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 77.84
2023-10-17 19:31:46,293 [podnet.py] => Task 0, Epoch 148/160 (LR 0.00138) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 78.02
2023-10-17 19:31:51,661 [podnet.py] => Task 0, Epoch 149/160 (LR 0.00116) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 78.12
2023-10-17 19:31:57,038 [podnet.py] => Task 0, Epoch 150/160 (LR 0.00096) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.88
2023-10-17 19:32:02,384 [podnet.py] => Task 0, Epoch 151/160 (LR 0.00078) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 77.54
2023-10-17 19:32:07,802 [podnet.py] => Task 0, Epoch 152/160 (LR 0.00062) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 77.64
2023-10-17 19:32:13,166 [podnet.py] => Task 0, Epoch 153/160 (LR 0.00047) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 77.88
2023-10-17 19:32:18,457 [podnet.py] => Task 0, Epoch 154/160 (LR 0.00035) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.88
2023-10-17 19:32:23,816 [podnet.py] => Task 0, Epoch 155/160 (LR 0.00024) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 77.92
2023-10-17 19:32:29,168 [podnet.py] => Task 0, Epoch 156/160 (LR 0.00015) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 77.86
2023-10-17 19:32:34,537 [podnet.py] => Task 0, Epoch 157/160 (LR 0.00009) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 77.78
2023-10-17 19:32:39,896 [podnet.py] => Task 0, Epoch 158/160 (LR 0.00004) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 77.90
2023-10-17 19:32:45,182 [podnet.py] => Task 0, Epoch 159/160 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.98
2023-10-17 19:32:50,488 [podnet.py] => Task 0, Epoch 160/160 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 77.70
2023-10-17 19:32:50,489 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:33:31,929 [podnet.py] => Exemplar size: 1000
2023-10-17 19:33:31,929 [trainer.py] => CNN: {'total': 77.7, '00-09': 80.7, '10-19': 71.9, '20-29': 81.0, '30-39': 75.4, '40-49': 79.5, 'old': 0, 'new': 77.7}
2023-10-17 19:33:31,930 [trainer.py] => NME: {'total': 77.44, '00-09': 81.0, '10-19': 71.5, '20-29': 80.8, '30-39': 74.4, '40-49': 79.5, 'old': 0, 'new': 77.44}
2023-10-17 19:33:31,930 [trainer.py] => CNN top1 curve: [77.7]
2023-10-17 19:33:31,930 [trainer.py] => CNN top5 curve: [94.08]
2023-10-17 19:33:31,931 [trainer.py] => NME top1 curve: [77.44]
2023-10-17 19:33:31,931 [trainer.py] => NME top5 curve: [93.98]

2023-10-17 19:33:31,931 [trainer.py] => Average Accuracy (CNN): 77.7
2023-10-17 19:33:31,931 [trainer.py] => Average Accuracy (NME): 77.44
2023-10-17 19:33:31,932 [trainer.py] => All params: 498257
2023-10-17 19:33:31,933 [trainer.py] => Trainable params: 498257
2023-10-17 19:33:31,933 [podnet.py] => Learning on 50-55
2023-10-17 19:33:31,964 [podnet.py] => Adaptive factor: 3.3166247903554
2023-10-17 19:33:34,016 [podnet.py] => Task 1, Epoch 1/160 (LR 0.09999) => LSC_loss 1.86, Spatial_loss 6.80, Flat_loss 1.55, Train_acc 61.51, Test_acc 17.47
2023-10-17 19:33:35,927 [podnet.py] => Task 1, Epoch 2/160 (LR 0.09996) => LSC_loss 0.84, Spatial_loss 5.58, Flat_loss 1.22, Train_acc 78.40, Test_acc 42.35
2023-10-17 19:33:37,819 [podnet.py] => Task 1, Epoch 3/160 (LR 0.09991) => LSC_loss 0.60, Spatial_loss 4.92, Flat_loss 0.99, Train_acc 83.69, Test_acc 46.05
2023-10-17 19:33:39,723 [podnet.py] => Task 1, Epoch 4/160 (LR 0.09985) => LSC_loss 0.54, Spatial_loss 4.73, Flat_loss 0.90, Train_acc 85.46, Test_acc 47.93
2023-10-17 19:33:41,633 [podnet.py] => Task 1, Epoch 5/160 (LR 0.09976) => LSC_loss 0.46, Spatial_loss 4.48, Flat_loss 0.80, Train_acc 87.86, Test_acc 49.49
2023-10-17 19:33:43,564 [podnet.py] => Task 1, Epoch 6/160 (LR 0.09965) => LSC_loss 0.40, Spatial_loss 4.43, Flat_loss 0.76, Train_acc 88.86, Test_acc 52.69
2023-10-17 19:33:45,489 [podnet.py] => Task 1, Epoch 7/160 (LR 0.09953) => LSC_loss 0.39, Spatial_loss 4.23, Flat_loss 0.71, Train_acc 90.14, Test_acc 52.16
2023-10-17 19:33:47,402 [podnet.py] => Task 1, Epoch 8/160 (LR 0.09938) => LSC_loss 0.37, Spatial_loss 4.16, Flat_loss 0.71, Train_acc 89.77, Test_acc 49.11
2023-10-17 19:33:49,313 [podnet.py] => Task 1, Epoch 9/160 (LR 0.09922) => LSC_loss 0.34, Spatial_loss 4.12, Flat_loss 0.68, Train_acc 90.37, Test_acc 53.62
2023-10-17 19:33:51,213 [podnet.py] => Task 1, Epoch 10/160 (LR 0.09904) => LSC_loss 0.31, Spatial_loss 3.94, Flat_loss 0.62, Train_acc 92.03, Test_acc 58.73
2023-10-17 19:33:53,157 [podnet.py] => Task 1, Epoch 11/160 (LR 0.09884) => LSC_loss 0.29, Spatial_loss 3.83, Flat_loss 0.60, Train_acc 92.06, Test_acc 53.45
2023-10-17 19:33:55,111 [podnet.py] => Task 1, Epoch 12/160 (LR 0.09862) => LSC_loss 0.29, Spatial_loss 3.77, Flat_loss 0.59, Train_acc 92.69, Test_acc 54.09
2023-10-17 19:33:57,082 [podnet.py] => Task 1, Epoch 13/160 (LR 0.09838) => LSC_loss 0.27, Spatial_loss 3.73, Flat_loss 0.58, Train_acc 93.11, Test_acc 58.49
2023-10-17 19:33:59,042 [podnet.py] => Task 1, Epoch 14/160 (LR 0.09812) => LSC_loss 0.27, Spatial_loss 3.68, Flat_loss 0.57, Train_acc 92.94, Test_acc 61.35
2023-10-17 19:34:00,943 [podnet.py] => Task 1, Epoch 15/160 (LR 0.09785) => LSC_loss 0.25, Spatial_loss 3.59, Flat_loss 0.54, Train_acc 93.86, Test_acc 65.05
2023-10-17 19:34:02,910 [podnet.py] => Task 1, Epoch 16/160 (LR 0.09755) => LSC_loss 0.27, Spatial_loss 3.56, Flat_loss 0.54, Train_acc 93.26, Test_acc 61.27
2023-10-17 19:34:04,861 [podnet.py] => Task 1, Epoch 17/160 (LR 0.09724) => LSC_loss 0.26, Spatial_loss 3.82, Flat_loss 0.57, Train_acc 93.57, Test_acc 55.71
2023-10-17 19:34:06,745 [podnet.py] => Task 1, Epoch 18/160 (LR 0.09691) => LSC_loss 0.25, Spatial_loss 3.59, Flat_loss 0.54, Train_acc 93.34, Test_acc 59.35
2023-10-17 19:34:08,679 [podnet.py] => Task 1, Epoch 19/160 (LR 0.09656) => LSC_loss 0.24, Spatial_loss 3.47, Flat_loss 0.52, Train_acc 93.57, Test_acc 60.15
2023-10-17 19:34:10,567 [podnet.py] => Task 1, Epoch 20/160 (LR 0.09619) => LSC_loss 0.24, Spatial_loss 3.56, Flat_loss 0.54, Train_acc 94.06, Test_acc 60.20
2023-10-17 19:34:12,469 [podnet.py] => Task 1, Epoch 21/160 (LR 0.09581) => LSC_loss 0.23, Spatial_loss 3.50, Flat_loss 0.52, Train_acc 93.94, Test_acc 60.38
2023-10-17 19:34:14,329 [podnet.py] => Task 1, Epoch 22/160 (LR 0.09541) => LSC_loss 0.23, Spatial_loss 3.40, Flat_loss 0.51, Train_acc 93.77, Test_acc 61.33
2023-10-17 19:34:16,228 [podnet.py] => Task 1, Epoch 23/160 (LR 0.09499) => LSC_loss 0.22, Spatial_loss 3.50, Flat_loss 0.52, Train_acc 94.09, Test_acc 63.15
2023-10-17 19:34:18,162 [podnet.py] => Task 1, Epoch 24/160 (LR 0.09455) => LSC_loss 0.23, Spatial_loss 3.38, Flat_loss 0.50, Train_acc 94.43, Test_acc 62.29
2023-10-17 19:34:20,077 [podnet.py] => Task 1, Epoch 25/160 (LR 0.09410) => LSC_loss 0.22, Spatial_loss 3.33, Flat_loss 0.49, Train_acc 94.49, Test_acc 56.09
2023-10-17 19:34:22,053 [podnet.py] => Task 1, Epoch 26/160 (LR 0.09362) => LSC_loss 0.22, Spatial_loss 3.35, Flat_loss 0.51, Train_acc 94.49, Test_acc 63.18
2023-10-17 19:34:23,932 [podnet.py] => Task 1, Epoch 27/160 (LR 0.09314) => LSC_loss 0.21, Spatial_loss 3.24, Flat_loss 0.49, Train_acc 94.60, Test_acc 65.11
2023-10-17 19:34:25,871 [podnet.py] => Task 1, Epoch 28/160 (LR 0.09263) => LSC_loss 0.20, Spatial_loss 3.26, Flat_loss 0.46, Train_acc 95.49, Test_acc 63.05
2023-10-17 19:34:27,831 [podnet.py] => Task 1, Epoch 29/160 (LR 0.09211) => LSC_loss 0.22, Spatial_loss 3.27, Flat_loss 0.48, Train_acc 94.34, Test_acc 57.33
2023-10-17 19:34:29,761 [podnet.py] => Task 1, Epoch 30/160 (LR 0.09157) => LSC_loss 0.22, Spatial_loss 3.23, Flat_loss 0.47, Train_acc 94.51, Test_acc 63.05
2023-10-17 19:34:31,695 [podnet.py] => Task 1, Epoch 31/160 (LR 0.09102) => LSC_loss 0.22, Spatial_loss 3.26, Flat_loss 0.47, Train_acc 94.46, Test_acc 61.58
2023-10-17 19:34:33,588 [podnet.py] => Task 1, Epoch 32/160 (LR 0.09045) => LSC_loss 0.20, Spatial_loss 3.12, Flat_loss 0.46, Train_acc 95.46, Test_acc 62.11
2023-10-17 19:34:35,506 [podnet.py] => Task 1, Epoch 33/160 (LR 0.08987) => LSC_loss 0.21, Spatial_loss 3.17, Flat_loss 0.46, Train_acc 95.23, Test_acc 59.69
2023-10-17 19:34:37,430 [podnet.py] => Task 1, Epoch 34/160 (LR 0.08927) => LSC_loss 0.21, Spatial_loss 3.19, Flat_loss 0.46, Train_acc 94.89, Test_acc 65.04
2023-10-17 19:34:39,325 [podnet.py] => Task 1, Epoch 35/160 (LR 0.08865) => LSC_loss 0.20, Spatial_loss 3.13, Flat_loss 0.45, Train_acc 94.86, Test_acc 64.15
2023-10-17 19:34:41,218 [podnet.py] => Task 1, Epoch 36/160 (LR 0.08802) => LSC_loss 0.20, Spatial_loss 3.11, Flat_loss 0.44, Train_acc 95.23, Test_acc 61.02
2023-10-17 19:34:43,145 [podnet.py] => Task 1, Epoch 37/160 (LR 0.08738) => LSC_loss 0.21, Spatial_loss 3.16, Flat_loss 0.46, Train_acc 94.63, Test_acc 62.40
2023-10-17 19:34:45,093 [podnet.py] => Task 1, Epoch 38/160 (LR 0.08672) => LSC_loss 0.21, Spatial_loss 3.21, Flat_loss 0.46, Train_acc 95.00, Test_acc 62.76
2023-10-17 19:34:47,055 [podnet.py] => Task 1, Epoch 39/160 (LR 0.08604) => LSC_loss 0.21, Spatial_loss 3.09, Flat_loss 0.45, Train_acc 95.06, Test_acc 56.80
2023-10-17 19:34:48,985 [podnet.py] => Task 1, Epoch 40/160 (LR 0.08536) => LSC_loss 0.21, Spatial_loss 3.14, Flat_loss 0.44, Train_acc 95.26, Test_acc 60.18
2023-10-17 19:34:50,916 [podnet.py] => Task 1, Epoch 41/160 (LR 0.08465) => LSC_loss 0.20, Spatial_loss 3.08, Flat_loss 0.44, Train_acc 95.20, Test_acc 63.53
2023-10-17 19:34:52,783 [podnet.py] => Task 1, Epoch 42/160 (LR 0.08394) => LSC_loss 0.20, Spatial_loss 3.09, Flat_loss 0.43, Train_acc 95.09, Test_acc 66.18
2023-10-17 19:34:54,665 [podnet.py] => Task 1, Epoch 43/160 (LR 0.08321) => LSC_loss 0.20, Spatial_loss 3.03, Flat_loss 0.43, Train_acc 95.14, Test_acc 59.73
2023-10-17 19:34:56,567 [podnet.py] => Task 1, Epoch 44/160 (LR 0.08247) => LSC_loss 0.20, Spatial_loss 3.17, Flat_loss 0.45, Train_acc 95.06, Test_acc 57.98
2023-10-17 19:34:58,458 [podnet.py] => Task 1, Epoch 45/160 (LR 0.08172) => LSC_loss 0.18, Spatial_loss 3.03, Flat_loss 0.42, Train_acc 95.29, Test_acc 63.95
2023-10-17 19:35:00,381 [podnet.py] => Task 1, Epoch 46/160 (LR 0.08095) => LSC_loss 0.20, Spatial_loss 2.98, Flat_loss 0.41, Train_acc 95.31, Test_acc 59.64
2023-10-17 19:35:02,264 [podnet.py] => Task 1, Epoch 47/160 (LR 0.08018) => LSC_loss 0.18, Spatial_loss 2.95, Flat_loss 0.42, Train_acc 95.74, Test_acc 65.22
2023-10-17 19:35:04,230 [podnet.py] => Task 1, Epoch 48/160 (LR 0.07939) => LSC_loss 0.21, Spatial_loss 3.00, Flat_loss 0.42, Train_acc 94.97, Test_acc 62.13
2023-10-17 19:35:06,168 [podnet.py] => Task 1, Epoch 49/160 (LR 0.07859) => LSC_loss 0.22, Spatial_loss 3.17, Flat_loss 0.46, Train_acc 94.17, Test_acc 64.16
2023-10-17 19:35:08,087 [podnet.py] => Task 1, Epoch 50/160 (LR 0.07778) => LSC_loss 0.19, Spatial_loss 3.01, Flat_loss 0.43, Train_acc 95.40, Test_acc 58.31
2023-10-17 19:35:10,026 [podnet.py] => Task 1, Epoch 51/160 (LR 0.07696) => LSC_loss 0.19, Spatial_loss 2.97, Flat_loss 0.42, Train_acc 96.00, Test_acc 63.47
2023-10-17 19:35:11,910 [podnet.py] => Task 1, Epoch 52/160 (LR 0.07612) => LSC_loss 0.19, Spatial_loss 3.02, Flat_loss 0.42, Train_acc 95.54, Test_acc 61.11
2023-10-17 19:35:13,821 [podnet.py] => Task 1, Epoch 53/160 (LR 0.07528) => LSC_loss 0.19, Spatial_loss 2.94, Flat_loss 0.40, Train_acc 95.60, Test_acc 65.69
2023-10-17 19:35:15,782 [podnet.py] => Task 1, Epoch 54/160 (LR 0.07443) => LSC_loss 0.19, Spatial_loss 2.89, Flat_loss 0.40, Train_acc 95.60, Test_acc 64.47
2023-10-17 19:35:17,702 [podnet.py] => Task 1, Epoch 55/160 (LR 0.07357) => LSC_loss 0.20, Spatial_loss 2.98, Flat_loss 0.42, Train_acc 94.91, Test_acc 61.67
2023-10-17 19:35:19,551 [podnet.py] => Task 1, Epoch 56/160 (LR 0.07270) => LSC_loss 0.18, Spatial_loss 2.94, Flat_loss 0.40, Train_acc 95.49, Test_acc 66.91
2023-10-17 19:35:21,401 [podnet.py] => Task 1, Epoch 57/160 (LR 0.07182) => LSC_loss 0.18, Spatial_loss 2.77, Flat_loss 0.38, Train_acc 95.86, Test_acc 65.91
2023-10-17 19:35:23,300 [podnet.py] => Task 1, Epoch 58/160 (LR 0.07093) => LSC_loss 0.18, Spatial_loss 2.81, Flat_loss 0.39, Train_acc 95.80, Test_acc 58.53
2023-10-17 19:35:25,219 [podnet.py] => Task 1, Epoch 59/160 (LR 0.07004) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.38, Train_acc 95.91, Test_acc 64.53
2023-10-17 19:35:27,102 [podnet.py] => Task 1, Epoch 60/160 (LR 0.06913) => LSC_loss 0.19, Spatial_loss 2.87, Flat_loss 0.40, Train_acc 95.71, Test_acc 62.47
2023-10-17 19:35:29,031 [podnet.py] => Task 1, Epoch 61/160 (LR 0.06822) => LSC_loss 0.19, Spatial_loss 2.90, Flat_loss 0.40, Train_acc 95.46, Test_acc 66.58
2023-10-17 19:35:30,940 [podnet.py] => Task 1, Epoch 62/160 (LR 0.06731) => LSC_loss 0.19, Spatial_loss 2.83, Flat_loss 0.38, Train_acc 95.49, Test_acc 66.78
2023-10-17 19:35:32,845 [podnet.py] => Task 1, Epoch 63/160 (LR 0.06638) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.39, Train_acc 95.66, Test_acc 66.75
2023-10-17 19:35:34,795 [podnet.py] => Task 1, Epoch 64/160 (LR 0.06545) => LSC_loss 0.20, Spatial_loss 2.89, Flat_loss 0.39, Train_acc 95.60, Test_acc 65.05
2023-10-17 19:35:36,703 [podnet.py] => Task 1, Epoch 65/160 (LR 0.06451) => LSC_loss 0.18, Spatial_loss 2.85, Flat_loss 0.39, Train_acc 95.89, Test_acc 65.62
2023-10-17 19:35:38,608 [podnet.py] => Task 1, Epoch 66/160 (LR 0.06357) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.38, Train_acc 95.46, Test_acc 66.93
2023-10-17 19:35:40,567 [podnet.py] => Task 1, Epoch 67/160 (LR 0.06262) => LSC_loss 0.17, Spatial_loss 2.73, Flat_loss 0.37, Train_acc 96.20, Test_acc 65.56
2023-10-17 19:35:42,484 [podnet.py] => Task 1, Epoch 68/160 (LR 0.06167) => LSC_loss 0.19, Spatial_loss 2.79, Flat_loss 0.37, Train_acc 96.17, Test_acc 65.00
2023-10-17 19:35:44,429 [podnet.py] => Task 1, Epoch 69/160 (LR 0.06072) => LSC_loss 0.18, Spatial_loss 2.74, Flat_loss 0.37, Train_acc 96.23, Test_acc 65.24
2023-10-17 19:35:46,362 [podnet.py] => Task 1, Epoch 70/160 (LR 0.05975) => LSC_loss 0.18, Spatial_loss 2.72, Flat_loss 0.37, Train_acc 96.17, Test_acc 68.40
2023-10-17 19:35:48,222 [podnet.py] => Task 1, Epoch 71/160 (LR 0.05879) => LSC_loss 0.19, Spatial_loss 2.65, Flat_loss 0.35, Train_acc 96.46, Test_acc 68.78
2023-10-17 19:35:50,177 [podnet.py] => Task 1, Epoch 72/160 (LR 0.05782) => LSC_loss 0.17, Spatial_loss 2.69, Flat_loss 0.37, Train_acc 96.00, Test_acc 65.47
2023-10-17 19:35:52,103 [podnet.py] => Task 1, Epoch 73/160 (LR 0.05685) => LSC_loss 0.18, Spatial_loss 2.73, Flat_loss 0.37, Train_acc 95.60, Test_acc 64.96
2023-10-17 19:35:54,011 [podnet.py] => Task 1, Epoch 74/160 (LR 0.05588) => LSC_loss 0.18, Spatial_loss 2.65, Flat_loss 0.37, Train_acc 95.57, Test_acc 66.89
2023-10-17 19:35:55,967 [podnet.py] => Task 1, Epoch 75/160 (LR 0.05490) => LSC_loss 0.19, Spatial_loss 2.76, Flat_loss 0.37, Train_acc 95.46, Test_acc 60.00
2023-10-17 19:35:57,865 [podnet.py] => Task 1, Epoch 76/160 (LR 0.05392) => LSC_loss 0.17, Spatial_loss 2.60, Flat_loss 0.36, Train_acc 96.20, Test_acc 67.13
2023-10-17 19:35:59,813 [podnet.py] => Task 1, Epoch 77/160 (LR 0.05294) => LSC_loss 0.17, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.49, Test_acc 65.84
2023-10-17 19:36:01,750 [podnet.py] => Task 1, Epoch 78/160 (LR 0.05196) => LSC_loss 0.18, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.17, Test_acc 66.58
2023-10-17 19:36:03,703 [podnet.py] => Task 1, Epoch 79/160 (LR 0.05098) => LSC_loss 0.18, Spatial_loss 2.71, Flat_loss 0.36, Train_acc 95.86, Test_acc 66.82
2023-10-17 19:36:05,578 [podnet.py] => Task 1, Epoch 80/160 (LR 0.05000) => LSC_loss 0.16, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.09, Test_acc 66.18
2023-10-17 19:36:07,457 [podnet.py] => Task 1, Epoch 81/160 (LR 0.04902) => LSC_loss 0.18, Spatial_loss 2.60, Flat_loss 0.35, Train_acc 95.74, Test_acc 66.96
2023-10-17 19:36:09,355 [podnet.py] => Task 1, Epoch 82/160 (LR 0.04804) => LSC_loss 0.19, Spatial_loss 2.65, Flat_loss 0.35, Train_acc 95.74, Test_acc 65.27
2023-10-17 19:36:11,277 [podnet.py] => Task 1, Epoch 83/160 (LR 0.04706) => LSC_loss 0.18, Spatial_loss 2.53, Flat_loss 0.35, Train_acc 95.69, Test_acc 67.58
2023-10-17 19:36:13,230 [podnet.py] => Task 1, Epoch 84/160 (LR 0.04608) => LSC_loss 0.18, Spatial_loss 2.56, Flat_loss 0.34, Train_acc 95.83, Test_acc 68.95
2023-10-17 19:36:15,155 [podnet.py] => Task 1, Epoch 85/160 (LR 0.04510) => LSC_loss 0.18, Spatial_loss 2.55, Flat_loss 0.34, Train_acc 96.03, Test_acc 66.64
2023-10-17 19:36:17,124 [podnet.py] => Task 1, Epoch 86/160 (LR 0.04412) => LSC_loss 0.16, Spatial_loss 2.52, Flat_loss 0.33, Train_acc 96.23, Test_acc 68.58
2023-10-17 19:36:19,088 [podnet.py] => Task 1, Epoch 87/160 (LR 0.04315) => LSC_loss 0.17, Spatial_loss 2.48, Flat_loss 0.33, Train_acc 96.46, Test_acc 67.31
2023-10-17 19:36:21,006 [podnet.py] => Task 1, Epoch 88/160 (LR 0.04218) => LSC_loss 0.17, Spatial_loss 2.47, Flat_loss 0.34, Train_acc 96.17, Test_acc 68.25
2023-10-17 19:36:22,930 [podnet.py] => Task 1, Epoch 89/160 (LR 0.04121) => LSC_loss 0.18, Spatial_loss 2.43, Flat_loss 0.32, Train_acc 96.20, Test_acc 68.25
2023-10-17 19:36:24,862 [podnet.py] => Task 1, Epoch 90/160 (LR 0.04025) => LSC_loss 0.18, Spatial_loss 2.39, Flat_loss 0.33, Train_acc 95.63, Test_acc 66.42
2023-10-17 19:36:26,712 [podnet.py] => Task 1, Epoch 91/160 (LR 0.03928) => LSC_loss 0.17, Spatial_loss 2.41, Flat_loss 0.32, Train_acc 95.89, Test_acc 68.20
2023-10-17 19:36:28,641 [podnet.py] => Task 1, Epoch 92/160 (LR 0.03833) => LSC_loss 0.16, Spatial_loss 2.38, Flat_loss 0.31, Train_acc 96.63, Test_acc 64.82
2023-10-17 19:36:30,487 [podnet.py] => Task 1, Epoch 93/160 (LR 0.03738) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 96.57, Test_acc 68.95
2023-10-17 19:36:32,410 [podnet.py] => Task 1, Epoch 94/160 (LR 0.03643) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 96.31, Test_acc 67.56
2023-10-17 19:36:34,362 [podnet.py] => Task 1, Epoch 95/160 (LR 0.03549) => LSC_loss 0.17, Spatial_loss 2.36, Flat_loss 0.31, Train_acc 96.37, Test_acc 68.45
2023-10-17 19:36:36,259 [podnet.py] => Task 1, Epoch 96/160 (LR 0.03455) => LSC_loss 0.18, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 95.77, Test_acc 68.33
2023-10-17 19:36:38,190 [podnet.py] => Task 1, Epoch 97/160 (LR 0.03362) => LSC_loss 0.17, Spatial_loss 2.33, Flat_loss 0.31, Train_acc 95.94, Test_acc 68.38
2023-10-17 19:36:40,100 [podnet.py] => Task 1, Epoch 98/160 (LR 0.03269) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 95.77, Test_acc 68.78
2023-10-17 19:36:41,954 [podnet.py] => Task 1, Epoch 99/160 (LR 0.03178) => LSC_loss 0.16, Spatial_loss 2.29, Flat_loss 0.30, Train_acc 96.60, Test_acc 69.60
2023-10-17 19:36:43,811 [podnet.py] => Task 1, Epoch 100/160 (LR 0.03087) => LSC_loss 0.16, Spatial_loss 2.29, Flat_loss 0.30, Train_acc 96.83, Test_acc 68.33
2023-10-17 19:36:45,660 [podnet.py] => Task 1, Epoch 101/160 (LR 0.02996) => LSC_loss 0.19, Spatial_loss 2.31, Flat_loss 0.30, Train_acc 95.83, Test_acc 67.80
2023-10-17 19:36:47,618 [podnet.py] => Task 1, Epoch 102/160 (LR 0.02907) => LSC_loss 0.17, Spatial_loss 2.28, Flat_loss 0.30, Train_acc 96.11, Test_acc 69.09
2023-10-17 19:36:49,566 [podnet.py] => Task 1, Epoch 103/160 (LR 0.02818) => LSC_loss 0.16, Spatial_loss 2.21, Flat_loss 0.29, Train_acc 96.57, Test_acc 68.84
2023-10-17 19:36:51,465 [podnet.py] => Task 1, Epoch 104/160 (LR 0.02730) => LSC_loss 0.18, Spatial_loss 2.24, Flat_loss 0.29, Train_acc 95.94, Test_acc 68.44
2023-10-17 19:36:53,409 [podnet.py] => Task 1, Epoch 105/160 (LR 0.02643) => LSC_loss 0.17, Spatial_loss 2.28, Flat_loss 0.29, Train_acc 96.23, Test_acc 68.24
2023-10-17 19:36:55,327 [podnet.py] => Task 1, Epoch 106/160 (LR 0.02557) => LSC_loss 0.17, Spatial_loss 2.22, Flat_loss 0.29, Train_acc 96.00, Test_acc 67.98
2023-10-17 19:36:57,322 [podnet.py] => Task 1, Epoch 107/160 (LR 0.02472) => LSC_loss 0.16, Spatial_loss 2.16, Flat_loss 0.28, Train_acc 96.63, Test_acc 70.18
2023-10-17 19:36:59,297 [podnet.py] => Task 1, Epoch 108/160 (LR 0.02388) => LSC_loss 0.18, Spatial_loss 2.14, Flat_loss 0.28, Train_acc 95.86, Test_acc 69.78
2023-10-17 19:37:01,230 [podnet.py] => Task 1, Epoch 109/160 (LR 0.02304) => LSC_loss 0.17, Spatial_loss 2.15, Flat_loss 0.28, Train_acc 96.77, Test_acc 68.73
2023-10-17 19:37:03,091 [podnet.py] => Task 1, Epoch 110/160 (LR 0.02222) => LSC_loss 0.17, Spatial_loss 2.20, Flat_loss 0.29, Train_acc 96.20, Test_acc 68.91
2023-10-17 19:37:04,977 [podnet.py] => Task 1, Epoch 111/160 (LR 0.02141) => LSC_loss 0.16, Spatial_loss 2.07, Flat_loss 0.27, Train_acc 96.49, Test_acc 69.44
2023-10-17 19:37:06,893 [podnet.py] => Task 1, Epoch 112/160 (LR 0.02061) => LSC_loss 0.17, Spatial_loss 2.11, Flat_loss 0.27, Train_acc 96.34, Test_acc 69.25
2023-10-17 19:37:08,819 [podnet.py] => Task 1, Epoch 113/160 (LR 0.01982) => LSC_loss 0.17, Spatial_loss 2.15, Flat_loss 0.28, Train_acc 96.40, Test_acc 69.95
2023-10-17 19:37:10,691 [podnet.py] => Task 1, Epoch 114/160 (LR 0.01905) => LSC_loss 0.16, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 97.06, Test_acc 71.00
2023-10-17 19:37:12,615 [podnet.py] => Task 1, Epoch 115/160 (LR 0.01828) => LSC_loss 0.17, Spatial_loss 2.08, Flat_loss 0.27, Train_acc 96.29, Test_acc 69.53
2023-10-17 19:37:14,513 [podnet.py] => Task 1, Epoch 116/160 (LR 0.01753) => LSC_loss 0.16, Spatial_loss 2.03, Flat_loss 0.27, Train_acc 96.51, Test_acc 70.00
2023-10-17 19:37:16,357 [podnet.py] => Task 1, Epoch 117/160 (LR 0.01679) => LSC_loss 0.16, Spatial_loss 2.04, Flat_loss 0.26, Train_acc 96.49, Test_acc 70.07
2023-10-17 19:37:18,262 [podnet.py] => Task 1, Epoch 118/160 (LR 0.01606) => LSC_loss 0.17, Spatial_loss 2.03, Flat_loss 0.26, Train_acc 96.03, Test_acc 70.05
2023-10-17 19:37:20,204 [podnet.py] => Task 1, Epoch 119/160 (LR 0.01535) => LSC_loss 0.17, Spatial_loss 2.03, Flat_loss 0.26, Train_acc 96.29, Test_acc 70.78
2023-10-17 19:37:22,125 [podnet.py] => Task 1, Epoch 120/160 (LR 0.01464) => LSC_loss 0.17, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 96.40, Test_acc 69.60
2023-10-17 19:37:24,055 [podnet.py] => Task 1, Epoch 121/160 (LR 0.01396) => LSC_loss 0.16, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 96.60, Test_acc 69.85
2023-10-17 19:37:25,912 [podnet.py] => Task 1, Epoch 122/160 (LR 0.01328) => LSC_loss 0.17, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 96.37, Test_acc 72.49
2023-10-17 19:37:27,857 [podnet.py] => Task 1, Epoch 123/160 (LR 0.01262) => LSC_loss 0.17, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 96.66, Test_acc 71.13
2023-10-17 19:37:29,830 [podnet.py] => Task 1, Epoch 124/160 (LR 0.01198) => LSC_loss 0.17, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 96.66, Test_acc 70.95
2023-10-17 19:37:31,709 [podnet.py] => Task 1, Epoch 125/160 (LR 0.01135) => LSC_loss 0.16, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 96.71, Test_acc 70.78
2023-10-17 19:37:33,662 [podnet.py] => Task 1, Epoch 126/160 (LR 0.01073) => LSC_loss 0.16, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 97.09, Test_acc 70.91
2023-10-17 19:37:35,508 [podnet.py] => Task 1, Epoch 127/160 (LR 0.01013) => LSC_loss 0.16, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 96.89, Test_acc 70.75
2023-10-17 19:37:37,442 [podnet.py] => Task 1, Epoch 128/160 (LR 0.00955) => LSC_loss 0.17, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 96.23, Test_acc 70.42
2023-10-17 19:37:39,347 [podnet.py] => Task 1, Epoch 129/160 (LR 0.00898) => LSC_loss 0.16, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 97.06, Test_acc 71.40
2023-10-17 19:37:41,257 [podnet.py] => Task 1, Epoch 130/160 (LR 0.00843) => LSC_loss 0.16, Spatial_loss 1.82, Flat_loss 0.24, Train_acc 96.40, Test_acc 70.45
2023-10-17 19:37:43,157 [podnet.py] => Task 1, Epoch 131/160 (LR 0.00789) => LSC_loss 0.16, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 97.03, Test_acc 71.05
2023-10-17 19:37:45,074 [podnet.py] => Task 1, Epoch 132/160 (LR 0.00737) => LSC_loss 0.17, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 96.60, Test_acc 70.98
2023-10-17 19:37:46,970 [podnet.py] => Task 1, Epoch 133/160 (LR 0.00686) => LSC_loss 0.16, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 97.06, Test_acc 70.89
2023-10-17 19:37:48,865 [podnet.py] => Task 1, Epoch 134/160 (LR 0.00638) => LSC_loss 0.16, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 97.17, Test_acc 71.22
2023-10-17 19:37:50,775 [podnet.py] => Task 1, Epoch 135/160 (LR 0.00590) => LSC_loss 0.18, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 96.49, Test_acc 70.87
2023-10-17 19:37:52,664 [podnet.py] => Task 1, Epoch 136/160 (LR 0.00545) => LSC_loss 0.17, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 96.63, Test_acc 70.45
2023-10-17 19:37:54,568 [podnet.py] => Task 1, Epoch 137/160 (LR 0.00501) => LSC_loss 0.16, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 96.66, Test_acc 71.27
2023-10-17 19:37:56,477 [podnet.py] => Task 1, Epoch 138/160 (LR 0.00459) => LSC_loss 0.17, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 96.66, Test_acc 72.07
2023-10-17 19:37:58,400 [podnet.py] => Task 1, Epoch 139/160 (LR 0.00419) => LSC_loss 0.16, Spatial_loss 1.72, Flat_loss 0.23, Train_acc 96.91, Test_acc 71.49
2023-10-17 19:38:00,334 [podnet.py] => Task 1, Epoch 140/160 (LR 0.00381) => LSC_loss 0.16, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 96.69, Test_acc 71.42
2023-10-17 19:38:02,276 [podnet.py] => Task 1, Epoch 141/160 (LR 0.00344) => LSC_loss 0.16, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 97.09, Test_acc 71.85
2023-10-17 19:38:04,190 [podnet.py] => Task 1, Epoch 142/160 (LR 0.00309) => LSC_loss 0.17, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 96.43, Test_acc 71.69
2023-10-17 19:38:06,070 [podnet.py] => Task 1, Epoch 143/160 (LR 0.00276) => LSC_loss 0.16, Spatial_loss 1.70, Flat_loss 0.22, Train_acc 96.91, Test_acc 71.36
2023-10-17 19:38:07,980 [podnet.py] => Task 1, Epoch 144/160 (LR 0.00245) => LSC_loss 0.15, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 97.23, Test_acc 71.62
2023-10-17 19:38:09,933 [podnet.py] => Task 1, Epoch 145/160 (LR 0.00215) => LSC_loss 0.17, Spatial_loss 1.71, Flat_loss 0.22, Train_acc 97.23, Test_acc 71.69
2023-10-17 19:38:11,820 [podnet.py] => Task 1, Epoch 146/160 (LR 0.00188) => LSC_loss 0.16, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 96.74, Test_acc 71.25
2023-10-17 19:38:13,784 [podnet.py] => Task 1, Epoch 147/160 (LR 0.00162) => LSC_loss 0.16, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 96.57, Test_acc 71.84
2023-10-17 19:38:15,681 [podnet.py] => Task 1, Epoch 148/160 (LR 0.00138) => LSC_loss 0.17, Spatial_loss 1.71, Flat_loss 0.23, Train_acc 96.60, Test_acc 71.73
2023-10-17 19:38:17,597 [podnet.py] => Task 1, Epoch 149/160 (LR 0.00116) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 96.71, Test_acc 71.69
2023-10-17 19:38:19,453 [podnet.py] => Task 1, Epoch 150/160 (LR 0.00096) => LSC_loss 0.15, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 97.54, Test_acc 71.75
2023-10-17 19:38:21,344 [podnet.py] => Task 1, Epoch 151/160 (LR 0.00078) => LSC_loss 0.16, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 97.14, Test_acc 71.80
2023-10-17 19:38:23,187 [podnet.py] => Task 1, Epoch 152/160 (LR 0.00062) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.22, Train_acc 97.37, Test_acc 71.73
2023-10-17 19:38:25,077 [podnet.py] => Task 1, Epoch 153/160 (LR 0.00047) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 96.91, Test_acc 71.76
2023-10-17 19:38:26,952 [podnet.py] => Task 1, Epoch 154/160 (LR 0.00035) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.22, Train_acc 96.43, Test_acc 71.75
2023-10-17 19:38:28,860 [podnet.py] => Task 1, Epoch 155/160 (LR 0.00024) => LSC_loss 0.15, Spatial_loss 1.58, Flat_loss 0.21, Train_acc 97.74, Test_acc 71.89
2023-10-17 19:38:30,691 [podnet.py] => Task 1, Epoch 156/160 (LR 0.00015) => LSC_loss 0.16, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 97.31, Test_acc 71.78
2023-10-17 19:38:32,583 [podnet.py] => Task 1, Epoch 157/160 (LR 0.00009) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 96.97, Test_acc 71.80
2023-10-17 19:38:34,509 [podnet.py] => Task 1, Epoch 158/160 (LR 0.00004) => LSC_loss 0.17, Spatial_loss 1.64, Flat_loss 0.22, Train_acc 96.49, Test_acc 71.69
2023-10-17 19:38:36,350 [podnet.py] => Task 1, Epoch 159/160 (LR 0.00001) => LSC_loss 0.15, Spatial_loss 1.56, Flat_loss 0.21, Train_acc 97.23, Test_acc 71.69
2023-10-17 19:38:38,269 [podnet.py] => Task 1, Epoch 160/160 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.21, Train_acc 97.09, Test_acc 71.69
2023-10-17 19:38:38,270 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 19:38:38,271 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:39:01,297 [podnet.py] => The size of finetune dataset: 1100
2023-10-17 19:39:02,709 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.12, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 96.73, Test_acc 70.53
2023-10-17 19:39:04,081 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.06, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 99.00, Test_acc 71.00
2023-10-17 19:39:05,421 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.05, Spatial_loss 1.69, Flat_loss 0.13, Train_acc 99.27, Test_acc 72.22
2023-10-17 19:39:06,790 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.05, Spatial_loss 1.67, Flat_loss 0.12, Train_acc 99.09, Test_acc 72.53
2023-10-17 19:39:08,176 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.05, Spatial_loss 1.66, Flat_loss 0.12, Train_acc 98.73, Test_acc 73.04
2023-10-17 19:39:09,539 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.06, Spatial_loss 1.59, Flat_loss 0.10, Train_acc 98.82, Test_acc 73.16
2023-10-17 19:39:10,897 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.05, Spatial_loss 1.61, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.15
2023-10-17 19:39:12,228 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.04, Spatial_loss 1.61, Flat_loss 0.10, Train_acc 99.55, Test_acc 73.67
2023-10-17 19:39:13,581 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.04, Spatial_loss 1.59, Flat_loss 0.10, Train_acc 99.45, Test_acc 73.60
2023-10-17 19:39:14,952 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.05, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.27, Test_acc 73.44
2023-10-17 19:39:16,331 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.04, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 99.18, Test_acc 73.69
2023-10-17 19:39:17,704 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.05, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.73
2023-10-17 19:39:19,057 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.04, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 99.55, Test_acc 73.55
2023-10-17 19:39:20,457 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.05, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.09, Test_acc 73.65
2023-10-17 19:39:21,806 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.05, Spatial_loss 1.52, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.75
2023-10-17 19:39:23,165 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.04, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.27, Test_acc 73.95
2023-10-17 19:39:24,544 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 1.51, Flat_loss 0.09, Train_acc 99.45, Test_acc 73.65
2023-10-17 19:39:25,927 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.04, Spatial_loss 1.52, Flat_loss 0.09, Train_acc 99.45, Test_acc 73.51
2023-10-17 19:39:27,292 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.05, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 99.09, Test_acc 73.65
2023-10-17 19:39:28,619 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 99.18, Test_acc 73.69
2023-10-17 19:39:28,620 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:39:52,611 [podnet.py] => Exemplar size: 1100
2023-10-17 19:39:52,611 [trainer.py] => CNN: {'total': 73.69, '00-09': 77.2, '10-19': 69.3, '20-29': 78.2, '30-39': 72.5, '40-49': 73.6, '50-59': 69.0, 'old': 74.16, 'new': 69.0}
2023-10-17 19:39:52,611 [trainer.py] => NME: {'total': 73.36, '00-09': 77.9, '10-19': 69.1, '20-29': 78.3, '30-39': 72.4, '40-49': 75.6, '50-59': 60.4, 'old': 74.66, 'new': 60.4}
2023-10-17 19:39:52,611 [trainer.py] => CNN top1 curve: [77.7, 73.69]
2023-10-17 19:39:52,611 [trainer.py] => CNN top5 curve: [94.08, 93.33]
2023-10-17 19:39:52,611 [trainer.py] => NME top1 curve: [77.44, 73.36]
2023-10-17 19:39:52,611 [trainer.py] => NME top5 curve: [93.98, 93.18]

2023-10-17 19:39:52,611 [trainer.py] => Average Accuracy (CNN): 75.695
2023-10-17 19:39:52,611 [trainer.py] => Average Accuracy (NME): 75.4
2023-10-17 19:39:52,612 [trainer.py] => All params: 501457
2023-10-17 19:39:52,612 [trainer.py] => Trainable params: 501457
2023-10-17 19:39:52,613 [podnet.py] => Learning on 55-60
2023-10-17 19:39:52,642 [podnet.py] => Adaptive factor: 3.4641016151377544
2023-10-17 19:39:54,742 [podnet.py] => Task 2, Epoch 1/160 (LR 0.09999) => LSC_loss 2.11, Spatial_loss 4.21, Flat_loss 1.10, Train_acc 53.36, Test_acc 42.88
2023-10-17 19:39:56,726 [podnet.py] => Task 2, Epoch 2/160 (LR 0.09996) => LSC_loss 0.97, Spatial_loss 4.34, Flat_loss 0.92, Train_acc 69.81, Test_acc 52.40
2023-10-17 19:39:58,734 [podnet.py] => Task 2, Epoch 3/160 (LR 0.09991) => LSC_loss 0.81, Spatial_loss 4.03, Flat_loss 0.77, Train_acc 75.89, Test_acc 53.58
2023-10-17 19:40:00,740 [podnet.py] => Task 2, Epoch 4/160 (LR 0.09985) => LSC_loss 0.71, Spatial_loss 3.81, Flat_loss 0.72, Train_acc 78.58, Test_acc 49.85
2023-10-17 19:40:02,808 [podnet.py] => Task 2, Epoch 5/160 (LR 0.09976) => LSC_loss 0.65, Spatial_loss 3.77, Flat_loss 0.66, Train_acc 80.36, Test_acc 57.58
2023-10-17 19:40:04,827 [podnet.py] => Task 2, Epoch 6/160 (LR 0.09965) => LSC_loss 0.59, Spatial_loss 3.56, Flat_loss 0.61, Train_acc 82.53, Test_acc 57.38
2023-10-17 19:40:06,763 [podnet.py] => Task 2, Epoch 7/160 (LR 0.09953) => LSC_loss 0.58, Spatial_loss 3.56, Flat_loss 0.59, Train_acc 82.92, Test_acc 56.23
2023-10-17 19:40:08,742 [podnet.py] => Task 2, Epoch 8/160 (LR 0.09938) => LSC_loss 0.55, Spatial_loss 3.62, Flat_loss 0.60, Train_acc 83.94, Test_acc 56.52
2023-10-17 19:40:10,752 [podnet.py] => Task 2, Epoch 9/160 (LR 0.09922) => LSC_loss 0.51, Spatial_loss 3.49, Flat_loss 0.57, Train_acc 85.33, Test_acc 57.02
2023-10-17 19:40:12,796 [podnet.py] => Task 2, Epoch 10/160 (LR 0.09904) => LSC_loss 0.47, Spatial_loss 3.39, Flat_loss 0.55, Train_acc 85.89, Test_acc 55.67
2023-10-17 19:40:14,729 [podnet.py] => Task 2, Epoch 11/160 (LR 0.09884) => LSC_loss 0.48, Spatial_loss 3.29, Flat_loss 0.53, Train_acc 85.25, Test_acc 56.32
2023-10-17 19:40:16,695 [podnet.py] => Task 2, Epoch 12/160 (LR 0.09862) => LSC_loss 0.45, Spatial_loss 3.42, Flat_loss 0.54, Train_acc 86.89, Test_acc 58.58
2023-10-17 19:40:18,698 [podnet.py] => Task 2, Epoch 13/160 (LR 0.09838) => LSC_loss 0.47, Spatial_loss 3.50, Flat_loss 0.57, Train_acc 86.75, Test_acc 58.83
2023-10-17 19:40:20,614 [podnet.py] => Task 2, Epoch 14/160 (LR 0.09812) => LSC_loss 0.45, Spatial_loss 3.33, Flat_loss 0.52, Train_acc 86.61, Test_acc 58.82
2023-10-17 19:40:22,596 [podnet.py] => Task 2, Epoch 15/160 (LR 0.09785) => LSC_loss 0.43, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 87.69, Test_acc 59.30
2023-10-17 19:40:24,648 [podnet.py] => Task 2, Epoch 16/160 (LR 0.09755) => LSC_loss 0.46, Spatial_loss 3.26, Flat_loss 0.51, Train_acc 87.31, Test_acc 53.95
2023-10-17 19:40:26,602 [podnet.py] => Task 2, Epoch 17/160 (LR 0.09724) => LSC_loss 0.44, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 87.78, Test_acc 57.20
2023-10-17 19:40:28,564 [podnet.py] => Task 2, Epoch 18/160 (LR 0.09691) => LSC_loss 0.42, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 88.33, Test_acc 60.33
2023-10-17 19:40:30,510 [podnet.py] => Task 2, Epoch 19/160 (LR 0.09656) => LSC_loss 0.39, Spatial_loss 3.30, Flat_loss 0.52, Train_acc 89.89, Test_acc 59.32
2023-10-17 19:40:32,542 [podnet.py] => Task 2, Epoch 20/160 (LR 0.09619) => LSC_loss 0.40, Spatial_loss 3.35, Flat_loss 0.52, Train_acc 89.03, Test_acc 61.22
2023-10-17 19:40:34,540 [podnet.py] => Task 2, Epoch 21/160 (LR 0.09581) => LSC_loss 0.43, Spatial_loss 3.29, Flat_loss 0.53, Train_acc 88.28, Test_acc 58.52
2023-10-17 19:40:36,554 [podnet.py] => Task 2, Epoch 22/160 (LR 0.09541) => LSC_loss 0.42, Spatial_loss 3.53, Flat_loss 0.59, Train_acc 87.75, Test_acc 62.92
2023-10-17 19:40:38,475 [podnet.py] => Task 2, Epoch 23/160 (LR 0.09499) => LSC_loss 0.37, Spatial_loss 3.25, Flat_loss 0.52, Train_acc 90.19, Test_acc 59.13
2023-10-17 19:40:40,500 [podnet.py] => Task 2, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.44, Flat_loss 0.55, Train_acc 89.81, Test_acc 54.97
2023-10-17 19:40:42,516 [podnet.py] => Task 2, Epoch 25/160 (LR 0.09410) => LSC_loss 0.36, Spatial_loss 3.22, Flat_loss 0.52, Train_acc 90.06, Test_acc 60.18
2023-10-17 19:40:44,530 [podnet.py] => Task 2, Epoch 26/160 (LR 0.09362) => LSC_loss 0.36, Spatial_loss 3.22, Flat_loss 0.50, Train_acc 90.44, Test_acc 62.38
2023-10-17 19:40:46,522 [podnet.py] => Task 2, Epoch 27/160 (LR 0.09314) => LSC_loss 0.36, Spatial_loss 3.25, Flat_loss 0.50, Train_acc 90.33, Test_acc 57.65
2023-10-17 19:40:48,528 [podnet.py] => Task 2, Epoch 28/160 (LR 0.09263) => LSC_loss 0.37, Spatial_loss 3.23, Flat_loss 0.49, Train_acc 90.31, Test_acc 60.55
2023-10-17 19:40:50,481 [podnet.py] => Task 2, Epoch 29/160 (LR 0.09211) => LSC_loss 0.35, Spatial_loss 3.24, Flat_loss 0.50, Train_acc 90.72, Test_acc 57.88
2023-10-17 19:40:52,446 [podnet.py] => Task 2, Epoch 30/160 (LR 0.09157) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.52, Train_acc 89.03, Test_acc 59.17
2023-10-17 19:40:54,485 [podnet.py] => Task 2, Epoch 31/160 (LR 0.09102) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.49, Train_acc 91.31, Test_acc 58.35
2023-10-17 19:40:56,486 [podnet.py] => Task 2, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 3.22, Flat_loss 0.51, Train_acc 89.86, Test_acc 58.68
2023-10-17 19:40:58,503 [podnet.py] => Task 2, Epoch 33/160 (LR 0.08987) => LSC_loss 0.35, Spatial_loss 3.20, Flat_loss 0.51, Train_acc 90.31, Test_acc 60.30
2023-10-17 19:41:00,532 [podnet.py] => Task 2, Epoch 34/160 (LR 0.08927) => LSC_loss 0.32, Spatial_loss 3.15, Flat_loss 0.49, Train_acc 91.31, Test_acc 58.85
2023-10-17 19:41:02,548 [podnet.py] => Task 2, Epoch 35/160 (LR 0.08865) => LSC_loss 0.31, Spatial_loss 3.00, Flat_loss 0.46, Train_acc 92.28, Test_acc 57.15
2023-10-17 19:41:04,566 [podnet.py] => Task 2, Epoch 36/160 (LR 0.08802) => LSC_loss 0.31, Spatial_loss 3.12, Flat_loss 0.47, Train_acc 91.64, Test_acc 60.50
2023-10-17 19:41:06,529 [podnet.py] => Task 2, Epoch 37/160 (LR 0.08738) => LSC_loss 0.33, Spatial_loss 3.10, Flat_loss 0.49, Train_acc 91.44, Test_acc 58.60
2023-10-17 19:41:08,477 [podnet.py] => Task 2, Epoch 38/160 (LR 0.08672) => LSC_loss 0.33, Spatial_loss 3.11, Flat_loss 0.50, Train_acc 90.67, Test_acc 59.35
2023-10-17 19:41:10,488 [podnet.py] => Task 2, Epoch 39/160 (LR 0.08604) => LSC_loss 0.34, Spatial_loss 3.12, Flat_loss 0.49, Train_acc 91.08, Test_acc 61.20
2023-10-17 19:41:12,484 [podnet.py] => Task 2, Epoch 40/160 (LR 0.08536) => LSC_loss 0.31, Spatial_loss 3.07, Flat_loss 0.48, Train_acc 91.58, Test_acc 62.13
2023-10-17 19:41:14,466 [podnet.py] => Task 2, Epoch 41/160 (LR 0.08465) => LSC_loss 0.31, Spatial_loss 3.08, Flat_loss 0.48, Train_acc 91.89, Test_acc 59.70
2023-10-17 19:41:16,463 [podnet.py] => Task 2, Epoch 42/160 (LR 0.08394) => LSC_loss 0.29, Spatial_loss 2.96, Flat_loss 0.45, Train_acc 92.17, Test_acc 62.53
2023-10-17 19:41:18,496 [podnet.py] => Task 2, Epoch 43/160 (LR 0.08321) => LSC_loss 0.30, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 92.31, Test_acc 61.23
2023-10-17 19:41:20,466 [podnet.py] => Task 2, Epoch 44/160 (LR 0.08247) => LSC_loss 0.29, Spatial_loss 3.03, Flat_loss 0.48, Train_acc 91.72, Test_acc 62.83
2023-10-17 19:41:22,475 [podnet.py] => Task 2, Epoch 45/160 (LR 0.08172) => LSC_loss 0.31, Spatial_loss 3.06, Flat_loss 0.48, Train_acc 91.78, Test_acc 58.85
2023-10-17 19:41:24,444 [podnet.py] => Task 2, Epoch 46/160 (LR 0.08095) => LSC_loss 0.33, Spatial_loss 2.98, Flat_loss 0.47, Train_acc 91.47, Test_acc 62.05
2023-10-17 19:41:26,453 [podnet.py] => Task 2, Epoch 47/160 (LR 0.08018) => LSC_loss 0.31, Spatial_loss 3.00, Flat_loss 0.47, Train_acc 91.78, Test_acc 59.82
2023-10-17 19:41:28,504 [podnet.py] => Task 2, Epoch 48/160 (LR 0.07939) => LSC_loss 0.30, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 92.11, Test_acc 61.33
2023-10-17 19:41:30,482 [podnet.py] => Task 2, Epoch 49/160 (LR 0.07859) => LSC_loss 0.30, Spatial_loss 3.00, Flat_loss 0.46, Train_acc 91.78, Test_acc 60.13
2023-10-17 19:41:32,480 [podnet.py] => Task 2, Epoch 50/160 (LR 0.07778) => LSC_loss 0.31, Spatial_loss 3.04, Flat_loss 0.48, Train_acc 91.17, Test_acc 60.95
2023-10-17 19:41:34,429 [podnet.py] => Task 2, Epoch 51/160 (LR 0.07696) => LSC_loss 0.31, Spatial_loss 3.11, Flat_loss 0.47, Train_acc 91.78, Test_acc 56.88
2023-10-17 19:41:36,414 [podnet.py] => Task 2, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.07, Flat_loss 0.48, Train_acc 91.31, Test_acc 61.58
2023-10-17 19:41:38,445 [podnet.py] => Task 2, Epoch 53/160 (LR 0.07528) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.46, Train_acc 91.94, Test_acc 57.78
2023-10-17 19:41:40,375 [podnet.py] => Task 2, Epoch 54/160 (LR 0.07443) => LSC_loss 0.30, Spatial_loss 3.02, Flat_loss 0.48, Train_acc 92.03, Test_acc 61.52
2023-10-17 19:41:42,356 [podnet.py] => Task 2, Epoch 55/160 (LR 0.07357) => LSC_loss 0.31, Spatial_loss 2.98, Flat_loss 0.46, Train_acc 92.44, Test_acc 63.27
2023-10-17 19:41:44,348 [podnet.py] => Task 2, Epoch 56/160 (LR 0.07270) => LSC_loss 0.31, Spatial_loss 3.04, Flat_loss 0.48, Train_acc 91.69, Test_acc 59.82
2023-10-17 19:41:46,327 [podnet.py] => Task 2, Epoch 57/160 (LR 0.07182) => LSC_loss 0.29, Spatial_loss 2.99, Flat_loss 0.47, Train_acc 93.03, Test_acc 59.70
2023-10-17 19:41:48,301 [podnet.py] => Task 2, Epoch 58/160 (LR 0.07093) => LSC_loss 0.26, Spatial_loss 2.91, Flat_loss 0.45, Train_acc 93.58, Test_acc 62.47
2023-10-17 19:41:50,262 [podnet.py] => Task 2, Epoch 59/160 (LR 0.07004) => LSC_loss 0.27, Spatial_loss 2.75, Flat_loss 0.42, Train_acc 93.11, Test_acc 60.88
2023-10-17 19:41:52,232 [podnet.py] => Task 2, Epoch 60/160 (LR 0.06913) => LSC_loss 0.27, Spatial_loss 2.73, Flat_loss 0.42, Train_acc 93.14, Test_acc 63.57
2023-10-17 19:41:54,276 [podnet.py] => Task 2, Epoch 61/160 (LR 0.06822) => LSC_loss 0.26, Spatial_loss 2.75, Flat_loss 0.43, Train_acc 92.64, Test_acc 62.38
2023-10-17 19:41:56,301 [podnet.py] => Task 2, Epoch 62/160 (LR 0.06731) => LSC_loss 0.25, Spatial_loss 2.75, Flat_loss 0.40, Train_acc 94.31, Test_acc 62.93
2023-10-17 19:41:58,296 [podnet.py] => Task 2, Epoch 63/160 (LR 0.06638) => LSC_loss 0.27, Spatial_loss 2.76, Flat_loss 0.42, Train_acc 93.50, Test_acc 60.05
2023-10-17 19:42:00,255 [podnet.py] => Task 2, Epoch 64/160 (LR 0.06545) => LSC_loss 0.30, Spatial_loss 2.77, Flat_loss 0.43, Train_acc 92.81, Test_acc 61.08
2023-10-17 19:42:02,228 [podnet.py] => Task 2, Epoch 65/160 (LR 0.06451) => LSC_loss 0.31, Spatial_loss 2.93, Flat_loss 0.46, Train_acc 92.25, Test_acc 59.90
2023-10-17 19:42:04,244 [podnet.py] => Task 2, Epoch 66/160 (LR 0.06357) => LSC_loss 0.31, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 91.64, Test_acc 63.70
2023-10-17 19:42:06,191 [podnet.py] => Task 2, Epoch 67/160 (LR 0.06262) => LSC_loss 0.25, Spatial_loss 2.68, Flat_loss 0.40, Train_acc 94.22, Test_acc 65.50
2023-10-17 19:42:08,216 [podnet.py] => Task 2, Epoch 68/160 (LR 0.06167) => LSC_loss 0.26, Spatial_loss 2.64, Flat_loss 0.40, Train_acc 93.78, Test_acc 64.17
2023-10-17 19:42:10,179 [podnet.py] => Task 2, Epoch 69/160 (LR 0.06072) => LSC_loss 0.25, Spatial_loss 2.66, Flat_loss 0.39, Train_acc 93.72, Test_acc 64.07
2023-10-17 19:42:12,165 [podnet.py] => Task 2, Epoch 70/160 (LR 0.05975) => LSC_loss 0.25, Spatial_loss 2.65, Flat_loss 0.39, Train_acc 93.86, Test_acc 62.13
2023-10-17 19:42:14,150 [podnet.py] => Task 2, Epoch 71/160 (LR 0.05879) => LSC_loss 0.28, Spatial_loss 2.79, Flat_loss 0.43, Train_acc 93.03, Test_acc 60.75
2023-10-17 19:42:16,121 [podnet.py] => Task 2, Epoch 72/160 (LR 0.05782) => LSC_loss 0.32, Spatial_loss 2.98, Flat_loss 0.48, Train_acc 91.50, Test_acc 61.95
2023-10-17 19:42:18,115 [podnet.py] => Task 2, Epoch 73/160 (LR 0.05685) => LSC_loss 0.34, Spatial_loss 2.95, Flat_loss 0.48, Train_acc 90.86, Test_acc 63.38
2023-10-17 19:42:20,135 [podnet.py] => Task 2, Epoch 74/160 (LR 0.05588) => LSC_loss 0.27, Spatial_loss 2.78, Flat_loss 0.43, Train_acc 93.83, Test_acc 61.37
2023-10-17 19:42:22,124 [podnet.py] => Task 2, Epoch 75/160 (LR 0.05490) => LSC_loss 0.27, Spatial_loss 2.79, Flat_loss 0.43, Train_acc 93.44, Test_acc 63.48
2023-10-17 19:42:24,122 [podnet.py] => Task 2, Epoch 76/160 (LR 0.05392) => LSC_loss 0.28, Spatial_loss 2.76, Flat_loss 0.42, Train_acc 92.75, Test_acc 62.10
2023-10-17 19:42:26,096 [podnet.py] => Task 2, Epoch 77/160 (LR 0.05294) => LSC_loss 0.26, Spatial_loss 2.68, Flat_loss 0.40, Train_acc 93.39, Test_acc 60.90
2023-10-17 19:42:28,088 [podnet.py] => Task 2, Epoch 78/160 (LR 0.05196) => LSC_loss 0.26, Spatial_loss 2.77, Flat_loss 0.41, Train_acc 93.78, Test_acc 65.68
2023-10-17 19:42:30,112 [podnet.py] => Task 2, Epoch 79/160 (LR 0.05098) => LSC_loss 0.24, Spatial_loss 2.66, Flat_loss 0.40, Train_acc 94.58, Test_acc 65.38
2023-10-17 19:42:32,100 [podnet.py] => Task 2, Epoch 80/160 (LR 0.05000) => LSC_loss 0.26, Spatial_loss 2.61, Flat_loss 0.39, Train_acc 93.47, Test_acc 64.40
2023-10-17 19:42:34,106 [podnet.py] => Task 2, Epoch 81/160 (LR 0.04902) => LSC_loss 0.25, Spatial_loss 2.63, Flat_loss 0.38, Train_acc 94.06, Test_acc 62.85
2023-10-17 19:42:36,093 [podnet.py] => Task 2, Epoch 82/160 (LR 0.04804) => LSC_loss 0.24, Spatial_loss 2.51, Flat_loss 0.38, Train_acc 94.53, Test_acc 65.28
2023-10-17 19:42:38,127 [podnet.py] => Task 2, Epoch 83/160 (LR 0.04706) => LSC_loss 0.25, Spatial_loss 2.45, Flat_loss 0.36, Train_acc 93.81, Test_acc 64.63
2023-10-17 19:42:40,141 [podnet.py] => Task 2, Epoch 84/160 (LR 0.04608) => LSC_loss 0.27, Spatial_loss 2.55, Flat_loss 0.38, Train_acc 94.14, Test_acc 63.05
2023-10-17 19:42:42,104 [podnet.py] => Task 2, Epoch 85/160 (LR 0.04510) => LSC_loss 0.26, Spatial_loss 2.52, Flat_loss 0.39, Train_acc 93.67, Test_acc 64.62
2023-10-17 19:42:44,050 [podnet.py] => Task 2, Epoch 86/160 (LR 0.04412) => LSC_loss 0.26, Spatial_loss 2.47, Flat_loss 0.37, Train_acc 94.44, Test_acc 60.10
2023-10-17 19:42:46,015 [podnet.py] => Task 2, Epoch 87/160 (LR 0.04315) => LSC_loss 0.26, Spatial_loss 2.48, Flat_loss 0.37, Train_acc 94.00, Test_acc 64.10
2023-10-17 19:42:47,991 [podnet.py] => Task 2, Epoch 88/160 (LR 0.04218) => LSC_loss 0.24, Spatial_loss 2.42, Flat_loss 0.36, Train_acc 94.17, Test_acc 65.40
2023-10-17 19:42:49,995 [podnet.py] => Task 2, Epoch 89/160 (LR 0.04121) => LSC_loss 0.24, Spatial_loss 2.50, Flat_loss 0.36, Train_acc 94.81, Test_acc 64.65
2023-10-17 19:42:52,024 [podnet.py] => Task 2, Epoch 90/160 (LR 0.04025) => LSC_loss 0.24, Spatial_loss 2.48, Flat_loss 0.37, Train_acc 94.53, Test_acc 63.92
2023-10-17 19:42:54,088 [podnet.py] => Task 2, Epoch 91/160 (LR 0.03928) => LSC_loss 0.25, Spatial_loss 2.43, Flat_loss 0.36, Train_acc 94.69, Test_acc 62.92
2023-10-17 19:42:56,110 [podnet.py] => Task 2, Epoch 92/160 (LR 0.03833) => LSC_loss 0.24, Spatial_loss 2.38, Flat_loss 0.37, Train_acc 95.06, Test_acc 66.25
2023-10-17 19:42:58,146 [podnet.py] => Task 2, Epoch 93/160 (LR 0.03738) => LSC_loss 0.23, Spatial_loss 2.35, Flat_loss 0.35, Train_acc 94.11, Test_acc 64.85
2023-10-17 19:43:00,124 [podnet.py] => Task 2, Epoch 94/160 (LR 0.03643) => LSC_loss 0.24, Spatial_loss 2.31, Flat_loss 0.34, Train_acc 94.64, Test_acc 66.17
2023-10-17 19:43:02,105 [podnet.py] => Task 2, Epoch 95/160 (LR 0.03549) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.34, Train_acc 94.72, Test_acc 65.05
2023-10-17 19:43:04,104 [podnet.py] => Task 2, Epoch 96/160 (LR 0.03455) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.34, Train_acc 94.44, Test_acc 65.78
2023-10-17 19:43:06,069 [podnet.py] => Task 2, Epoch 97/160 (LR 0.03362) => LSC_loss 0.23, Spatial_loss 2.30, Flat_loss 0.34, Train_acc 95.11, Test_acc 65.12
2023-10-17 19:43:08,032 [podnet.py] => Task 2, Epoch 98/160 (LR 0.03269) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.33, Train_acc 94.28, Test_acc 65.20
2023-10-17 19:43:09,964 [podnet.py] => Task 2, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.27, Flat_loss 0.34, Train_acc 94.08, Test_acc 64.63
2023-10-17 19:43:11,946 [podnet.py] => Task 2, Epoch 100/160 (LR 0.03087) => LSC_loss 0.26, Spatial_loss 2.29, Flat_loss 0.34, Train_acc 93.83, Test_acc 64.38
2023-10-17 19:43:13,963 [podnet.py] => Task 2, Epoch 101/160 (LR 0.02996) => LSC_loss 0.26, Spatial_loss 2.34, Flat_loss 0.35, Train_acc 94.36, Test_acc 66.07
2023-10-17 19:43:15,952 [podnet.py] => Task 2, Epoch 102/160 (LR 0.02907) => LSC_loss 0.23, Spatial_loss 2.24, Flat_loss 0.32, Train_acc 94.83, Test_acc 65.43
2023-10-17 19:43:17,922 [podnet.py] => Task 2, Epoch 103/160 (LR 0.02818) => LSC_loss 0.24, Spatial_loss 2.18, Flat_loss 0.31, Train_acc 94.50, Test_acc 64.65
2023-10-17 19:43:19,911 [podnet.py] => Task 2, Epoch 104/160 (LR 0.02730) => LSC_loss 0.25, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 94.94, Test_acc 66.50
2023-10-17 19:43:21,835 [podnet.py] => Task 2, Epoch 105/160 (LR 0.02643) => LSC_loss 0.26, Spatial_loss 2.25, Flat_loss 0.34, Train_acc 94.33, Test_acc 66.23
2023-10-17 19:43:23,826 [podnet.py] => Task 2, Epoch 106/160 (LR 0.02557) => LSC_loss 0.25, Spatial_loss 2.18, Flat_loss 0.32, Train_acc 94.17, Test_acc 67.00
2023-10-17 19:43:25,826 [podnet.py] => Task 2, Epoch 107/160 (LR 0.02472) => LSC_loss 0.24, Spatial_loss 2.22, Flat_loss 0.32, Train_acc 94.39, Test_acc 65.12
2023-10-17 19:43:27,804 [podnet.py] => Task 2, Epoch 108/160 (LR 0.02388) => LSC_loss 0.22, Spatial_loss 2.13, Flat_loss 0.31, Train_acc 95.36, Test_acc 66.50
2023-10-17 19:43:29,783 [podnet.py] => Task 2, Epoch 109/160 (LR 0.02304) => LSC_loss 0.25, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 94.89, Test_acc 65.25
2023-10-17 19:43:31,816 [podnet.py] => Task 2, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 2.20, Flat_loss 0.33, Train_acc 94.08, Test_acc 65.82
2023-10-17 19:43:33,837 [podnet.py] => Task 2, Epoch 111/160 (LR 0.02141) => LSC_loss 0.24, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 94.86, Test_acc 63.43
2023-10-17 19:43:35,842 [podnet.py] => Task 2, Epoch 112/160 (LR 0.02061) => LSC_loss 0.24, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 94.53, Test_acc 64.57
2023-10-17 19:43:37,788 [podnet.py] => Task 2, Epoch 113/160 (LR 0.01982) => LSC_loss 0.23, Spatial_loss 2.13, Flat_loss 0.31, Train_acc 95.61, Test_acc 65.53
2023-10-17 19:43:39,751 [podnet.py] => Task 2, Epoch 114/160 (LR 0.01905) => LSC_loss 0.22, Spatial_loss 2.06, Flat_loss 0.30, Train_acc 95.00, Test_acc 66.82
2023-10-17 19:43:41,717 [podnet.py] => Task 2, Epoch 115/160 (LR 0.01828) => LSC_loss 0.25, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 94.97, Test_acc 66.40
2023-10-17 19:43:43,714 [podnet.py] => Task 2, Epoch 116/160 (LR 0.01753) => LSC_loss 0.25, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 94.31, Test_acc 67.42
2023-10-17 19:43:45,776 [podnet.py] => Task 2, Epoch 117/160 (LR 0.01679) => LSC_loss 0.22, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 95.47, Test_acc 66.42
2023-10-17 19:43:47,811 [podnet.py] => Task 2, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 95.36, Test_acc 65.45
2023-10-17 19:43:49,733 [podnet.py] => Task 2, Epoch 119/160 (LR 0.01535) => LSC_loss 0.22, Spatial_loss 1.95, Flat_loss 0.30, Train_acc 95.19, Test_acc 67.25
2023-10-17 19:43:51,744 [podnet.py] => Task 2, Epoch 120/160 (LR 0.01464) => LSC_loss 0.23, Spatial_loss 1.99, Flat_loss 0.29, Train_acc 94.86, Test_acc 65.00
2023-10-17 19:43:53,727 [podnet.py] => Task 2, Epoch 121/160 (LR 0.01396) => LSC_loss 0.23, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 95.11, Test_acc 67.67
2023-10-17 19:43:55,700 [podnet.py] => Task 2, Epoch 122/160 (LR 0.01328) => LSC_loss 0.24, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 95.11, Test_acc 68.17
2023-10-17 19:43:57,677 [podnet.py] => Task 2, Epoch 123/160 (LR 0.01262) => LSC_loss 0.23, Spatial_loss 1.92, Flat_loss 0.28, Train_acc 95.86, Test_acc 66.40
2023-10-17 19:43:59,680 [podnet.py] => Task 2, Epoch 124/160 (LR 0.01198) => LSC_loss 0.23, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 95.36, Test_acc 67.42
2023-10-17 19:44:01,623 [podnet.py] => Task 2, Epoch 125/160 (LR 0.01135) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.28, Train_acc 94.67, Test_acc 66.25
2023-10-17 19:44:03,643 [podnet.py] => Task 2, Epoch 126/160 (LR 0.01073) => LSC_loss 0.24, Spatial_loss 1.88, Flat_loss 0.28, Train_acc 95.19, Test_acc 67.77
2023-10-17 19:44:05,670 [podnet.py] => Task 2, Epoch 127/160 (LR 0.01013) => LSC_loss 0.22, Spatial_loss 1.84, Flat_loss 0.27, Train_acc 95.36, Test_acc 68.20
2023-10-17 19:44:07,626 [podnet.py] => Task 2, Epoch 128/160 (LR 0.00955) => LSC_loss 0.24, Spatial_loss 1.84, Flat_loss 0.27, Train_acc 95.03, Test_acc 67.45
2023-10-17 19:44:09,616 [podnet.py] => Task 2, Epoch 129/160 (LR 0.00898) => LSC_loss 0.23, Spatial_loss 1.84, Flat_loss 0.28, Train_acc 95.17, Test_acc 67.32
2023-10-17 19:44:11,580 [podnet.py] => Task 2, Epoch 130/160 (LR 0.00843) => LSC_loss 0.22, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 96.14, Test_acc 67.17
2023-10-17 19:44:13,540 [podnet.py] => Task 2, Epoch 131/160 (LR 0.00789) => LSC_loss 0.23, Spatial_loss 1.75, Flat_loss 0.26, Train_acc 95.31, Test_acc 67.60
2023-10-17 19:44:15,527 [podnet.py] => Task 2, Epoch 132/160 (LR 0.00737) => LSC_loss 0.22, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 95.58, Test_acc 67.00
2023-10-17 19:44:17,486 [podnet.py] => Task 2, Epoch 133/160 (LR 0.00686) => LSC_loss 0.23, Spatial_loss 1.79, Flat_loss 0.27, Train_acc 95.28, Test_acc 66.83
2023-10-17 19:44:19,490 [podnet.py] => Task 2, Epoch 134/160 (LR 0.00638) => LSC_loss 0.23, Spatial_loss 1.76, Flat_loss 0.26, Train_acc 95.22, Test_acc 67.62
2023-10-17 19:44:21,524 [podnet.py] => Task 2, Epoch 135/160 (LR 0.00590) => LSC_loss 0.21, Spatial_loss 1.74, Flat_loss 0.26, Train_acc 96.03, Test_acc 67.15
2023-10-17 19:44:23,488 [podnet.py] => Task 2, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 94.83, Test_acc 67.82
2023-10-17 19:44:25,481 [podnet.py] => Task 2, Epoch 137/160 (LR 0.00501) => LSC_loss 0.22, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 96.03, Test_acc 67.72
2023-10-17 19:44:27,484 [podnet.py] => Task 2, Epoch 138/160 (LR 0.00459) => LSC_loss 0.22, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 95.69, Test_acc 66.97
2023-10-17 19:44:29,460 [podnet.py] => Task 2, Epoch 139/160 (LR 0.00419) => LSC_loss 0.22, Spatial_loss 1.72, Flat_loss 0.25, Train_acc 95.36, Test_acc 67.57
2023-10-17 19:44:31,477 [podnet.py] => Task 2, Epoch 140/160 (LR 0.00381) => LSC_loss 0.22, Spatial_loss 1.70, Flat_loss 0.26, Train_acc 95.97, Test_acc 68.05
2023-10-17 19:44:33,455 [podnet.py] => Task 2, Epoch 141/160 (LR 0.00344) => LSC_loss 0.21, Spatial_loss 1.66, Flat_loss 0.25, Train_acc 95.47, Test_acc 67.90
2023-10-17 19:44:35,443 [podnet.py] => Task 2, Epoch 142/160 (LR 0.00309) => LSC_loss 0.22, Spatial_loss 1.67, Flat_loss 0.25, Train_acc 95.81, Test_acc 68.23
2023-10-17 19:44:37,396 [podnet.py] => Task 2, Epoch 143/160 (LR 0.00276) => LSC_loss 0.21, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.67, Test_acc 67.65
2023-10-17 19:44:39,430 [podnet.py] => Task 2, Epoch 144/160 (LR 0.00245) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.25, Train_acc 96.03, Test_acc 68.18
2023-10-17 19:44:41,425 [podnet.py] => Task 2, Epoch 145/160 (LR 0.00215) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.24, Train_acc 95.78, Test_acc 68.05
2023-10-17 19:44:43,401 [podnet.py] => Task 2, Epoch 146/160 (LR 0.00188) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.14, Test_acc 67.82
2023-10-17 19:44:45,430 [podnet.py] => Task 2, Epoch 147/160 (LR 0.00162) => LSC_loss 0.22, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.39, Test_acc 68.17
2023-10-17 19:44:47,430 [podnet.py] => Task 2, Epoch 148/160 (LR 0.00138) => LSC_loss 0.21, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.92, Test_acc 68.02
2023-10-17 19:44:49,455 [podnet.py] => Task 2, Epoch 149/160 (LR 0.00116) => LSC_loss 0.24, Spatial_loss 1.68, Flat_loss 0.26, Train_acc 95.78, Test_acc 67.48
2023-10-17 19:44:51,447 [podnet.py] => Task 2, Epoch 150/160 (LR 0.00096) => LSC_loss 0.22, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 95.81, Test_acc 67.95
2023-10-17 19:44:53,458 [podnet.py] => Task 2, Epoch 151/160 (LR 0.00078) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.24, Train_acc 96.69, Test_acc 68.37
2023-10-17 19:44:55,413 [podnet.py] => Task 2, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 1.60, Flat_loss 0.24, Train_acc 96.06, Test_acc 68.00
2023-10-17 19:44:57,401 [podnet.py] => Task 2, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 95.39, Test_acc 68.07
2023-10-17 19:44:59,391 [podnet.py] => Task 2, Epoch 154/160 (LR 0.00035) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.92, Test_acc 67.30
2023-10-17 19:45:01,393 [podnet.py] => Task 2, Epoch 155/160 (LR 0.00024) => LSC_loss 0.22, Spatial_loss 1.57, Flat_loss 0.24, Train_acc 95.97, Test_acc 67.87
2023-10-17 19:45:03,362 [podnet.py] => Task 2, Epoch 156/160 (LR 0.00015) => LSC_loss 0.21, Spatial_loss 1.57, Flat_loss 0.24, Train_acc 96.47, Test_acc 67.72
2023-10-17 19:45:05,315 [podnet.py] => Task 2, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.92, Test_acc 68.13
2023-10-17 19:45:07,364 [podnet.py] => Task 2, Epoch 158/160 (LR 0.00004) => LSC_loss 0.21, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 96.03, Test_acc 67.62
2023-10-17 19:45:09,337 [podnet.py] => Task 2, Epoch 159/160 (LR 0.00001) => LSC_loss 0.21, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 96.53, Test_acc 67.73
2023-10-17 19:45:11,403 [podnet.py] => Task 2, Epoch 160/160 (LR 0.00000) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.24, Train_acc 95.64, Test_acc 67.68
2023-10-17 19:45:11,404 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 19:45:11,404 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:45:36,113 [podnet.py] => The size of finetune dataset: 1200
2023-10-17 19:45:37,566 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 95.33, Test_acc 67.03
2023-10-17 19:45:38,983 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.08, Spatial_loss 1.83, Flat_loss 0.18, Train_acc 98.83, Test_acc 67.88
2023-10-17 19:45:40,388 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.06, Spatial_loss 1.69, Flat_loss 0.13, Train_acc 98.92, Test_acc 68.13
2023-10-17 19:45:41,761 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.06, Spatial_loss 1.57, Flat_loss 0.11, Train_acc 99.33, Test_acc 68.63
2023-10-17 19:45:43,160 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.06, Spatial_loss 1.61, Flat_loss 0.11, Train_acc 99.08, Test_acc 69.08
2023-10-17 19:45:44,572 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.07, Spatial_loss 1.57, Flat_loss 0.10, Train_acc 98.75, Test_acc 69.58
2023-10-17 19:45:45,964 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.06, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 98.92, Test_acc 69.20
2023-10-17 19:45:47,393 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.06, Spatial_loss 1.54, Flat_loss 0.11, Train_acc 99.50, Test_acc 69.43
2023-10-17 19:45:48,849 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.07, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 98.67, Test_acc 69.20
2023-10-17 19:45:50,267 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.05, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 98.75, Test_acc 69.35
2023-10-17 19:45:51,696 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 98.83, Test_acc 69.22
2023-10-17 19:45:53,033 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.05, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 99.25, Test_acc 69.35
2023-10-17 19:45:54,437 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.09, Train_acc 99.17, Test_acc 69.93
2023-10-17 19:45:55,820 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.06, Spatial_loss 1.46, Flat_loss 0.10, Train_acc 98.92, Test_acc 69.55
2023-10-17 19:45:57,223 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.05, Spatial_loss 1.44, Flat_loss 0.10, Train_acc 99.00, Test_acc 69.73
2023-10-17 19:45:58,580 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 98.67, Test_acc 69.58
2023-10-17 19:46:00,004 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.06, Spatial_loss 1.49, Flat_loss 0.10, Train_acc 99.25, Test_acc 69.58
2023-10-17 19:46:01,459 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.05, Spatial_loss 1.52, Flat_loss 0.10, Train_acc 99.33, Test_acc 69.37
2023-10-17 19:46:02,838 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.07, Spatial_loss 1.44, Flat_loss 0.10, Train_acc 98.50, Test_acc 69.58
2023-10-17 19:46:04,214 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 99.17, Test_acc 69.58
2023-10-17 19:46:04,216 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:46:30,168 [podnet.py] => Exemplar size: 1200
2023-10-17 19:46:30,169 [trainer.py] => CNN: {'total': 69.58, '00-09': 74.9, '10-19': 64.1, '20-29': 75.4, '30-39': 70.3, '40-49': 72.9, '50-59': 59.9, 'old': 71.36, 'new': 50.0}
2023-10-17 19:46:30,169 [trainer.py] => NME: {'total': 69.3, '00-09': 76.2, '10-19': 66.4, '20-29': 75.4, '30-39': 70.3, '40-49': 73.8, '50-59': 53.7, 'old': 71.51, 'new': 45.0}
2023-10-17 19:46:30,169 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58]
2023-10-17 19:46:30,169 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27]
2023-10-17 19:46:30,169 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3]
2023-10-17 19:46:30,169 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18]

2023-10-17 19:46:30,169 [trainer.py] => Average Accuracy (CNN): 73.65666666666665
2023-10-17 19:46:30,169 [trainer.py] => Average Accuracy (NME): 73.36666666666667
2023-10-17 19:46:30,169 [trainer.py] => All params: 504657
2023-10-17 19:46:30,170 [trainer.py] => Trainable params: 504657
2023-10-17 19:46:30,170 [podnet.py] => Learning on 60-65
2023-10-17 19:46:30,200 [podnet.py] => Adaptive factor: 3.605551275463989
2023-10-17 19:46:32,289 [podnet.py] => Task 3, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 3.77, Flat_loss 0.80, Train_acc 66.65, Test_acc 44.95
2023-10-17 19:46:34,298 [podnet.py] => Task 3, Epoch 2/160 (LR 0.09996) => LSC_loss 0.64, Spatial_loss 3.79, Flat_loss 0.59, Train_acc 82.57, Test_acc 57.15
2023-10-17 19:46:36,332 [podnet.py] => Task 3, Epoch 3/160 (LR 0.09991) => LSC_loss 0.48, Spatial_loss 3.42, Flat_loss 0.47, Train_acc 86.43, Test_acc 57.82
2023-10-17 19:46:38,314 [podnet.py] => Task 3, Epoch 4/160 (LR 0.09985) => LSC_loss 0.43, Spatial_loss 3.24, Flat_loss 0.42, Train_acc 88.46, Test_acc 58.57
2023-10-17 19:46:40,307 [podnet.py] => Task 3, Epoch 5/160 (LR 0.09976) => LSC_loss 0.41, Spatial_loss 3.10, Flat_loss 0.39, Train_acc 89.59, Test_acc 54.05
2023-10-17 19:46:42,329 [podnet.py] => Task 3, Epoch 6/160 (LR 0.09965) => LSC_loss 0.40, Spatial_loss 3.07, Flat_loss 0.38, Train_acc 89.22, Test_acc 59.94
2023-10-17 19:46:44,369 [podnet.py] => Task 3, Epoch 7/160 (LR 0.09953) => LSC_loss 0.35, Spatial_loss 2.96, Flat_loss 0.36, Train_acc 90.43, Test_acc 59.28
2023-10-17 19:46:46,335 [podnet.py] => Task 3, Epoch 8/160 (LR 0.09938) => LSC_loss 0.34, Spatial_loss 2.99, Flat_loss 0.36, Train_acc 91.35, Test_acc 56.20
2023-10-17 19:46:48,356 [podnet.py] => Task 3, Epoch 9/160 (LR 0.09922) => LSC_loss 0.33, Spatial_loss 2.97, Flat_loss 0.35, Train_acc 91.78, Test_acc 58.51
2023-10-17 19:46:50,355 [podnet.py] => Task 3, Epoch 10/160 (LR 0.09904) => LSC_loss 0.34, Spatial_loss 2.91, Flat_loss 0.34, Train_acc 91.51, Test_acc 56.75
2023-10-17 19:46:52,376 [podnet.py] => Task 3, Epoch 11/160 (LR 0.09884) => LSC_loss 0.31, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 92.32, Test_acc 54.80
2023-10-17 19:46:54,418 [podnet.py] => Task 3, Epoch 12/160 (LR 0.09862) => LSC_loss 0.32, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 92.08, Test_acc 55.22
2023-10-17 19:46:56,523 [podnet.py] => Task 3, Epoch 13/160 (LR 0.09838) => LSC_loss 0.30, Spatial_loss 2.81, Flat_loss 0.33, Train_acc 92.76, Test_acc 58.60
2023-10-17 19:46:58,560 [podnet.py] => Task 3, Epoch 14/160 (LR 0.09812) => LSC_loss 0.30, Spatial_loss 2.87, Flat_loss 0.32, Train_acc 92.41, Test_acc 57.85
2023-10-17 19:47:00,623 [podnet.py] => Task 3, Epoch 15/160 (LR 0.09785) => LSC_loss 0.29, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 92.81, Test_acc 57.20
2023-10-17 19:47:02,647 [podnet.py] => Task 3, Epoch 16/160 (LR 0.09755) => LSC_loss 0.29, Spatial_loss 2.88, Flat_loss 0.33, Train_acc 93.03, Test_acc 55.26
2023-10-17 19:47:04,632 [podnet.py] => Task 3, Epoch 17/160 (LR 0.09724) => LSC_loss 0.28, Spatial_loss 2.82, Flat_loss 0.32, Train_acc 93.00, Test_acc 54.02
2023-10-17 19:47:06,628 [podnet.py] => Task 3, Epoch 18/160 (LR 0.09691) => LSC_loss 0.27, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.78, Test_acc 55.92
2023-10-17 19:47:08,671 [podnet.py] => Task 3, Epoch 19/160 (LR 0.09656) => LSC_loss 0.27, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.89, Test_acc 58.18
2023-10-17 19:47:10,656 [podnet.py] => Task 3, Epoch 20/160 (LR 0.09619) => LSC_loss 0.26, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 93.78, Test_acc 59.92
2023-10-17 19:47:12,627 [podnet.py] => Task 3, Epoch 21/160 (LR 0.09581) => LSC_loss 0.27, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 93.76, Test_acc 57.49
2023-10-17 19:47:14,595 [podnet.py] => Task 3, Epoch 22/160 (LR 0.09541) => LSC_loss 0.27, Spatial_loss 2.81, Flat_loss 0.31, Train_acc 94.03, Test_acc 56.94
2023-10-17 19:47:16,608 [podnet.py] => Task 3, Epoch 23/160 (LR 0.09499) => LSC_loss 0.26, Spatial_loss 2.77, Flat_loss 0.31, Train_acc 93.78, Test_acc 59.91
2023-10-17 19:47:18,628 [podnet.py] => Task 3, Epoch 24/160 (LR 0.09455) => LSC_loss 0.26, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 93.76, Test_acc 58.52
2023-10-17 19:47:20,684 [podnet.py] => Task 3, Epoch 25/160 (LR 0.09410) => LSC_loss 0.27, Spatial_loss 2.82, Flat_loss 0.31, Train_acc 93.59, Test_acc 56.97
2023-10-17 19:47:22,719 [podnet.py] => Task 3, Epoch 26/160 (LR 0.09362) => LSC_loss 0.25, Spatial_loss 2.79, Flat_loss 0.31, Train_acc 95.00, Test_acc 53.57
2023-10-17 19:47:24,767 [podnet.py] => Task 3, Epoch 27/160 (LR 0.09314) => LSC_loss 0.26, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.81, Test_acc 58.32
2023-10-17 19:47:26,732 [podnet.py] => Task 3, Epoch 28/160 (LR 0.09263) => LSC_loss 0.25, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 94.38, Test_acc 59.75
2023-10-17 19:47:28,758 [podnet.py] => Task 3, Epoch 29/160 (LR 0.09211) => LSC_loss 0.25, Spatial_loss 2.72, Flat_loss 0.30, Train_acc 94.84, Test_acc 56.29
2023-10-17 19:47:30,807 [podnet.py] => Task 3, Epoch 30/160 (LR 0.09157) => LSC_loss 0.24, Spatial_loss 2.67, Flat_loss 0.30, Train_acc 94.51, Test_acc 54.03
2023-10-17 19:47:32,821 [podnet.py] => Task 3, Epoch 31/160 (LR 0.09102) => LSC_loss 0.26, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 94.00, Test_acc 57.35
2023-10-17 19:47:34,896 [podnet.py] => Task 3, Epoch 32/160 (LR 0.09045) => LSC_loss 0.27, Spatial_loss 2.82, Flat_loss 0.32, Train_acc 93.14, Test_acc 59.14
2023-10-17 19:47:36,870 [podnet.py] => Task 3, Epoch 33/160 (LR 0.08987) => LSC_loss 0.24, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 94.03, Test_acc 60.17
2023-10-17 19:47:38,919 [podnet.py] => Task 3, Epoch 34/160 (LR 0.08927) => LSC_loss 0.24, Spatial_loss 2.66, Flat_loss 0.29, Train_acc 94.92, Test_acc 59.62
2023-10-17 19:47:40,895 [podnet.py] => Task 3, Epoch 35/160 (LR 0.08865) => LSC_loss 0.24, Spatial_loss 2.65, Flat_loss 0.29, Train_acc 94.84, Test_acc 58.85
2023-10-17 19:47:42,974 [podnet.py] => Task 3, Epoch 36/160 (LR 0.08802) => LSC_loss 0.25, Spatial_loss 2.69, Flat_loss 0.30, Train_acc 94.24, Test_acc 57.52
2023-10-17 19:47:44,981 [podnet.py] => Task 3, Epoch 37/160 (LR 0.08738) => LSC_loss 0.26, Spatial_loss 2.65, Flat_loss 0.30, Train_acc 93.49, Test_acc 59.06
2023-10-17 19:47:46,925 [podnet.py] => Task 3, Epoch 38/160 (LR 0.08672) => LSC_loss 0.23, Spatial_loss 2.72, Flat_loss 0.30, Train_acc 95.00, Test_acc 61.34
2023-10-17 19:47:48,972 [podnet.py] => Task 3, Epoch 39/160 (LR 0.08604) => LSC_loss 0.24, Spatial_loss 2.56, Flat_loss 0.28, Train_acc 94.62, Test_acc 58.23
2023-10-17 19:47:50,971 [podnet.py] => Task 3, Epoch 40/160 (LR 0.08536) => LSC_loss 0.23, Spatial_loss 2.59, Flat_loss 0.28, Train_acc 94.70, Test_acc 61.51
2023-10-17 19:47:53,009 [podnet.py] => Task 3, Epoch 41/160 (LR 0.08465) => LSC_loss 0.24, Spatial_loss 2.64, Flat_loss 0.29, Train_acc 94.38, Test_acc 59.38
2023-10-17 19:47:55,039 [podnet.py] => Task 3, Epoch 42/160 (LR 0.08394) => LSC_loss 0.24, Spatial_loss 2.61, Flat_loss 0.29, Train_acc 94.43, Test_acc 58.62
2023-10-17 19:47:56,992 [podnet.py] => Task 3, Epoch 43/160 (LR 0.08321) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 94.92, Test_acc 56.75
2023-10-17 19:47:58,974 [podnet.py] => Task 3, Epoch 44/160 (LR 0.08247) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 94.59, Test_acc 60.22
2023-10-17 19:48:01,007 [podnet.py] => Task 3, Epoch 45/160 (LR 0.08172) => LSC_loss 0.24, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 94.78, Test_acc 59.31
2023-10-17 19:48:03,010 [podnet.py] => Task 3, Epoch 46/160 (LR 0.08095) => LSC_loss 0.22, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 95.78, Test_acc 57.31
2023-10-17 19:48:05,024 [podnet.py] => Task 3, Epoch 47/160 (LR 0.08018) => LSC_loss 0.23, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.11, Test_acc 55.00
2023-10-17 19:48:06,997 [podnet.py] => Task 3, Epoch 48/160 (LR 0.07939) => LSC_loss 0.23, Spatial_loss 2.66, Flat_loss 0.29, Train_acc 94.95, Test_acc 57.69
2023-10-17 19:48:08,961 [podnet.py] => Task 3, Epoch 49/160 (LR 0.07859) => LSC_loss 0.24, Spatial_loss 2.63, Flat_loss 0.29, Train_acc 94.68, Test_acc 58.23
2023-10-17 19:48:10,957 [podnet.py] => Task 3, Epoch 50/160 (LR 0.07778) => LSC_loss 0.23, Spatial_loss 2.59, Flat_loss 0.28, Train_acc 94.81, Test_acc 61.09
2023-10-17 19:48:13,000 [podnet.py] => Task 3, Epoch 51/160 (LR 0.07696) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.27, Train_acc 95.46, Test_acc 58.62
2023-10-17 19:48:15,021 [podnet.py] => Task 3, Epoch 52/160 (LR 0.07612) => LSC_loss 0.22, Spatial_loss 2.57, Flat_loss 0.28, Train_acc 95.03, Test_acc 58.00
2023-10-17 19:48:17,003 [podnet.py] => Task 3, Epoch 53/160 (LR 0.07528) => LSC_loss 0.22, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 94.97, Test_acc 59.52
2023-10-17 19:48:18,996 [podnet.py] => Task 3, Epoch 54/160 (LR 0.07443) => LSC_loss 0.22, Spatial_loss 2.57, Flat_loss 0.27, Train_acc 95.30, Test_acc 57.54
2023-10-17 19:48:20,924 [podnet.py] => Task 3, Epoch 55/160 (LR 0.07357) => LSC_loss 0.22, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 95.16, Test_acc 60.57
2023-10-17 19:48:22,927 [podnet.py] => Task 3, Epoch 56/160 (LR 0.07270) => LSC_loss 0.23, Spatial_loss 2.56, Flat_loss 0.28, Train_acc 94.89, Test_acc 59.57
2023-10-17 19:48:24,926 [podnet.py] => Task 3, Epoch 57/160 (LR 0.07182) => LSC_loss 0.23, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.03, Test_acc 56.69
2023-10-17 19:48:26,943 [podnet.py] => Task 3, Epoch 58/160 (LR 0.07093) => LSC_loss 0.22, Spatial_loss 2.46, Flat_loss 0.26, Train_acc 95.24, Test_acc 58.66
2023-10-17 19:48:28,928 [podnet.py] => Task 3, Epoch 59/160 (LR 0.07004) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.26, Train_acc 95.92, Test_acc 60.65
2023-10-17 19:48:30,888 [podnet.py] => Task 3, Epoch 60/160 (LR 0.06913) => LSC_loss 0.22, Spatial_loss 2.42, Flat_loss 0.26, Train_acc 95.68, Test_acc 57.78
2023-10-17 19:48:32,902 [podnet.py] => Task 3, Epoch 61/160 (LR 0.06822) => LSC_loss 0.23, Spatial_loss 2.49, Flat_loss 0.26, Train_acc 95.03, Test_acc 58.28
2023-10-17 19:48:34,921 [podnet.py] => Task 3, Epoch 62/160 (LR 0.06731) => LSC_loss 0.23, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 94.51, Test_acc 56.92
2023-10-17 19:48:36,933 [podnet.py] => Task 3, Epoch 63/160 (LR 0.06638) => LSC_loss 0.21, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.76, Test_acc 59.71
2023-10-17 19:48:38,986 [podnet.py] => Task 3, Epoch 64/160 (LR 0.06545) => LSC_loss 0.22, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 95.14, Test_acc 59.78
2023-10-17 19:48:41,019 [podnet.py] => Task 3, Epoch 65/160 (LR 0.06451) => LSC_loss 0.21, Spatial_loss 2.37, Flat_loss 0.25, Train_acc 95.81, Test_acc 60.29
2023-10-17 19:48:43,135 [podnet.py] => Task 3, Epoch 66/160 (LR 0.06357) => LSC_loss 0.22, Spatial_loss 2.33, Flat_loss 0.25, Train_acc 95.38, Test_acc 56.74
2023-10-17 19:48:45,227 [podnet.py] => Task 3, Epoch 67/160 (LR 0.06262) => LSC_loss 0.22, Spatial_loss 2.35, Flat_loss 0.25, Train_acc 95.27, Test_acc 60.40
2023-10-17 19:48:47,244 [podnet.py] => Task 3, Epoch 68/160 (LR 0.06167) => LSC_loss 0.21, Spatial_loss 2.32, Flat_loss 0.25, Train_acc 95.32, Test_acc 60.40
2023-10-17 19:48:49,235 [podnet.py] => Task 3, Epoch 69/160 (LR 0.06072) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.24, Train_acc 94.62, Test_acc 59.55
2023-10-17 19:48:51,301 [podnet.py] => Task 3, Epoch 70/160 (LR 0.05975) => LSC_loss 0.23, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 95.08, Test_acc 59.28
2023-10-17 19:48:53,323 [podnet.py] => Task 3, Epoch 71/160 (LR 0.05879) => LSC_loss 0.21, Spatial_loss 2.35, Flat_loss 0.25, Train_acc 95.49, Test_acc 60.06
2023-10-17 19:48:55,343 [podnet.py] => Task 3, Epoch 72/160 (LR 0.05782) => LSC_loss 0.21, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 95.57, Test_acc 61.74
2023-10-17 19:48:57,403 [podnet.py] => Task 3, Epoch 73/160 (LR 0.05685) => LSC_loss 0.21, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 95.62, Test_acc 58.98
2023-10-17 19:48:59,500 [podnet.py] => Task 3, Epoch 74/160 (LR 0.05588) => LSC_loss 0.21, Spatial_loss 2.33, Flat_loss 0.24, Train_acc 95.54, Test_acc 56.32
2023-10-17 19:49:01,526 [podnet.py] => Task 3, Epoch 75/160 (LR 0.05490) => LSC_loss 0.21, Spatial_loss 2.32, Flat_loss 0.24, Train_acc 95.54, Test_acc 60.91
2023-10-17 19:49:03,561 [podnet.py] => Task 3, Epoch 76/160 (LR 0.05392) => LSC_loss 0.20, Spatial_loss 2.28, Flat_loss 0.24, Train_acc 96.38, Test_acc 61.51
2023-10-17 19:49:05,598 [podnet.py] => Task 3, Epoch 77/160 (LR 0.05294) => LSC_loss 0.21, Spatial_loss 2.28, Flat_loss 0.24, Train_acc 95.62, Test_acc 60.46
2023-10-17 19:49:07,629 [podnet.py] => Task 3, Epoch 78/160 (LR 0.05196) => LSC_loss 0.21, Spatial_loss 2.21, Flat_loss 0.23, Train_acc 95.46, Test_acc 60.49
2023-10-17 19:49:09,719 [podnet.py] => Task 3, Epoch 79/160 (LR 0.05098) => LSC_loss 0.21, Spatial_loss 2.18, Flat_loss 0.23, Train_acc 95.43, Test_acc 59.86
2023-10-17 19:49:11,760 [podnet.py] => Task 3, Epoch 80/160 (LR 0.05000) => LSC_loss 0.21, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 95.84, Test_acc 63.52
2023-10-17 19:49:13,810 [podnet.py] => Task 3, Epoch 81/160 (LR 0.04902) => LSC_loss 0.22, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 95.32, Test_acc 60.08
2023-10-17 19:49:15,927 [podnet.py] => Task 3, Epoch 82/160 (LR 0.04804) => LSC_loss 0.21, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 95.62, Test_acc 61.77
2023-10-17 19:49:17,981 [podnet.py] => Task 3, Epoch 83/160 (LR 0.04706) => LSC_loss 0.20, Spatial_loss 2.18, Flat_loss 0.22, Train_acc 96.24, Test_acc 63.37
2023-10-17 19:49:20,082 [podnet.py] => Task 3, Epoch 84/160 (LR 0.04608) => LSC_loss 0.21, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 95.46, Test_acc 61.68
2023-10-17 19:49:22,076 [podnet.py] => Task 3, Epoch 85/160 (LR 0.04510) => LSC_loss 0.22, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 95.41, Test_acc 62.43
2023-10-17 19:49:24,206 [podnet.py] => Task 3, Epoch 86/160 (LR 0.04412) => LSC_loss 0.21, Spatial_loss 2.17, Flat_loss 0.22, Train_acc 96.24, Test_acc 58.66
2023-10-17 19:49:26,241 [podnet.py] => Task 3, Epoch 87/160 (LR 0.04315) => LSC_loss 0.21, Spatial_loss 2.11, Flat_loss 0.22, Train_acc 96.00, Test_acc 62.02
2023-10-17 19:49:28,288 [podnet.py] => Task 3, Epoch 88/160 (LR 0.04218) => LSC_loss 0.21, Spatial_loss 2.16, Flat_loss 0.22, Train_acc 95.70, Test_acc 60.32
2023-10-17 19:49:30,358 [podnet.py] => Task 3, Epoch 89/160 (LR 0.04121) => LSC_loss 0.21, Spatial_loss 2.06, Flat_loss 0.21, Train_acc 95.84, Test_acc 60.49
2023-10-17 19:49:32,352 [podnet.py] => Task 3, Epoch 90/160 (LR 0.04025) => LSC_loss 0.20, Spatial_loss 2.03, Flat_loss 0.21, Train_acc 96.16, Test_acc 60.26
2023-10-17 19:49:34,321 [podnet.py] => Task 3, Epoch 91/160 (LR 0.03928) => LSC_loss 0.21, Spatial_loss 2.02, Flat_loss 0.21, Train_acc 95.86, Test_acc 62.00
2023-10-17 19:49:36,249 [podnet.py] => Task 3, Epoch 92/160 (LR 0.03833) => LSC_loss 0.20, Spatial_loss 2.00, Flat_loss 0.21, Train_acc 96.22, Test_acc 59.35
2023-10-17 19:49:38,188 [podnet.py] => Task 3, Epoch 93/160 (LR 0.03738) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.97, Test_acc 63.60
2023-10-17 19:49:40,192 [podnet.py] => Task 3, Epoch 94/160 (LR 0.03643) => LSC_loss 0.20, Spatial_loss 2.00, Flat_loss 0.20, Train_acc 96.05, Test_acc 61.68
2023-10-17 19:49:42,177 [podnet.py] => Task 3, Epoch 95/160 (LR 0.03549) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 96.32, Test_acc 63.31
2023-10-17 19:49:44,158 [podnet.py] => Task 3, Epoch 96/160 (LR 0.03455) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.95, Test_acc 61.02
2023-10-17 19:49:46,187 [podnet.py] => Task 3, Epoch 97/160 (LR 0.03362) => LSC_loss 0.20, Spatial_loss 1.91, Flat_loss 0.20, Train_acc 96.14, Test_acc 61.20
2023-10-17 19:49:48,190 [podnet.py] => Task 3, Epoch 98/160 (LR 0.03269) => LSC_loss 0.20, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 96.27, Test_acc 61.83
2023-10-17 19:49:50,160 [podnet.py] => Task 3, Epoch 99/160 (LR 0.03178) => LSC_loss 0.21, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 95.92, Test_acc 61.62
2023-10-17 19:49:52,153 [podnet.py] => Task 3, Epoch 100/160 (LR 0.03087) => LSC_loss 0.20, Spatial_loss 1.91, Flat_loss 0.19, Train_acc 96.41, Test_acc 61.42
2023-10-17 19:49:54,158 [podnet.py] => Task 3, Epoch 101/160 (LR 0.02996) => LSC_loss 0.20, Spatial_loss 1.90, Flat_loss 0.19, Train_acc 96.00, Test_acc 61.09
2023-10-17 19:49:56,169 [podnet.py] => Task 3, Epoch 102/160 (LR 0.02907) => LSC_loss 0.20, Spatial_loss 1.94, Flat_loss 0.19, Train_acc 96.03, Test_acc 62.23
2023-10-17 19:49:58,173 [podnet.py] => Task 3, Epoch 103/160 (LR 0.02818) => LSC_loss 0.20, Spatial_loss 1.85, Flat_loss 0.19, Train_acc 96.14, Test_acc 60.42
2023-10-17 19:50:00,284 [podnet.py] => Task 3, Epoch 104/160 (LR 0.02730) => LSC_loss 0.21, Spatial_loss 1.84, Flat_loss 0.18, Train_acc 96.03, Test_acc 61.55
2023-10-17 19:50:02,348 [podnet.py] => Task 3, Epoch 105/160 (LR 0.02643) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 95.59, Test_acc 62.94
2023-10-17 19:50:04,306 [podnet.py] => Task 3, Epoch 106/160 (LR 0.02557) => LSC_loss 0.20, Spatial_loss 1.81, Flat_loss 0.18, Train_acc 96.38, Test_acc 62.94
2023-10-17 19:50:06,354 [podnet.py] => Task 3, Epoch 107/160 (LR 0.02472) => LSC_loss 0.20, Spatial_loss 1.84, Flat_loss 0.18, Train_acc 96.65, Test_acc 63.71
2023-10-17 19:50:08,330 [podnet.py] => Task 3, Epoch 108/160 (LR 0.02388) => LSC_loss 0.20, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 96.08, Test_acc 63.46
2023-10-17 19:50:10,382 [podnet.py] => Task 3, Epoch 109/160 (LR 0.02304) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 96.00, Test_acc 63.25
2023-10-17 19:50:12,382 [podnet.py] => Task 3, Epoch 110/160 (LR 0.02222) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 96.30, Test_acc 63.40
2023-10-17 19:50:14,406 [podnet.py] => Task 3, Epoch 111/160 (LR 0.02141) => LSC_loss 0.20, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 96.43, Test_acc 62.68
2023-10-17 19:50:16,446 [podnet.py] => Task 3, Epoch 112/160 (LR 0.02061) => LSC_loss 0.20, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 96.24, Test_acc 61.37
2023-10-17 19:50:18,506 [podnet.py] => Task 3, Epoch 113/160 (LR 0.01982) => LSC_loss 0.20, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 96.08, Test_acc 64.71
2023-10-17 19:50:20,505 [podnet.py] => Task 3, Epoch 114/160 (LR 0.01905) => LSC_loss 0.21, Spatial_loss 1.75, Flat_loss 0.17, Train_acc 95.59, Test_acc 62.23
2023-10-17 19:50:22,476 [podnet.py] => Task 3, Epoch 115/160 (LR 0.01828) => LSC_loss 0.21, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 95.89, Test_acc 60.98
2023-10-17 19:50:24,484 [podnet.py] => Task 3, Epoch 116/160 (LR 0.01753) => LSC_loss 0.19, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 96.73, Test_acc 62.00
2023-10-17 19:50:26,570 [podnet.py] => Task 3, Epoch 117/160 (LR 0.01679) => LSC_loss 0.20, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 96.35, Test_acc 62.86
2023-10-17 19:50:28,667 [podnet.py] => Task 3, Epoch 118/160 (LR 0.01606) => LSC_loss 0.20, Spatial_loss 1.67, Flat_loss 0.16, Train_acc 96.51, Test_acc 61.80
2023-10-17 19:50:30,704 [podnet.py] => Task 3, Epoch 119/160 (LR 0.01535) => LSC_loss 0.20, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 96.24, Test_acc 64.22
2023-10-17 19:50:32,690 [podnet.py] => Task 3, Epoch 120/160 (LR 0.01464) => LSC_loss 0.21, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 95.81, Test_acc 62.48
2023-10-17 19:50:34,668 [podnet.py] => Task 3, Epoch 121/160 (LR 0.01396) => LSC_loss 0.19, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 96.68, Test_acc 62.49
2023-10-17 19:50:36,612 [podnet.py] => Task 3, Epoch 122/160 (LR 0.01328) => LSC_loss 0.20, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 96.30, Test_acc 63.60
2023-10-17 19:50:38,638 [podnet.py] => Task 3, Epoch 123/160 (LR 0.01262) => LSC_loss 0.20, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 96.22, Test_acc 64.46
2023-10-17 19:50:40,653 [podnet.py] => Task 3, Epoch 124/160 (LR 0.01198) => LSC_loss 0.20, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 96.70, Test_acc 65.37
2023-10-17 19:50:42,670 [podnet.py] => Task 3, Epoch 125/160 (LR 0.01135) => LSC_loss 0.20, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 96.24, Test_acc 62.51
2023-10-17 19:50:44,751 [podnet.py] => Task 3, Epoch 126/160 (LR 0.01073) => LSC_loss 0.20, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 96.22, Test_acc 63.57
2023-10-17 19:50:46,760 [podnet.py] => Task 3, Epoch 127/160 (LR 0.01013) => LSC_loss 0.20, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 96.76, Test_acc 63.85
2023-10-17 19:50:48,812 [podnet.py] => Task 3, Epoch 128/160 (LR 0.00955) => LSC_loss 0.20, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 96.78, Test_acc 62.54
2023-10-17 19:50:50,799 [podnet.py] => Task 3, Epoch 129/160 (LR 0.00898) => LSC_loss 0.20, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 96.57, Test_acc 63.88
2023-10-17 19:50:52,793 [podnet.py] => Task 3, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.95, Test_acc 62.49
2023-10-17 19:50:54,815 [podnet.py] => Task 3, Epoch 131/160 (LR 0.00789) => LSC_loss 0.19, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 96.30, Test_acc 62.86
2023-10-17 19:50:56,830 [podnet.py] => Task 3, Epoch 132/160 (LR 0.00737) => LSC_loss 0.19, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 96.59, Test_acc 64.58
2023-10-17 19:50:58,838 [podnet.py] => Task 3, Epoch 133/160 (LR 0.00686) => LSC_loss 0.20, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 96.35, Test_acc 64.51
2023-10-17 19:51:00,938 [podnet.py] => Task 3, Epoch 134/160 (LR 0.00638) => LSC_loss 0.20, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 96.43, Test_acc 64.14
2023-10-17 19:51:02,886 [podnet.py] => Task 3, Epoch 135/160 (LR 0.00590) => LSC_loss 0.20, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 96.73, Test_acc 63.62
2023-10-17 19:51:04,883 [podnet.py] => Task 3, Epoch 136/160 (LR 0.00545) => LSC_loss 0.20, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 96.62, Test_acc 64.02
2023-10-17 19:51:06,876 [podnet.py] => Task 3, Epoch 137/160 (LR 0.00501) => LSC_loss 0.20, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 96.35, Test_acc 63.15
2023-10-17 19:51:08,904 [podnet.py] => Task 3, Epoch 138/160 (LR 0.00459) => LSC_loss 0.20, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 96.54, Test_acc 64.20
2023-10-17 19:51:10,865 [podnet.py] => Task 3, Epoch 139/160 (LR 0.00419) => LSC_loss 0.21, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 96.68, Test_acc 63.98
2023-10-17 19:51:12,891 [podnet.py] => Task 3, Epoch 140/160 (LR 0.00381) => LSC_loss 0.19, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 96.97, Test_acc 64.14
2023-10-17 19:51:14,901 [podnet.py] => Task 3, Epoch 141/160 (LR 0.00344) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 97.08, Test_acc 64.32
2023-10-17 19:51:16,892 [podnet.py] => Task 3, Epoch 142/160 (LR 0.00309) => LSC_loss 0.20, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 96.57, Test_acc 64.05
2023-10-17 19:51:18,928 [podnet.py] => Task 3, Epoch 143/160 (LR 0.00276) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 96.38, Test_acc 64.12
2023-10-17 19:51:20,944 [podnet.py] => Task 3, Epoch 144/160 (LR 0.00245) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 96.54, Test_acc 64.46
2023-10-17 19:51:22,907 [podnet.py] => Task 3, Epoch 145/160 (LR 0.00215) => LSC_loss 0.20, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 96.57, Test_acc 64.03
2023-10-17 19:51:24,899 [podnet.py] => Task 3, Epoch 146/160 (LR 0.00188) => LSC_loss 0.21, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 96.16, Test_acc 63.91
2023-10-17 19:51:26,901 [podnet.py] => Task 3, Epoch 147/160 (LR 0.00162) => LSC_loss 0.20, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 95.95, Test_acc 64.38
2023-10-17 19:51:28,910 [podnet.py] => Task 3, Epoch 148/160 (LR 0.00138) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.11, Test_acc 64.20
2023-10-17 19:51:30,874 [podnet.py] => Task 3, Epoch 149/160 (LR 0.00116) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.65, Test_acc 64.26
2023-10-17 19:51:32,903 [podnet.py] => Task 3, Epoch 150/160 (LR 0.00096) => LSC_loss 0.19, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 96.86, Test_acc 64.55
2023-10-17 19:51:34,938 [podnet.py] => Task 3, Epoch 151/160 (LR 0.00078) => LSC_loss 0.20, Spatial_loss 1.29, Flat_loss 0.13, Train_acc 96.84, Test_acc 64.49
2023-10-17 19:51:36,966 [podnet.py] => Task 3, Epoch 152/160 (LR 0.00062) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.51, Test_acc 64.46
2023-10-17 19:51:39,003 [podnet.py] => Task 3, Epoch 153/160 (LR 0.00047) => LSC_loss 0.20, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 96.49, Test_acc 64.34
2023-10-17 19:51:41,053 [podnet.py] => Task 3, Epoch 154/160 (LR 0.00035) => LSC_loss 0.20, Spatial_loss 1.30, Flat_loss 0.14, Train_acc 96.70, Test_acc 64.34
2023-10-17 19:51:43,042 [podnet.py] => Task 3, Epoch 155/160 (LR 0.00024) => LSC_loss 0.20, Spatial_loss 1.29, Flat_loss 0.14, Train_acc 96.84, Test_acc 64.46
2023-10-17 19:51:45,032 [podnet.py] => Task 3, Epoch 156/160 (LR 0.00015) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.24, Test_acc 64.46
2023-10-17 19:51:47,039 [podnet.py] => Task 3, Epoch 157/160 (LR 0.00009) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.41, Test_acc 64.32
2023-10-17 19:51:49,032 [podnet.py] => Task 3, Epoch 158/160 (LR 0.00004) => LSC_loss 0.19, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 96.84, Test_acc 64.35
2023-10-17 19:51:51,076 [podnet.py] => Task 3, Epoch 159/160 (LR 0.00001) => LSC_loss 0.20, Spatial_loss 1.26, Flat_loss 0.13, Train_acc 96.54, Test_acc 64.45
2023-10-17 19:51:53,095 [podnet.py] => Task 3, Epoch 160/160 (LR 0.00000) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.32, Test_acc 64.37
2023-10-17 19:51:53,095 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 19:51:53,095 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:52:19,918 [podnet.py] => The size of finetune dataset: 1300
2023-10-17 19:52:21,403 [podnet.py] => Task 3, Epoch 1/20 (LR 0.00497) => LSC_loss 0.17, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.31, Test_acc 64.11
2023-10-17 19:52:22,891 [podnet.py] => Task 3, Epoch 2/20 (LR 0.00488) => LSC_loss 0.08, Spatial_loss 1.57, Flat_loss 0.11, Train_acc 99.00, Test_acc 65.43
2023-10-17 19:52:24,372 [podnet.py] => Task 3, Epoch 3/20 (LR 0.00473) => LSC_loss 0.08, Spatial_loss 1.50, Flat_loss 0.09, Train_acc 99.00, Test_acc 65.65
2023-10-17 19:52:25,842 [podnet.py] => Task 3, Epoch 4/20 (LR 0.00452) => LSC_loss 0.08, Spatial_loss 1.51, Flat_loss 0.09, Train_acc 98.46, Test_acc 66.05
2023-10-17 19:52:27,286 [podnet.py] => Task 3, Epoch 5/20 (LR 0.00427) => LSC_loss 0.07, Spatial_loss 1.47, Flat_loss 0.08, Train_acc 99.15, Test_acc 66.20
2023-10-17 19:52:28,780 [podnet.py] => Task 3, Epoch 6/20 (LR 0.00397) => LSC_loss 0.06, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 98.92, Test_acc 65.66
2023-10-17 19:52:30,232 [podnet.py] => Task 3, Epoch 7/20 (LR 0.00363) => LSC_loss 0.09, Spatial_loss 1.43, Flat_loss 0.08, Train_acc 99.00, Test_acc 66.18
2023-10-17 19:52:31,755 [podnet.py] => Task 3, Epoch 8/20 (LR 0.00327) => LSC_loss 0.08, Spatial_loss 1.46, Flat_loss 0.08, Train_acc 99.15, Test_acc 66.05
2023-10-17 19:52:33,252 [podnet.py] => Task 3, Epoch 9/20 (LR 0.00289) => LSC_loss 0.07, Spatial_loss 1.35, Flat_loss 0.07, Train_acc 98.85, Test_acc 65.82
2023-10-17 19:52:34,648 [podnet.py] => Task 3, Epoch 10/20 (LR 0.00250) => LSC_loss 0.06, Spatial_loss 1.38, Flat_loss 0.07, Train_acc 99.15, Test_acc 66.20
2023-10-17 19:52:36,079 [podnet.py] => Task 3, Epoch 11/20 (LR 0.00211) => LSC_loss 0.07, Spatial_loss 1.41, Flat_loss 0.08, Train_acc 99.46, Test_acc 66.00
2023-10-17 19:52:37,538 [podnet.py] => Task 3, Epoch 12/20 (LR 0.00173) => LSC_loss 0.06, Spatial_loss 1.30, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.18
2023-10-17 19:52:38,957 [podnet.py] => Task 3, Epoch 13/20 (LR 0.00137) => LSC_loss 0.07, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 98.77, Test_acc 66.40
2023-10-17 19:52:40,381 [podnet.py] => Task 3, Epoch 14/20 (LR 0.00103) => LSC_loss 0.06, Spatial_loss 1.42, Flat_loss 0.08, Train_acc 99.38, Test_acc 66.20
2023-10-17 19:52:41,866 [podnet.py] => Task 3, Epoch 15/20 (LR 0.00073) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.07, Train_acc 99.31, Test_acc 65.77
2023-10-17 19:52:43,329 [podnet.py] => Task 3, Epoch 16/20 (LR 0.00048) => LSC_loss 0.07, Spatial_loss 1.30, Flat_loss 0.07, Train_acc 98.77, Test_acc 66.00
2023-10-17 19:52:44,757 [podnet.py] => Task 3, Epoch 17/20 (LR 0.00027) => LSC_loss 0.06, Spatial_loss 1.38, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.05
2023-10-17 19:52:46,211 [podnet.py] => Task 3, Epoch 18/20 (LR 0.00012) => LSC_loss 0.08, Spatial_loss 1.35, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.03
2023-10-17 19:52:47,663 [podnet.py] => Task 3, Epoch 19/20 (LR 0.00003) => LSC_loss 0.07, Spatial_loss 1.34, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.26
2023-10-17 19:52:49,168 [podnet.py] => Task 3, Epoch 20/20 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 99.31, Test_acc 66.06
2023-10-17 19:52:49,171 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:53:17,412 [podnet.py] => Exemplar size: 1300
2023-10-17 19:53:17,412 [trainer.py] => CNN: {'total': 66.06, '00-09': 72.8, '10-19': 60.4, '20-29': 73.9, '30-39': 66.7, '40-49': 68.5, '50-59': 55.6, '60-69': 63.0, 'old': 66.32, 'new': 63.0}
2023-10-17 19:53:17,412 [trainer.py] => NME: {'total': 66.14, '00-09': 75.5, '10-19': 63.9, '20-29': 74.8, '30-39': 68.1, '40-49': 70.7, '50-59': 50.1, '60-69': 53.6, 'old': 67.18, 'new': 53.6}
2023-10-17 19:53:17,412 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06]
2023-10-17 19:53:17,413 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49]
2023-10-17 19:53:17,413 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14]
2023-10-17 19:53:17,413 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22]

2023-10-17 19:53:17,413 [trainer.py] => Average Accuracy (CNN): 71.7575
2023-10-17 19:53:17,413 [trainer.py] => Average Accuracy (NME): 71.56
2023-10-17 19:53:17,413 [trainer.py] => All params: 507857
2023-10-17 19:53:17,413 [trainer.py] => Trainable params: 507857
2023-10-17 19:53:17,414 [podnet.py] => Learning on 65-70
2023-10-17 19:53:17,446 [podnet.py] => Adaptive factor: 3.7416573867739413
2023-10-17 19:53:19,560 [podnet.py] => Task 4, Epoch 1/160 (LR 0.09999) => LSC_loss 1.66, Spatial_loss 4.04, Flat_loss 0.82, Train_acc 69.32, Test_acc 46.57
2023-10-17 19:53:21,628 [podnet.py] => Task 4, Epoch 2/160 (LR 0.09996) => LSC_loss 0.61, Spatial_loss 4.18, Flat_loss 0.64, Train_acc 83.08, Test_acc 44.96
2023-10-17 19:53:23,718 [podnet.py] => Task 4, Epoch 3/160 (LR 0.09991) => LSC_loss 0.47, Spatial_loss 3.81, Flat_loss 0.52, Train_acc 87.76, Test_acc 53.10
2023-10-17 19:53:25,792 [podnet.py] => Task 4, Epoch 4/160 (LR 0.09985) => LSC_loss 0.38, Spatial_loss 3.57, Flat_loss 0.44, Train_acc 90.18, Test_acc 51.56
2023-10-17 19:53:27,851 [podnet.py] => Task 4, Epoch 5/160 (LR 0.09976) => LSC_loss 0.34, Spatial_loss 3.36, Flat_loss 0.40, Train_acc 91.50, Test_acc 53.60
2023-10-17 19:53:29,890 [podnet.py] => Task 4, Epoch 6/160 (LR 0.09965) => LSC_loss 0.31, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 92.32, Test_acc 52.09
2023-10-17 19:53:31,977 [podnet.py] => Task 4, Epoch 7/160 (LR 0.09953) => LSC_loss 0.28, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 93.24, Test_acc 52.17
2023-10-17 19:53:34,054 [podnet.py] => Task 4, Epoch 8/160 (LR 0.09938) => LSC_loss 0.31, Spatial_loss 3.11, Flat_loss 0.34, Train_acc 91.95, Test_acc 53.27
2023-10-17 19:53:36,032 [podnet.py] => Task 4, Epoch 9/160 (LR 0.09922) => LSC_loss 0.28, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 93.11, Test_acc 59.43
2023-10-17 19:53:38,115 [podnet.py] => Task 4, Epoch 10/160 (LR 0.09904) => LSC_loss 0.27, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 94.13, Test_acc 56.69
2023-10-17 19:53:40,227 [podnet.py] => Task 4, Epoch 11/160 (LR 0.09884) => LSC_loss 0.27, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 93.95, Test_acc 54.64
2023-10-17 19:53:42,345 [podnet.py] => Task 4, Epoch 12/160 (LR 0.09862) => LSC_loss 0.27, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 93.61, Test_acc 53.10
2023-10-17 19:53:44,389 [podnet.py] => Task 4, Epoch 13/160 (LR 0.09838) => LSC_loss 0.26, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 94.11, Test_acc 55.57
2023-10-17 19:53:46,557 [podnet.py] => Task 4, Epoch 14/160 (LR 0.09812) => LSC_loss 0.26, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 94.05, Test_acc 55.73
2023-10-17 19:53:48,664 [podnet.py] => Task 4, Epoch 15/160 (LR 0.09785) => LSC_loss 0.27, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 93.42, Test_acc 51.77
2023-10-17 19:53:50,752 [podnet.py] => Task 4, Epoch 16/160 (LR 0.09755) => LSC_loss 0.25, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 94.18, Test_acc 57.51
2023-10-17 19:53:52,867 [podnet.py] => Task 4, Epoch 17/160 (LR 0.09724) => LSC_loss 0.26, Spatial_loss 2.96, Flat_loss 0.30, Train_acc 94.03, Test_acc 55.37
2023-10-17 19:53:54,964 [podnet.py] => Task 4, Epoch 18/160 (LR 0.09691) => LSC_loss 0.24, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 94.29, Test_acc 53.57
2023-10-17 19:53:57,017 [podnet.py] => Task 4, Epoch 19/160 (LR 0.09656) => LSC_loss 0.25, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 94.26, Test_acc 54.81
2023-10-17 19:53:59,090 [podnet.py] => Task 4, Epoch 20/160 (LR 0.09619) => LSC_loss 0.24, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 94.53, Test_acc 56.93
2023-10-17 19:54:01,135 [podnet.py] => Task 4, Epoch 21/160 (LR 0.09581) => LSC_loss 0.23, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 95.29, Test_acc 56.83
2023-10-17 19:54:03,156 [podnet.py] => Task 4, Epoch 22/160 (LR 0.09541) => LSC_loss 0.24, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 94.47, Test_acc 53.77
2023-10-17 19:54:05,233 [podnet.py] => Task 4, Epoch 23/160 (LR 0.09499) => LSC_loss 0.25, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 94.53, Test_acc 52.87
2023-10-17 19:54:07,266 [podnet.py] => Task 4, Epoch 24/160 (LR 0.09455) => LSC_loss 0.23, Spatial_loss 2.86, Flat_loss 0.29, Train_acc 94.97, Test_acc 55.06
2023-10-17 19:54:09,344 [podnet.py] => Task 4, Epoch 25/160 (LR 0.09410) => LSC_loss 0.24, Spatial_loss 2.88, Flat_loss 0.29, Train_acc 94.18, Test_acc 51.10
2023-10-17 19:54:11,466 [podnet.py] => Task 4, Epoch 26/160 (LR 0.09362) => LSC_loss 0.23, Spatial_loss 2.81, Flat_loss 0.28, Train_acc 95.34, Test_acc 53.59
2023-10-17 19:54:13,561 [podnet.py] => Task 4, Epoch 27/160 (LR 0.09314) => LSC_loss 0.22, Spatial_loss 2.76, Flat_loss 0.27, Train_acc 95.42, Test_acc 51.97
2023-10-17 19:54:15,624 [podnet.py] => Task 4, Epoch 28/160 (LR 0.09263) => LSC_loss 0.23, Spatial_loss 2.79, Flat_loss 0.28, Train_acc 95.29, Test_acc 55.24
2023-10-17 19:54:17,681 [podnet.py] => Task 4, Epoch 29/160 (LR 0.09211) => LSC_loss 0.23, Spatial_loss 2.80, Flat_loss 0.28, Train_acc 94.92, Test_acc 50.23
2023-10-17 19:54:19,705 [podnet.py] => Task 4, Epoch 30/160 (LR 0.09157) => LSC_loss 0.23, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 94.84, Test_acc 53.03
2023-10-17 19:54:21,756 [podnet.py] => Task 4, Epoch 31/160 (LR 0.09102) => LSC_loss 0.22, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 95.55, Test_acc 54.40
2023-10-17 19:54:23,865 [podnet.py] => Task 4, Epoch 32/160 (LR 0.09045) => LSC_loss 0.22, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 94.74, Test_acc 54.63
2023-10-17 19:54:25,895 [podnet.py] => Task 4, Epoch 33/160 (LR 0.08987) => LSC_loss 0.22, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 95.34, Test_acc 56.76
2023-10-17 19:54:27,949 [podnet.py] => Task 4, Epoch 34/160 (LR 0.08927) => LSC_loss 0.22, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 95.00, Test_acc 54.10
2023-10-17 19:54:29,991 [podnet.py] => Task 4, Epoch 35/160 (LR 0.08865) => LSC_loss 0.22, Spatial_loss 2.78, Flat_loss 0.27, Train_acc 95.00, Test_acc 57.69
2023-10-17 19:54:32,048 [podnet.py] => Task 4, Epoch 36/160 (LR 0.08802) => LSC_loss 0.22, Spatial_loss 2.71, Flat_loss 0.28, Train_acc 95.45, Test_acc 57.89
2023-10-17 19:54:34,085 [podnet.py] => Task 4, Epoch 37/160 (LR 0.08738) => LSC_loss 0.21, Spatial_loss 2.67, Flat_loss 0.27, Train_acc 95.71, Test_acc 56.56
2023-10-17 19:54:36,165 [podnet.py] => Task 4, Epoch 38/160 (LR 0.08672) => LSC_loss 0.21, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 95.11, Test_acc 54.49
2023-10-17 19:54:38,239 [podnet.py] => Task 4, Epoch 39/160 (LR 0.08604) => LSC_loss 0.22, Spatial_loss 2.72, Flat_loss 0.27, Train_acc 95.21, Test_acc 58.00
2023-10-17 19:54:40,275 [podnet.py] => Task 4, Epoch 40/160 (LR 0.08536) => LSC_loss 0.21, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 95.42, Test_acc 57.30
2023-10-17 19:54:42,357 [podnet.py] => Task 4, Epoch 41/160 (LR 0.08465) => LSC_loss 0.22, Spatial_loss 2.70, Flat_loss 0.27, Train_acc 95.08, Test_acc 58.26
2023-10-17 19:54:44,424 [podnet.py] => Task 4, Epoch 42/160 (LR 0.08394) => LSC_loss 0.21, Spatial_loss 2.69, Flat_loss 0.26, Train_acc 95.87, Test_acc 54.16
2023-10-17 19:54:46,445 [podnet.py] => Task 4, Epoch 43/160 (LR 0.08321) => LSC_loss 0.22, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 95.03, Test_acc 55.19
2023-10-17 19:54:48,480 [podnet.py] => Task 4, Epoch 44/160 (LR 0.08247) => LSC_loss 0.21, Spatial_loss 2.67, Flat_loss 0.27, Train_acc 95.61, Test_acc 56.07
2023-10-17 19:54:50,503 [podnet.py] => Task 4, Epoch 45/160 (LR 0.08172) => LSC_loss 0.21, Spatial_loss 2.68, Flat_loss 0.26, Train_acc 95.68, Test_acc 51.54
2023-10-17 19:54:52,586 [podnet.py] => Task 4, Epoch 46/160 (LR 0.08095) => LSC_loss 0.22, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 95.37, Test_acc 55.13
2023-10-17 19:54:54,676 [podnet.py] => Task 4, Epoch 47/160 (LR 0.08018) => LSC_loss 0.21, Spatial_loss 2.58, Flat_loss 0.26, Train_acc 95.61, Test_acc 56.39
2023-10-17 19:54:56,783 [podnet.py] => Task 4, Epoch 48/160 (LR 0.07939) => LSC_loss 0.21, Spatial_loss 2.66, Flat_loss 0.26, Train_acc 95.76, Test_acc 59.81
2023-10-17 19:54:58,825 [podnet.py] => Task 4, Epoch 49/160 (LR 0.07859) => LSC_loss 0.21, Spatial_loss 2.63, Flat_loss 0.26, Train_acc 95.63, Test_acc 52.24
2023-10-17 19:55:00,873 [podnet.py] => Task 4, Epoch 50/160 (LR 0.07778) => LSC_loss 0.20, Spatial_loss 2.65, Flat_loss 0.26, Train_acc 96.16, Test_acc 57.19
2023-10-17 19:55:02,965 [podnet.py] => Task 4, Epoch 51/160 (LR 0.07696) => LSC_loss 0.21, Spatial_loss 2.63, Flat_loss 0.26, Train_acc 95.66, Test_acc 55.01
2023-10-17 19:55:05,067 [podnet.py] => Task 4, Epoch 52/160 (LR 0.07612) => LSC_loss 0.21, Spatial_loss 2.64, Flat_loss 0.26, Train_acc 95.08, Test_acc 56.87
2023-10-17 19:55:07,127 [podnet.py] => Task 4, Epoch 53/160 (LR 0.07528) => LSC_loss 0.22, Spatial_loss 2.60, Flat_loss 0.26, Train_acc 95.24, Test_acc 54.30
2023-10-17 19:55:09,211 [podnet.py] => Task 4, Epoch 54/160 (LR 0.07443) => LSC_loss 0.20, Spatial_loss 2.58, Flat_loss 0.25, Train_acc 95.74, Test_acc 55.91
2023-10-17 19:55:11,275 [podnet.py] => Task 4, Epoch 55/160 (LR 0.07357) => LSC_loss 0.20, Spatial_loss 2.46, Flat_loss 0.24, Train_acc 95.97, Test_acc 55.51
2023-10-17 19:55:13,309 [podnet.py] => Task 4, Epoch 56/160 (LR 0.07270) => LSC_loss 0.21, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 95.58, Test_acc 54.30
2023-10-17 19:55:15,399 [podnet.py] => Task 4, Epoch 57/160 (LR 0.07182) => LSC_loss 0.20, Spatial_loss 2.50, Flat_loss 0.24, Train_acc 95.71, Test_acc 54.69
2023-10-17 19:55:17,433 [podnet.py] => Task 4, Epoch 58/160 (LR 0.07093) => LSC_loss 0.19, Spatial_loss 2.51, Flat_loss 0.24, Train_acc 95.95, Test_acc 53.59
2023-10-17 19:55:19,464 [podnet.py] => Task 4, Epoch 59/160 (LR 0.07004) => LSC_loss 0.21, Spatial_loss 2.51, Flat_loss 0.24, Train_acc 95.66, Test_acc 57.91
2023-10-17 19:55:21,551 [podnet.py] => Task 4, Epoch 60/160 (LR 0.06913) => LSC_loss 0.20, Spatial_loss 2.52, Flat_loss 0.24, Train_acc 96.18, Test_acc 57.84
2023-10-17 19:55:23,611 [podnet.py] => Task 4, Epoch 61/160 (LR 0.06822) => LSC_loss 0.21, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 95.68, Test_acc 51.06
2023-10-17 19:55:25,650 [podnet.py] => Task 4, Epoch 62/160 (LR 0.06731) => LSC_loss 0.21, Spatial_loss 2.52, Flat_loss 0.24, Train_acc 95.61, Test_acc 56.16
2023-10-17 19:55:27,681 [podnet.py] => Task 4, Epoch 63/160 (LR 0.06638) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.24, Train_acc 95.50, Test_acc 55.56
2023-10-17 19:55:29,761 [podnet.py] => Task 4, Epoch 64/160 (LR 0.06545) => LSC_loss 0.20, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 96.24, Test_acc 55.53
2023-10-17 19:55:31,862 [podnet.py] => Task 4, Epoch 65/160 (LR 0.06451) => LSC_loss 0.20, Spatial_loss 2.41, Flat_loss 0.22, Train_acc 96.05, Test_acc 55.90
2023-10-17 19:55:33,975 [podnet.py] => Task 4, Epoch 66/160 (LR 0.06357) => LSC_loss 0.20, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 95.84, Test_acc 60.51
2023-10-17 19:55:36,012 [podnet.py] => Task 4, Epoch 67/160 (LR 0.06262) => LSC_loss 0.19, Spatial_loss 2.40, Flat_loss 0.23, Train_acc 96.47, Test_acc 57.66
2023-10-17 19:55:38,084 [podnet.py] => Task 4, Epoch 68/160 (LR 0.06167) => LSC_loss 0.19, Spatial_loss 2.32, Flat_loss 0.22, Train_acc 96.26, Test_acc 57.04
2023-10-17 19:55:40,145 [podnet.py] => Task 4, Epoch 69/160 (LR 0.06072) => LSC_loss 0.20, Spatial_loss 2.37, Flat_loss 0.23, Train_acc 95.71, Test_acc 55.80
2023-10-17 19:55:42,239 [podnet.py] => Task 4, Epoch 70/160 (LR 0.05975) => LSC_loss 0.19, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 96.32, Test_acc 55.67
2023-10-17 19:55:44,326 [podnet.py] => Task 4, Epoch 71/160 (LR 0.05879) => LSC_loss 0.19, Spatial_loss 2.36, Flat_loss 0.22, Train_acc 96.24, Test_acc 58.01
2023-10-17 19:55:46,359 [podnet.py] => Task 4, Epoch 72/160 (LR 0.05782) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.63, Test_acc 55.19
2023-10-17 19:55:48,442 [podnet.py] => Task 4, Epoch 73/160 (LR 0.05685) => LSC_loss 0.20, Spatial_loss 2.40, Flat_loss 0.22, Train_acc 96.03, Test_acc 59.63
2023-10-17 19:55:50,469 [podnet.py] => Task 4, Epoch 74/160 (LR 0.05588) => LSC_loss 0.19, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 96.21, Test_acc 56.01
2023-10-17 19:55:52,492 [podnet.py] => Task 4, Epoch 75/160 (LR 0.05490) => LSC_loss 0.19, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 96.55, Test_acc 57.60
2023-10-17 19:55:54,609 [podnet.py] => Task 4, Epoch 76/160 (LR 0.05392) => LSC_loss 0.20, Spatial_loss 2.25, Flat_loss 0.21, Train_acc 95.55, Test_acc 58.03
2023-10-17 19:55:56,669 [podnet.py] => Task 4, Epoch 77/160 (LR 0.05294) => LSC_loss 0.18, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 96.87, Test_acc 56.07
2023-10-17 19:55:58,716 [podnet.py] => Task 4, Epoch 78/160 (LR 0.05196) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.21, Test_acc 56.09
2023-10-17 19:56:00,842 [podnet.py] => Task 4, Epoch 79/160 (LR 0.05098) => LSC_loss 0.20, Spatial_loss 2.31, Flat_loss 0.22, Train_acc 95.84, Test_acc 57.99
2023-10-17 19:56:02,846 [podnet.py] => Task 4, Epoch 80/160 (LR 0.05000) => LSC_loss 0.19, Spatial_loss 2.28, Flat_loss 0.21, Train_acc 96.00, Test_acc 58.61
2023-10-17 19:56:04,926 [podnet.py] => Task 4, Epoch 81/160 (LR 0.04902) => LSC_loss 0.18, Spatial_loss 2.24, Flat_loss 0.20, Train_acc 96.50, Test_acc 54.50
2023-10-17 19:56:07,016 [podnet.py] => Task 4, Epoch 82/160 (LR 0.04804) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.03, Test_acc 57.07
2023-10-17 19:56:09,055 [podnet.py] => Task 4, Epoch 83/160 (LR 0.04706) => LSC_loss 0.19, Spatial_loss 2.23, Flat_loss 0.20, Train_acc 96.61, Test_acc 59.07
2023-10-17 19:56:11,154 [podnet.py] => Task 4, Epoch 84/160 (LR 0.04608) => LSC_loss 0.19, Spatial_loss 2.20, Flat_loss 0.20, Train_acc 96.13, Test_acc 59.87
2023-10-17 19:56:13,208 [podnet.py] => Task 4, Epoch 85/160 (LR 0.04510) => LSC_loss 0.18, Spatial_loss 2.21, Flat_loss 0.20, Train_acc 96.55, Test_acc 55.10
2023-10-17 19:56:15,301 [podnet.py] => Task 4, Epoch 86/160 (LR 0.04412) => LSC_loss 0.20, Spatial_loss 2.16, Flat_loss 0.19, Train_acc 96.18, Test_acc 56.33
2023-10-17 19:56:17,283 [podnet.py] => Task 4, Epoch 87/160 (LR 0.04315) => LSC_loss 0.18, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 97.08, Test_acc 57.86
2023-10-17 19:56:19,315 [podnet.py] => Task 4, Epoch 88/160 (LR 0.04218) => LSC_loss 0.19, Spatial_loss 2.11, Flat_loss 0.19, Train_acc 96.34, Test_acc 56.83
2023-10-17 19:56:21,345 [podnet.py] => Task 4, Epoch 89/160 (LR 0.04121) => LSC_loss 0.19, Spatial_loss 2.15, Flat_loss 0.19, Train_acc 96.24, Test_acc 57.21
2023-10-17 19:56:23,383 [podnet.py] => Task 4, Epoch 90/160 (LR 0.04025) => LSC_loss 0.19, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 96.34, Test_acc 58.20
2023-10-17 19:56:25,482 [podnet.py] => Task 4, Epoch 91/160 (LR 0.03928) => LSC_loss 0.18, Spatial_loss 2.08, Flat_loss 0.19, Train_acc 96.84, Test_acc 60.99
2023-10-17 19:56:27,569 [podnet.py] => Task 4, Epoch 92/160 (LR 0.03833) => LSC_loss 0.19, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 96.16, Test_acc 56.49
2023-10-17 19:56:29,676 [podnet.py] => Task 4, Epoch 93/160 (LR 0.03738) => LSC_loss 0.19, Spatial_loss 2.01, Flat_loss 0.18, Train_acc 96.45, Test_acc 58.97
2023-10-17 19:56:31,708 [podnet.py] => Task 4, Epoch 94/160 (LR 0.03643) => LSC_loss 0.19, Spatial_loss 2.02, Flat_loss 0.19, Train_acc 95.89, Test_acc 59.14
2023-10-17 19:56:33,748 [podnet.py] => Task 4, Epoch 95/160 (LR 0.03549) => LSC_loss 0.18, Spatial_loss 2.04, Flat_loss 0.18, Train_acc 96.66, Test_acc 60.00
2023-10-17 19:56:35,803 [podnet.py] => Task 4, Epoch 96/160 (LR 0.03455) => LSC_loss 0.19, Spatial_loss 2.03, Flat_loss 0.18, Train_acc 96.16, Test_acc 59.81
2023-10-17 19:56:37,886 [podnet.py] => Task 4, Epoch 97/160 (LR 0.03362) => LSC_loss 0.19, Spatial_loss 2.01, Flat_loss 0.18, Train_acc 96.32, Test_acc 58.41
2023-10-17 19:56:39,937 [podnet.py] => Task 4, Epoch 98/160 (LR 0.03269) => LSC_loss 0.18, Spatial_loss 1.99, Flat_loss 0.18, Train_acc 96.71, Test_acc 59.51
2023-10-17 19:56:41,952 [podnet.py] => Task 4, Epoch 99/160 (LR 0.03178) => LSC_loss 0.17, Spatial_loss 1.98, Flat_loss 0.17, Train_acc 96.92, Test_acc 59.50
2023-10-17 19:56:44,033 [podnet.py] => Task 4, Epoch 100/160 (LR 0.03087) => LSC_loss 0.18, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 96.53, Test_acc 59.69
2023-10-17 19:56:46,160 [podnet.py] => Task 4, Epoch 101/160 (LR 0.02996) => LSC_loss 0.19, Spatial_loss 1.93, Flat_loss 0.17, Train_acc 96.37, Test_acc 58.10
2023-10-17 19:56:48,222 [podnet.py] => Task 4, Epoch 102/160 (LR 0.02907) => LSC_loss 0.18, Spatial_loss 1.97, Flat_loss 0.17, Train_acc 96.84, Test_acc 58.40
2023-10-17 19:56:50,298 [podnet.py] => Task 4, Epoch 103/160 (LR 0.02818) => LSC_loss 0.18, Spatial_loss 1.99, Flat_loss 0.17, Train_acc 96.58, Test_acc 57.80
2023-10-17 19:56:52,370 [podnet.py] => Task 4, Epoch 104/160 (LR 0.02730) => LSC_loss 0.18, Spatial_loss 1.90, Flat_loss 0.17, Train_acc 96.61, Test_acc 59.27
2023-10-17 19:56:54,452 [podnet.py] => Task 4, Epoch 105/160 (LR 0.02643) => LSC_loss 0.18, Spatial_loss 1.89, Flat_loss 0.17, Train_acc 96.87, Test_acc 60.60
2023-10-17 19:56:56,522 [podnet.py] => Task 4, Epoch 106/160 (LR 0.02557) => LSC_loss 0.19, Spatial_loss 1.83, Flat_loss 0.16, Train_acc 96.53, Test_acc 59.60
2023-10-17 19:56:58,523 [podnet.py] => Task 4, Epoch 107/160 (LR 0.02472) => LSC_loss 0.18, Spatial_loss 1.80, Flat_loss 0.15, Train_acc 96.89, Test_acc 59.27
2023-10-17 19:57:00,615 [podnet.py] => Task 4, Epoch 108/160 (LR 0.02388) => LSC_loss 0.17, Spatial_loss 1.82, Flat_loss 0.16, Train_acc 97.08, Test_acc 60.11
2023-10-17 19:57:02,666 [podnet.py] => Task 4, Epoch 109/160 (LR 0.02304) => LSC_loss 0.18, Spatial_loss 1.82, Flat_loss 0.16, Train_acc 97.37, Test_acc 58.80
2023-10-17 19:57:04,763 [podnet.py] => Task 4, Epoch 110/160 (LR 0.02222) => LSC_loss 0.18, Spatial_loss 1.73, Flat_loss 0.15, Train_acc 96.87, Test_acc 59.16
2023-10-17 19:57:06,792 [podnet.py] => Task 4, Epoch 111/160 (LR 0.02141) => LSC_loss 0.18, Spatial_loss 1.77, Flat_loss 0.15, Train_acc 97.11, Test_acc 57.74
2023-10-17 19:57:08,880 [podnet.py] => Task 4, Epoch 112/160 (LR 0.02061) => LSC_loss 0.18, Spatial_loss 1.74, Flat_loss 0.15, Train_acc 97.00, Test_acc 59.39
2023-10-17 19:57:10,958 [podnet.py] => Task 4, Epoch 113/160 (LR 0.01982) => LSC_loss 0.18, Spatial_loss 1.75, Flat_loss 0.15, Train_acc 96.61, Test_acc 57.46
2023-10-17 19:57:13,004 [podnet.py] => Task 4, Epoch 114/160 (LR 0.01905) => LSC_loss 0.18, Spatial_loss 1.79, Flat_loss 0.15, Train_acc 96.71, Test_acc 58.90
2023-10-17 19:57:15,105 [podnet.py] => Task 4, Epoch 115/160 (LR 0.01828) => LSC_loss 0.18, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 96.89, Test_acc 61.37
2023-10-17 19:57:17,178 [podnet.py] => Task 4, Epoch 116/160 (LR 0.01753) => LSC_loss 0.18, Spatial_loss 1.70, Flat_loss 0.15, Train_acc 96.74, Test_acc 57.94
2023-10-17 19:57:19,255 [podnet.py] => Task 4, Epoch 117/160 (LR 0.01679) => LSC_loss 0.19, Spatial_loss 1.69, Flat_loss 0.15, Train_acc 96.74, Test_acc 58.34
2023-10-17 19:57:21,331 [podnet.py] => Task 4, Epoch 118/160 (LR 0.01606) => LSC_loss 0.19, Spatial_loss 1.74, Flat_loss 0.15, Train_acc 96.47, Test_acc 59.81
2023-10-17 19:57:23,427 [podnet.py] => Task 4, Epoch 119/160 (LR 0.01535) => LSC_loss 0.18, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 96.87, Test_acc 61.56
2023-10-17 19:57:25,436 [podnet.py] => Task 4, Epoch 120/160 (LR 0.01464) => LSC_loss 0.18, Spatial_loss 1.66, Flat_loss 0.14, Train_acc 97.00, Test_acc 60.51
2023-10-17 19:57:27,499 [podnet.py] => Task 4, Epoch 121/160 (LR 0.01396) => LSC_loss 0.18, Spatial_loss 1.65, Flat_loss 0.14, Train_acc 97.29, Test_acc 59.47
2023-10-17 19:57:29,493 [podnet.py] => Task 4, Epoch 122/160 (LR 0.01328) => LSC_loss 0.17, Spatial_loss 1.58, Flat_loss 0.14, Train_acc 97.34, Test_acc 59.81
2023-10-17 19:57:31,634 [podnet.py] => Task 4, Epoch 123/160 (LR 0.01262) => LSC_loss 0.18, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 97.11, Test_acc 61.09
2023-10-17 19:57:33,669 [podnet.py] => Task 4, Epoch 124/160 (LR 0.01198) => LSC_loss 0.19, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 96.79, Test_acc 59.69
2023-10-17 19:57:35,719 [podnet.py] => Task 4, Epoch 125/160 (LR 0.01135) => LSC_loss 0.18, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 96.95, Test_acc 59.97
2023-10-17 19:57:37,796 [podnet.py] => Task 4, Epoch 126/160 (LR 0.01073) => LSC_loss 0.18, Spatial_loss 1.52, Flat_loss 0.13, Train_acc 96.71, Test_acc 59.81
2023-10-17 19:57:39,878 [podnet.py] => Task 4, Epoch 127/160 (LR 0.01013) => LSC_loss 0.18, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 96.53, Test_acc 61.17
2023-10-17 19:57:41,929 [podnet.py] => Task 4, Epoch 128/160 (LR 0.00955) => LSC_loss 0.18, Spatial_loss 1.52, Flat_loss 0.13, Train_acc 96.84, Test_acc 61.31
2023-10-17 19:57:44,005 [podnet.py] => Task 4, Epoch 129/160 (LR 0.00898) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 97.08, Test_acc 60.71
2023-10-17 19:57:46,035 [podnet.py] => Task 4, Epoch 130/160 (LR 0.00843) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 97.00, Test_acc 60.09
2023-10-17 19:57:48,158 [podnet.py] => Task 4, Epoch 131/160 (LR 0.00789) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 96.84, Test_acc 60.13
2023-10-17 19:57:50,234 [podnet.py] => Task 4, Epoch 132/160 (LR 0.00737) => LSC_loss 0.17, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 97.16, Test_acc 60.66
2023-10-17 19:57:52,363 [podnet.py] => Task 4, Epoch 133/160 (LR 0.00686) => LSC_loss 0.18, Spatial_loss 1.49, Flat_loss 0.13, Train_acc 96.66, Test_acc 60.41
2023-10-17 19:57:54,448 [podnet.py] => Task 4, Epoch 134/160 (LR 0.00638) => LSC_loss 0.18, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 96.95, Test_acc 60.51
2023-10-17 19:57:56,524 [podnet.py] => Task 4, Epoch 135/160 (LR 0.00590) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 96.79, Test_acc 60.41
2023-10-17 19:57:58,609 [podnet.py] => Task 4, Epoch 136/160 (LR 0.00545) => LSC_loss 0.18, Spatial_loss 1.38, Flat_loss 0.12, Train_acc 97.05, Test_acc 60.93
2023-10-17 19:58:00,719 [podnet.py] => Task 4, Epoch 137/160 (LR 0.00501) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.12, Train_acc 96.50, Test_acc 61.01
2023-10-17 19:58:02,820 [podnet.py] => Task 4, Epoch 138/160 (LR 0.00459) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 96.68, Test_acc 60.73
2023-10-17 19:58:04,922 [podnet.py] => Task 4, Epoch 139/160 (LR 0.00419) => LSC_loss 0.19, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 96.87, Test_acc 60.19
2023-10-17 19:58:06,991 [podnet.py] => Task 4, Epoch 140/160 (LR 0.00381) => LSC_loss 0.18, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 97.16, Test_acc 61.14
2023-10-17 19:58:09,012 [podnet.py] => Task 4, Epoch 141/160 (LR 0.00344) => LSC_loss 0.18, Spatial_loss 1.42, Flat_loss 0.12, Train_acc 97.13, Test_acc 59.90
2023-10-17 19:58:10,989 [podnet.py] => Task 4, Epoch 142/160 (LR 0.00309) => LSC_loss 0.18, Spatial_loss 1.36, Flat_loss 0.12, Train_acc 97.16, Test_acc 61.51
2023-10-17 19:58:13,035 [podnet.py] => Task 4, Epoch 143/160 (LR 0.00276) => LSC_loss 0.18, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 97.08, Test_acc 60.99
2023-10-17 19:58:15,052 [podnet.py] => Task 4, Epoch 144/160 (LR 0.00245) => LSC_loss 0.18, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 96.84, Test_acc 60.71
2023-10-17 19:58:17,058 [podnet.py] => Task 4, Epoch 145/160 (LR 0.00215) => LSC_loss 0.18, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 96.84, Test_acc 60.99
2023-10-17 19:58:19,106 [podnet.py] => Task 4, Epoch 146/160 (LR 0.00188) => LSC_loss 0.18, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 96.82, Test_acc 61.20
2023-10-17 19:58:21,167 [podnet.py] => Task 4, Epoch 147/160 (LR 0.00162) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.08, Test_acc 61.17
2023-10-17 19:58:23,263 [podnet.py] => Task 4, Epoch 148/160 (LR 0.00138) => LSC_loss 0.17, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 97.26, Test_acc 61.53
2023-10-17 19:58:25,291 [podnet.py] => Task 4, Epoch 149/160 (LR 0.00116) => LSC_loss 0.17, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 97.50, Test_acc 61.26
2023-10-17 19:58:27,361 [podnet.py] => Task 4, Epoch 150/160 (LR 0.00096) => LSC_loss 0.18, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 97.00, Test_acc 61.04
2023-10-17 19:58:29,501 [podnet.py] => Task 4, Epoch 151/160 (LR 0.00078) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.55, Test_acc 61.43
2023-10-17 19:58:31,581 [podnet.py] => Task 4, Epoch 152/160 (LR 0.00062) => LSC_loss 0.17, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 97.34, Test_acc 61.40
2023-10-17 19:58:33,685 [podnet.py] => Task 4, Epoch 153/160 (LR 0.00047) => LSC_loss 0.17, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 97.05, Test_acc 61.47
2023-10-17 19:58:35,696 [podnet.py] => Task 4, Epoch 154/160 (LR 0.00035) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.97, Test_acc 61.14
2023-10-17 19:58:37,755 [podnet.py] => Task 4, Epoch 155/160 (LR 0.00024) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.76, Test_acc 61.13
2023-10-17 19:58:39,836 [podnet.py] => Task 4, Epoch 156/160 (LR 0.00015) => LSC_loss 0.18, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 96.55, Test_acc 61.06
2023-10-17 19:58:41,920 [podnet.py] => Task 4, Epoch 157/160 (LR 0.00009) => LSC_loss 0.18, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 96.42, Test_acc 61.19
2023-10-17 19:58:44,061 [podnet.py] => Task 4, Epoch 158/160 (LR 0.00004) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.03, Test_acc 61.51
2023-10-17 19:58:46,121 [podnet.py] => Task 4, Epoch 159/160 (LR 0.00001) => LSC_loss 0.18, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 97.05, Test_acc 61.20
2023-10-17 19:58:48,221 [podnet.py] => Task 4, Epoch 160/160 (LR 0.00000) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.21, Test_acc 61.57
2023-10-17 19:58:48,222 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 19:58:48,222 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 19:59:16,287 [podnet.py] => The size of finetune dataset: 1400
2023-10-17 19:59:17,821 [podnet.py] => Task 4, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 1.66, Flat_loss 0.14, Train_acc 95.71, Test_acc 62.41
2023-10-17 19:59:19,375 [podnet.py] => Task 4, Epoch 2/20 (LR 0.00488) => LSC_loss 0.10, Spatial_loss 1.41, Flat_loss 0.08, Train_acc 98.64, Test_acc 62.06
2023-10-17 19:59:20,875 [podnet.py] => Task 4, Epoch 3/20 (LR 0.00473) => LSC_loss 0.08, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.93, Test_acc 62.50
2023-10-17 19:59:22,403 [podnet.py] => Task 4, Epoch 4/20 (LR 0.00452) => LSC_loss 0.08, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 99.00, Test_acc 63.54
2023-10-17 19:59:23,923 [podnet.py] => Task 4, Epoch 5/20 (LR 0.00427) => LSC_loss 0.09, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.93, Test_acc 63.23
2023-10-17 19:59:25,433 [podnet.py] => Task 4, Epoch 6/20 (LR 0.00397) => LSC_loss 0.07, Spatial_loss 1.21, Flat_loss 0.05, Train_acc 99.57, Test_acc 63.23
2023-10-17 19:59:26,943 [podnet.py] => Task 4, Epoch 7/20 (LR 0.00363) => LSC_loss 0.09, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 99.07, Test_acc 63.29
2023-10-17 19:59:28,419 [podnet.py] => Task 4, Epoch 8/20 (LR 0.00327) => LSC_loss 0.08, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 99.29, Test_acc 63.30
2023-10-17 19:59:29,905 [podnet.py] => Task 4, Epoch 9/20 (LR 0.00289) => LSC_loss 0.08, Spatial_loss 1.17, Flat_loss 0.05, Train_acc 98.86, Test_acc 63.37
2023-10-17 19:59:31,388 [podnet.py] => Task 4, Epoch 10/20 (LR 0.00250) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.71, Test_acc 63.24
2023-10-17 19:59:32,878 [podnet.py] => Task 4, Epoch 11/20 (LR 0.00211) => LSC_loss 0.08, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 99.21, Test_acc 63.41
2023-10-17 19:59:34,322 [podnet.py] => Task 4, Epoch 12/20 (LR 0.00173) => LSC_loss 0.07, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 99.43, Test_acc 63.59
2023-10-17 19:59:35,805 [podnet.py] => Task 4, Epoch 13/20 (LR 0.00137) => LSC_loss 0.08, Spatial_loss 1.33, Flat_loss 0.05, Train_acc 99.36, Test_acc 63.03
2023-10-17 19:59:37,273 [podnet.py] => Task 4, Epoch 14/20 (LR 0.00103) => LSC_loss 0.07, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 99.21, Test_acc 63.69
2023-10-17 19:59:38,740 [podnet.py] => Task 4, Epoch 15/20 (LR 0.00073) => LSC_loss 0.08, Spatial_loss 1.15, Flat_loss 0.05, Train_acc 98.93, Test_acc 63.39
2023-10-17 19:59:40,226 [podnet.py] => Task 4, Epoch 16/20 (LR 0.00048) => LSC_loss 0.07, Spatial_loss 1.16, Flat_loss 0.05, Train_acc 99.00, Test_acc 63.44
2023-10-17 19:59:41,707 [podnet.py] => Task 4, Epoch 17/20 (LR 0.00027) => LSC_loss 0.08, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 99.29, Test_acc 63.31
2023-10-17 19:59:43,182 [podnet.py] => Task 4, Epoch 18/20 (LR 0.00012) => LSC_loss 0.08, Spatial_loss 1.17, Flat_loss 0.05, Train_acc 99.36, Test_acc 63.39
2023-10-17 19:59:44,666 [podnet.py] => Task 4, Epoch 19/20 (LR 0.00003) => LSC_loss 0.08, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.86, Test_acc 63.57
2023-10-17 19:59:46,181 [podnet.py] => Task 4, Epoch 20/20 (LR 0.00000) => LSC_loss 0.08, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 99.43, Test_acc 63.51
2023-10-17 19:59:46,184 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:00:16,314 [podnet.py] => Exemplar size: 1400
2023-10-17 20:00:16,314 [trainer.py] => CNN: {'total': 63.51, '00-09': 70.5, '10-19': 54.7, '20-29': 71.6, '30-39': 62.8, '40-49': 67.4, '50-59': 52.5, '60-69': 65.1, 'old': 62.91, 'new': 71.4}
2023-10-17 20:00:16,314 [trainer.py] => NME: {'total': 64.09, '00-09': 74.1, '10-19': 60.8, '20-29': 73.1, '30-39': 64.8, '40-49': 68.8, '50-59': 47.8, '60-69': 59.2, 'old': 64.09, 'new': 64.0}
2023-10-17 20:00:16,315 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51]
2023-10-17 20:00:16,315 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67]
2023-10-17 20:00:16,315 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09]
2023-10-17 20:00:16,315 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2]

2023-10-17 20:00:16,315 [trainer.py] => Average Accuracy (CNN): 70.10799999999999
2023-10-17 20:00:16,315 [trainer.py] => Average Accuracy (NME): 70.066
2023-10-17 20:00:16,315 [trainer.py] => All params: 511057
2023-10-17 20:00:16,315 [trainer.py] => Trainable params: 511057
2023-10-17 20:00:16,316 [podnet.py] => Learning on 70-75
2023-10-17 20:00:16,347 [podnet.py] => Adaptive factor: 3.872983346207417
2023-10-17 20:00:18,486 [podnet.py] => Task 5, Epoch 1/160 (LR 0.09999) => LSC_loss 1.73, Spatial_loss 4.16, Flat_loss 0.84, Train_acc 67.31, Test_acc 42.76
2023-10-17 20:00:20,632 [podnet.py] => Task 5, Epoch 2/160 (LR 0.09996) => LSC_loss 0.70, Spatial_loss 4.28, Flat_loss 0.68, Train_acc 80.79, Test_acc 46.36
2023-10-17 20:00:22,703 [podnet.py] => Task 5, Epoch 3/160 (LR 0.09991) => LSC_loss 0.57, Spatial_loss 3.94, Flat_loss 0.56, Train_acc 84.38, Test_acc 49.39
2023-10-17 20:00:24,754 [podnet.py] => Task 5, Epoch 4/160 (LR 0.09985) => LSC_loss 0.50, Spatial_loss 3.65, Flat_loss 0.49, Train_acc 86.72, Test_acc 48.96
2023-10-17 20:00:26,867 [podnet.py] => Task 5, Epoch 5/160 (LR 0.09976) => LSC_loss 0.45, Spatial_loss 3.51, Flat_loss 0.44, Train_acc 88.74, Test_acc 49.61
2023-10-17 20:00:28,982 [podnet.py] => Task 5, Epoch 6/160 (LR 0.09965) => LSC_loss 0.45, Spatial_loss 3.38, Flat_loss 0.42, Train_acc 88.49, Test_acc 45.81
2023-10-17 20:00:31,138 [podnet.py] => Task 5, Epoch 7/160 (LR 0.09953) => LSC_loss 0.41, Spatial_loss 3.41, Flat_loss 0.40, Train_acc 89.69, Test_acc 47.59
2023-10-17 20:00:33,287 [podnet.py] => Task 5, Epoch 8/160 (LR 0.09938) => LSC_loss 0.40, Spatial_loss 3.38, Flat_loss 0.39, Train_acc 89.97, Test_acc 49.95
2023-10-17 20:00:35,459 [podnet.py] => Task 5, Epoch 9/160 (LR 0.09922) => LSC_loss 0.38, Spatial_loss 3.30, Flat_loss 0.37, Train_acc 91.15, Test_acc 51.43
2023-10-17 20:00:37,590 [podnet.py] => Task 5, Epoch 10/160 (LR 0.09904) => LSC_loss 0.37, Spatial_loss 3.16, Flat_loss 0.35, Train_acc 90.77, Test_acc 53.05
2023-10-17 20:00:39,683 [podnet.py] => Task 5, Epoch 11/160 (LR 0.09884) => LSC_loss 0.36, Spatial_loss 3.25, Flat_loss 0.36, Train_acc 91.46, Test_acc 49.43
2023-10-17 20:00:41,845 [podnet.py] => Task 5, Epoch 12/160 (LR 0.09862) => LSC_loss 0.35, Spatial_loss 3.12, Flat_loss 0.34, Train_acc 92.00, Test_acc 51.21
2023-10-17 20:00:43,905 [podnet.py] => Task 5, Epoch 13/160 (LR 0.09838) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.15, Test_acc 46.69
2023-10-17 20:00:46,004 [podnet.py] => Task 5, Epoch 14/160 (LR 0.09812) => LSC_loss 0.36, Spatial_loss 3.18, Flat_loss 0.35, Train_acc 91.15, Test_acc 48.71
2023-10-17 20:00:48,151 [podnet.py] => Task 5, Epoch 15/160 (LR 0.09785) => LSC_loss 0.34, Spatial_loss 3.13, Flat_loss 0.34, Train_acc 92.15, Test_acc 49.69
2023-10-17 20:00:50,248 [podnet.py] => Task 5, Epoch 16/160 (LR 0.09755) => LSC_loss 0.34, Spatial_loss 3.08, Flat_loss 0.34, Train_acc 91.95, Test_acc 52.60
2023-10-17 20:00:52,362 [podnet.py] => Task 5, Epoch 17/160 (LR 0.09724) => LSC_loss 0.33, Spatial_loss 3.04, Flat_loss 0.32, Train_acc 92.51, Test_acc 51.91
2023-10-17 20:00:54,551 [podnet.py] => Task 5, Epoch 18/160 (LR 0.09691) => LSC_loss 0.32, Spatial_loss 2.92, Flat_loss 0.30, Train_acc 92.67, Test_acc 52.55
2023-10-17 20:00:56,704 [podnet.py] => Task 5, Epoch 19/160 (LR 0.09656) => LSC_loss 0.32, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.74, Test_acc 47.71
2023-10-17 20:00:58,826 [podnet.py] => Task 5, Epoch 20/160 (LR 0.09619) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.34, Train_acc 92.41, Test_acc 43.76
2023-10-17 20:01:00,964 [podnet.py] => Task 5, Epoch 21/160 (LR 0.09581) => LSC_loss 0.32, Spatial_loss 3.09, Flat_loss 0.33, Train_acc 92.74, Test_acc 51.81
2023-10-17 20:01:03,145 [podnet.py] => Task 5, Epoch 22/160 (LR 0.09541) => LSC_loss 0.32, Spatial_loss 3.19, Flat_loss 0.34, Train_acc 92.36, Test_acc 52.75
2023-10-17 20:01:05,242 [podnet.py] => Task 5, Epoch 23/160 (LR 0.09499) => LSC_loss 0.30, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 93.13, Test_acc 46.40
2023-10-17 20:01:07,345 [podnet.py] => Task 5, Epoch 24/160 (LR 0.09455) => LSC_loss 0.31, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 92.67, Test_acc 52.23
2023-10-17 20:01:09,476 [podnet.py] => Task 5, Epoch 25/160 (LR 0.09410) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 93.10, Test_acc 52.13
2023-10-17 20:01:11,562 [podnet.py] => Task 5, Epoch 26/160 (LR 0.09362) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 93.23, Test_acc 47.68
2023-10-17 20:01:13,686 [podnet.py] => Task 5, Epoch 27/160 (LR 0.09314) => LSC_loss 0.29, Spatial_loss 2.91, Flat_loss 0.30, Train_acc 93.74, Test_acc 53.97
2023-10-17 20:01:15,852 [podnet.py] => Task 5, Epoch 28/160 (LR 0.09263) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 93.79, Test_acc 50.80
2023-10-17 20:01:18,007 [podnet.py] => Task 5, Epoch 29/160 (LR 0.09211) => LSC_loss 0.29, Spatial_loss 2.86, Flat_loss 0.29, Train_acc 93.90, Test_acc 52.53
2023-10-17 20:01:20,104 [podnet.py] => Task 5, Epoch 30/160 (LR 0.09157) => LSC_loss 0.30, Spatial_loss 2.83, Flat_loss 0.30, Train_acc 93.15, Test_acc 54.61
2023-10-17 20:01:22,277 [podnet.py] => Task 5, Epoch 31/160 (LR 0.09102) => LSC_loss 0.30, Spatial_loss 2.88, Flat_loss 0.30, Train_acc 92.59, Test_acc 50.75
2023-10-17 20:01:24,450 [podnet.py] => Task 5, Epoch 32/160 (LR 0.09045) => LSC_loss 0.30, Spatial_loss 2.97, Flat_loss 0.31, Train_acc 92.77, Test_acc 55.01
2023-10-17 20:01:26,528 [podnet.py] => Task 5, Epoch 33/160 (LR 0.08987) => LSC_loss 0.30, Spatial_loss 3.00, Flat_loss 0.31, Train_acc 93.00, Test_acc 52.39
2023-10-17 20:01:28,726 [podnet.py] => Task 5, Epoch 34/160 (LR 0.08927) => LSC_loss 0.28, Spatial_loss 2.99, Flat_loss 0.31, Train_acc 94.00, Test_acc 52.57
2023-10-17 20:01:30,934 [podnet.py] => Task 5, Epoch 35/160 (LR 0.08865) => LSC_loss 0.28, Spatial_loss 2.84, Flat_loss 0.29, Train_acc 93.95, Test_acc 54.16
2023-10-17 20:01:33,106 [podnet.py] => Task 5, Epoch 36/160 (LR 0.08802) => LSC_loss 0.30, Spatial_loss 2.93, Flat_loss 0.30, Train_acc 92.85, Test_acc 51.83
2023-10-17 20:01:35,266 [podnet.py] => Task 5, Epoch 37/160 (LR 0.08738) => LSC_loss 0.28, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 93.59, Test_acc 54.00
2023-10-17 20:01:37,418 [podnet.py] => Task 5, Epoch 38/160 (LR 0.08672) => LSC_loss 0.31, Spatial_loss 2.99, Flat_loss 0.32, Train_acc 92.62, Test_acc 50.80
2023-10-17 20:01:39,528 [podnet.py] => Task 5, Epoch 39/160 (LR 0.08604) => LSC_loss 0.30, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 92.44, Test_acc 48.24
2023-10-17 20:01:41,627 [podnet.py] => Task 5, Epoch 40/160 (LR 0.08536) => LSC_loss 0.28, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 93.26, Test_acc 53.41
2023-10-17 20:01:43,766 [podnet.py] => Task 5, Epoch 41/160 (LR 0.08465) => LSC_loss 0.27, Spatial_loss 2.79, Flat_loss 0.29, Train_acc 93.46, Test_acc 53.09
2023-10-17 20:01:45,933 [podnet.py] => Task 5, Epoch 42/160 (LR 0.08394) => LSC_loss 0.28, Spatial_loss 2.78, Flat_loss 0.29, Train_acc 94.46, Test_acc 48.64
2023-10-17 20:01:48,085 [podnet.py] => Task 5, Epoch 43/160 (LR 0.08321) => LSC_loss 0.28, Spatial_loss 2.79, Flat_loss 0.29, Train_acc 93.77, Test_acc 46.85
2023-10-17 20:01:50,240 [podnet.py] => Task 5, Epoch 44/160 (LR 0.08247) => LSC_loss 0.26, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 94.51, Test_acc 52.44
2023-10-17 20:01:52,384 [podnet.py] => Task 5, Epoch 45/160 (LR 0.08172) => LSC_loss 0.27, Spatial_loss 2.76, Flat_loss 0.28, Train_acc 93.97, Test_acc 53.60
2023-10-17 20:01:54,449 [podnet.py] => Task 5, Epoch 46/160 (LR 0.08095) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.27, Train_acc 94.38, Test_acc 53.25
2023-10-17 20:01:56,546 [podnet.py] => Task 5, Epoch 47/160 (LR 0.08018) => LSC_loss 0.26, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 94.59, Test_acc 52.63
2023-10-17 20:01:58,658 [podnet.py] => Task 5, Epoch 48/160 (LR 0.07939) => LSC_loss 0.28, Spatial_loss 2.61, Flat_loss 0.27, Train_acc 94.00, Test_acc 55.39
2023-10-17 20:02:00,672 [podnet.py] => Task 5, Epoch 49/160 (LR 0.07859) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.28, Train_acc 94.08, Test_acc 51.44
2023-10-17 20:02:02,844 [podnet.py] => Task 5, Epoch 50/160 (LR 0.07778) => LSC_loss 0.27, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 94.36, Test_acc 53.19
2023-10-17 20:02:04,983 [podnet.py] => Task 5, Epoch 51/160 (LR 0.07696) => LSC_loss 0.26, Spatial_loss 2.64, Flat_loss 0.26, Train_acc 94.54, Test_acc 52.83
2023-10-17 20:02:07,080 [podnet.py] => Task 5, Epoch 52/160 (LR 0.07612) => LSC_loss 0.26, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.56, Test_acc 48.73
2023-10-17 20:02:09,233 [podnet.py] => Task 5, Epoch 53/160 (LR 0.07528) => LSC_loss 0.25, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.97, Test_acc 55.71
2023-10-17 20:02:11,385 [podnet.py] => Task 5, Epoch 54/160 (LR 0.07443) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.26, Train_acc 94.03, Test_acc 52.03
2023-10-17 20:02:13,490 [podnet.py] => Task 5, Epoch 55/160 (LR 0.07357) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.26, Train_acc 94.54, Test_acc 51.95
2023-10-17 20:02:15,561 [podnet.py] => Task 5, Epoch 56/160 (LR 0.07270) => LSC_loss 0.27, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.64, Test_acc 50.27
2023-10-17 20:02:17,696 [podnet.py] => Task 5, Epoch 57/160 (LR 0.07182) => LSC_loss 0.26, Spatial_loss 2.65, Flat_loss 0.26, Train_acc 93.90, Test_acc 50.91
2023-10-17 20:02:19,830 [podnet.py] => Task 5, Epoch 58/160 (LR 0.07093) => LSC_loss 0.28, Spatial_loss 2.66, Flat_loss 0.27, Train_acc 93.67, Test_acc 50.87
2023-10-17 20:02:21,969 [podnet.py] => Task 5, Epoch 59/160 (LR 0.07004) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.26, Train_acc 94.05, Test_acc 50.91
2023-10-17 20:02:24,123 [podnet.py] => Task 5, Epoch 60/160 (LR 0.06913) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 94.56, Test_acc 52.68
2023-10-17 20:02:26,244 [podnet.py] => Task 5, Epoch 61/160 (LR 0.06822) => LSC_loss 0.25, Spatial_loss 2.55, Flat_loss 0.26, Train_acc 94.23, Test_acc 55.63
2023-10-17 20:02:28,395 [podnet.py] => Task 5, Epoch 62/160 (LR 0.06731) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.33, Test_acc 50.43
2023-10-17 20:02:30,508 [podnet.py] => Task 5, Epoch 63/160 (LR 0.06638) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 94.26, Test_acc 51.87
2023-10-17 20:02:32,627 [podnet.py] => Task 5, Epoch 64/160 (LR 0.06545) => LSC_loss 0.26, Spatial_loss 2.54, Flat_loss 0.25, Train_acc 94.10, Test_acc 52.32
2023-10-17 20:02:34,740 [podnet.py] => Task 5, Epoch 65/160 (LR 0.06451) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.25, Train_acc 94.15, Test_acc 53.36
2023-10-17 20:02:36,867 [podnet.py] => Task 5, Epoch 66/160 (LR 0.06357) => LSC_loss 0.25, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 95.23, Test_acc 55.32
2023-10-17 20:02:39,017 [podnet.py] => Task 5, Epoch 67/160 (LR 0.06262) => LSC_loss 0.25, Spatial_loss 2.54, Flat_loss 0.24, Train_acc 94.79, Test_acc 54.43
2023-10-17 20:02:41,106 [podnet.py] => Task 5, Epoch 68/160 (LR 0.06167) => LSC_loss 0.26, Spatial_loss 2.46, Flat_loss 0.24, Train_acc 94.72, Test_acc 55.73
2023-10-17 20:02:43,270 [podnet.py] => Task 5, Epoch 69/160 (LR 0.06072) => LSC_loss 0.26, Spatial_loss 2.58, Flat_loss 0.25, Train_acc 94.33, Test_acc 52.93
2023-10-17 20:02:45,377 [podnet.py] => Task 5, Epoch 70/160 (LR 0.05975) => LSC_loss 0.25, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 94.95, Test_acc 50.35
2023-10-17 20:02:47,533 [podnet.py] => Task 5, Epoch 71/160 (LR 0.05879) => LSC_loss 0.24, Spatial_loss 2.42, Flat_loss 0.24, Train_acc 95.33, Test_acc 51.31
2023-10-17 20:02:49,659 [podnet.py] => Task 5, Epoch 72/160 (LR 0.05782) => LSC_loss 0.26, Spatial_loss 2.45, Flat_loss 0.24, Train_acc 94.05, Test_acc 52.83
2023-10-17 20:02:51,756 [podnet.py] => Task 5, Epoch 73/160 (LR 0.05685) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.23, Train_acc 94.46, Test_acc 52.36
2023-10-17 20:02:53,886 [podnet.py] => Task 5, Epoch 74/160 (LR 0.05588) => LSC_loss 0.25, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 94.79, Test_acc 54.79
2023-10-17 20:02:55,995 [podnet.py] => Task 5, Epoch 75/160 (LR 0.05490) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.24, Train_acc 94.28, Test_acc 56.33
2023-10-17 20:02:58,195 [podnet.py] => Task 5, Epoch 76/160 (LR 0.05392) => LSC_loss 0.25, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 94.67, Test_acc 54.37
2023-10-17 20:03:00,316 [podnet.py] => Task 5, Epoch 77/160 (LR 0.05294) => LSC_loss 0.24, Spatial_loss 2.31, Flat_loss 0.22, Train_acc 95.28, Test_acc 54.09
2023-10-17 20:03:02,432 [podnet.py] => Task 5, Epoch 78/160 (LR 0.05196) => LSC_loss 0.26, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 94.46, Test_acc 54.67
2023-10-17 20:03:04,560 [podnet.py] => Task 5, Epoch 79/160 (LR 0.05098) => LSC_loss 0.25, Spatial_loss 2.35, Flat_loss 0.22, Train_acc 94.67, Test_acc 51.91
2023-10-17 20:03:06,663 [podnet.py] => Task 5, Epoch 80/160 (LR 0.05000) => LSC_loss 0.26, Spatial_loss 2.31, Flat_loss 0.23, Train_acc 94.28, Test_acc 53.97
2023-10-17 20:03:08,840 [podnet.py] => Task 5, Epoch 81/160 (LR 0.04902) => LSC_loss 0.24, Spatial_loss 2.32, Flat_loss 0.22, Train_acc 95.18, Test_acc 55.84
2023-10-17 20:03:10,938 [podnet.py] => Task 5, Epoch 82/160 (LR 0.04804) => LSC_loss 0.25, Spatial_loss 2.28, Flat_loss 0.22, Train_acc 94.85, Test_acc 56.87
2023-10-17 20:03:13,100 [podnet.py] => Task 5, Epoch 83/160 (LR 0.04706) => LSC_loss 0.24, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 95.28, Test_acc 55.32
2023-10-17 20:03:15,269 [podnet.py] => Task 5, Epoch 84/160 (LR 0.04608) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.21, Train_acc 95.38, Test_acc 57.00
2023-10-17 20:03:17,450 [podnet.py] => Task 5, Epoch 85/160 (LR 0.04510) => LSC_loss 0.24, Spatial_loss 2.34, Flat_loss 0.21, Train_acc 94.95, Test_acc 53.69
2023-10-17 20:03:19,595 [podnet.py] => Task 5, Epoch 86/160 (LR 0.04412) => LSC_loss 0.24, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 95.31, Test_acc 54.09
2023-10-17 20:03:21,787 [podnet.py] => Task 5, Epoch 87/160 (LR 0.04315) => LSC_loss 0.24, Spatial_loss 2.15, Flat_loss 0.20, Train_acc 95.36, Test_acc 56.16
2023-10-17 20:03:23,940 [podnet.py] => Task 5, Epoch 88/160 (LR 0.04218) => LSC_loss 0.23, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 95.62, Test_acc 55.28
2023-10-17 20:03:26,026 [podnet.py] => Task 5, Epoch 89/160 (LR 0.04121) => LSC_loss 0.24, Spatial_loss 2.16, Flat_loss 0.20, Train_acc 95.26, Test_acc 53.65
2023-10-17 20:03:28,135 [podnet.py] => Task 5, Epoch 90/160 (LR 0.04025) => LSC_loss 0.23, Spatial_loss 2.18, Flat_loss 0.20, Train_acc 95.21, Test_acc 57.13
2023-10-17 20:03:30,262 [podnet.py] => Task 5, Epoch 91/160 (LR 0.03928) => LSC_loss 0.24, Spatial_loss 2.20, Flat_loss 0.20, Train_acc 94.64, Test_acc 57.51
2023-10-17 20:03:32,407 [podnet.py] => Task 5, Epoch 92/160 (LR 0.03833) => LSC_loss 0.24, Spatial_loss 2.19, Flat_loss 0.20, Train_acc 95.08, Test_acc 53.73
2023-10-17 20:03:34,541 [podnet.py] => Task 5, Epoch 93/160 (LR 0.03738) => LSC_loss 0.23, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.33, Test_acc 55.85
2023-10-17 20:03:36,709 [podnet.py] => Task 5, Epoch 94/160 (LR 0.03643) => LSC_loss 0.25, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 95.05, Test_acc 54.32
2023-10-17 20:03:38,824 [podnet.py] => Task 5, Epoch 95/160 (LR 0.03549) => LSC_loss 0.24, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.18, Test_acc 55.08
2023-10-17 20:03:40,956 [podnet.py] => Task 5, Epoch 96/160 (LR 0.03455) => LSC_loss 0.23, Spatial_loss 2.06, Flat_loss 0.19, Train_acc 95.62, Test_acc 55.77
2023-10-17 20:03:43,107 [podnet.py] => Task 5, Epoch 97/160 (LR 0.03362) => LSC_loss 0.23, Spatial_loss 1.99, Flat_loss 0.18, Train_acc 95.74, Test_acc 54.81
2023-10-17 20:03:45,212 [podnet.py] => Task 5, Epoch 98/160 (LR 0.03269) => LSC_loss 0.23, Spatial_loss 2.05, Flat_loss 0.18, Train_acc 95.69, Test_acc 54.81
2023-10-17 20:03:47,381 [podnet.py] => Task 5, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.12, Flat_loss 0.19, Train_acc 94.72, Test_acc 55.73
2023-10-17 20:03:49,494 [podnet.py] => Task 5, Epoch 100/160 (LR 0.03087) => LSC_loss 0.24, Spatial_loss 2.00, Flat_loss 0.18, Train_acc 94.95, Test_acc 56.05
2023-10-17 20:03:51,626 [podnet.py] => Task 5, Epoch 101/160 (LR 0.02996) => LSC_loss 0.24, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.36, Test_acc 53.73
2023-10-17 20:03:53,756 [podnet.py] => Task 5, Epoch 102/160 (LR 0.02907) => LSC_loss 0.23, Spatial_loss 2.02, Flat_loss 0.18, Train_acc 95.51, Test_acc 55.71
2023-10-17 20:03:55,888 [podnet.py] => Task 5, Epoch 103/160 (LR 0.02818) => LSC_loss 0.23, Spatial_loss 1.96, Flat_loss 0.18, Train_acc 95.46, Test_acc 55.76
2023-10-17 20:03:58,048 [podnet.py] => Task 5, Epoch 104/160 (LR 0.02730) => LSC_loss 0.23, Spatial_loss 1.92, Flat_loss 0.17, Train_acc 95.72, Test_acc 55.28
2023-10-17 20:04:00,172 [podnet.py] => Task 5, Epoch 105/160 (LR 0.02643) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.18, Train_acc 95.56, Test_acc 54.52
2023-10-17 20:04:02,307 [podnet.py] => Task 5, Epoch 106/160 (LR 0.02557) => LSC_loss 0.23, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.33, Test_acc 57.44
2023-10-17 20:04:04,398 [podnet.py] => Task 5, Epoch 107/160 (LR 0.02472) => LSC_loss 0.25, Spatial_loss 2.02, Flat_loss 0.18, Train_acc 94.97, Test_acc 58.40
2023-10-17 20:04:06,528 [podnet.py] => Task 5, Epoch 108/160 (LR 0.02388) => LSC_loss 0.22, Spatial_loss 1.86, Flat_loss 0.17, Train_acc 95.90, Test_acc 56.43
2023-10-17 20:04:08,666 [podnet.py] => Task 5, Epoch 109/160 (LR 0.02304) => LSC_loss 0.22, Spatial_loss 1.84, Flat_loss 0.17, Train_acc 95.87, Test_acc 54.55
2023-10-17 20:04:10,808 [podnet.py] => Task 5, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 1.81, Flat_loss 0.17, Train_acc 95.72, Test_acc 55.73
2023-10-17 20:04:12,949 [podnet.py] => Task 5, Epoch 111/160 (LR 0.02141) => LSC_loss 0.24, Spatial_loss 1.86, Flat_loss 0.17, Train_acc 94.90, Test_acc 56.47
2023-10-17 20:04:15,064 [podnet.py] => Task 5, Epoch 112/160 (LR 0.02061) => LSC_loss 0.23, Spatial_loss 1.78, Flat_loss 0.16, Train_acc 95.23, Test_acc 56.91
2023-10-17 20:04:17,196 [podnet.py] => Task 5, Epoch 113/160 (LR 0.01982) => LSC_loss 0.23, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.72, Test_acc 54.88
2023-10-17 20:04:19,315 [podnet.py] => Task 5, Epoch 114/160 (LR 0.01905) => LSC_loss 0.23, Spatial_loss 1.81, Flat_loss 0.17, Train_acc 95.44, Test_acc 58.07
2023-10-17 20:04:21,479 [podnet.py] => Task 5, Epoch 115/160 (LR 0.01828) => LSC_loss 0.23, Spatial_loss 1.81, Flat_loss 0.16, Train_acc 95.62, Test_acc 58.41
2023-10-17 20:04:23,623 [podnet.py] => Task 5, Epoch 116/160 (LR 0.01753) => LSC_loss 0.24, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.44, Test_acc 56.04
2023-10-17 20:04:25,750 [podnet.py] => Task 5, Epoch 117/160 (LR 0.01679) => LSC_loss 0.23, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.64, Test_acc 58.01
2023-10-17 20:04:27,909 [podnet.py] => Task 5, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.70, Flat_loss 0.15, Train_acc 95.56, Test_acc 57.20
2023-10-17 20:04:30,043 [podnet.py] => Task 5, Epoch 119/160 (LR 0.01535) => LSC_loss 0.24, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 95.26, Test_acc 55.28
2023-10-17 20:04:32,172 [podnet.py] => Task 5, Epoch 120/160 (LR 0.01464) => LSC_loss 0.23, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 95.87, Test_acc 57.41
2023-10-17 20:04:34,339 [podnet.py] => Task 5, Epoch 121/160 (LR 0.01396) => LSC_loss 0.24, Spatial_loss 1.67, Flat_loss 0.15, Train_acc 95.21, Test_acc 55.81
2023-10-17 20:04:36,448 [podnet.py] => Task 5, Epoch 122/160 (LR 0.01328) => LSC_loss 0.23, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 95.95, Test_acc 58.15
2023-10-17 20:04:38,667 [podnet.py] => Task 5, Epoch 123/160 (LR 0.01262) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.15, Train_acc 95.74, Test_acc 57.08
2023-10-17 20:04:40,831 [podnet.py] => Task 5, Epoch 124/160 (LR 0.01198) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 95.77, Test_acc 57.83
2023-10-17 20:04:42,963 [podnet.py] => Task 5, Epoch 125/160 (LR 0.01135) => LSC_loss 0.24, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 95.51, Test_acc 57.57
2023-10-17 20:04:45,115 [podnet.py] => Task 5, Epoch 126/160 (LR 0.01073) => LSC_loss 0.23, Spatial_loss 1.66, Flat_loss 0.15, Train_acc 95.54, Test_acc 57.87
2023-10-17 20:04:47,233 [podnet.py] => Task 5, Epoch 127/160 (LR 0.01013) => LSC_loss 0.23, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 95.56, Test_acc 56.59
2023-10-17 20:04:49,381 [podnet.py] => Task 5, Epoch 128/160 (LR 0.00955) => LSC_loss 0.22, Spatial_loss 1.60, Flat_loss 0.14, Train_acc 96.05, Test_acc 58.29
2023-10-17 20:04:51,585 [podnet.py] => Task 5, Epoch 129/160 (LR 0.00898) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 95.54, Test_acc 58.28
2023-10-17 20:04:53,743 [podnet.py] => Task 5, Epoch 130/160 (LR 0.00843) => LSC_loss 0.24, Spatial_loss 1.55, Flat_loss 0.15, Train_acc 95.26, Test_acc 57.96
2023-10-17 20:04:55,896 [podnet.py] => Task 5, Epoch 131/160 (LR 0.00789) => LSC_loss 0.24, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 95.23, Test_acc 57.64
2023-10-17 20:04:57,998 [podnet.py] => Task 5, Epoch 132/160 (LR 0.00737) => LSC_loss 0.23, Spatial_loss 1.54, Flat_loss 0.14, Train_acc 95.46, Test_acc 57.24
2023-10-17 20:05:00,179 [podnet.py] => Task 5, Epoch 133/160 (LR 0.00686) => LSC_loss 0.23, Spatial_loss 1.48, Flat_loss 0.14, Train_acc 95.49, Test_acc 57.84
2023-10-17 20:05:02,342 [podnet.py] => Task 5, Epoch 134/160 (LR 0.00638) => LSC_loss 0.23, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 95.85, Test_acc 58.81
2023-10-17 20:05:04,491 [podnet.py] => Task 5, Epoch 135/160 (LR 0.00590) => LSC_loss 0.24, Spatial_loss 1.50, Flat_loss 0.14, Train_acc 95.31, Test_acc 57.87
2023-10-17 20:05:06,680 [podnet.py] => Task 5, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 95.56, Test_acc 58.27
2023-10-17 20:05:08,796 [podnet.py] => Task 5, Epoch 137/160 (LR 0.00501) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 95.69, Test_acc 57.91
2023-10-17 20:05:10,947 [podnet.py] => Task 5, Epoch 138/160 (LR 0.00459) => LSC_loss 0.22, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.90, Test_acc 58.40
2023-10-17 20:05:13,047 [podnet.py] => Task 5, Epoch 139/160 (LR 0.00419) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.64, Test_acc 58.37
2023-10-17 20:05:15,181 [podnet.py] => Task 5, Epoch 140/160 (LR 0.00381) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 95.77, Test_acc 58.77
2023-10-17 20:05:17,280 [podnet.py] => Task 5, Epoch 141/160 (LR 0.00344) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 95.87, Test_acc 57.76
2023-10-17 20:05:19,356 [podnet.py] => Task 5, Epoch 142/160 (LR 0.00309) => LSC_loss 0.23, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 95.56, Test_acc 58.51
2023-10-17 20:05:21,442 [podnet.py] => Task 5, Epoch 143/160 (LR 0.00276) => LSC_loss 0.23, Spatial_loss 1.49, Flat_loss 0.14, Train_acc 95.56, Test_acc 58.04
2023-10-17 20:05:23,532 [podnet.py] => Task 5, Epoch 144/160 (LR 0.00245) => LSC_loss 0.22, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 96.28, Test_acc 57.93
2023-10-17 20:05:25,605 [podnet.py] => Task 5, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 96.13, Test_acc 58.60
2023-10-17 20:05:27,781 [podnet.py] => Task 5, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.72, Test_acc 58.05
2023-10-17 20:05:29,959 [podnet.py] => Task 5, Epoch 147/160 (LR 0.00162) => LSC_loss 0.23, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 95.77, Test_acc 58.59
2023-10-17 20:05:32,120 [podnet.py] => Task 5, Epoch 148/160 (LR 0.00138) => LSC_loss 0.24, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.33, Test_acc 58.16
2023-10-17 20:05:34,351 [podnet.py] => Task 5, Epoch 149/160 (LR 0.00116) => LSC_loss 0.23, Spatial_loss 1.37, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.35
2023-10-17 20:05:36,579 [podnet.py] => Task 5, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 96.08, Test_acc 58.43
2023-10-17 20:05:38,803 [podnet.py] => Task 5, Epoch 151/160 (LR 0.00078) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 95.49, Test_acc 58.39
2023-10-17 20:05:41,011 [podnet.py] => Task 5, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.85, Test_acc 58.56
2023-10-17 20:05:43,211 [podnet.py] => Task 5, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.69
2023-10-17 20:05:45,357 [podnet.py] => Task 5, Epoch 154/160 (LR 0.00035) => LSC_loss 0.23, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 95.92, Test_acc 58.63
2023-10-17 20:05:47,572 [podnet.py] => Task 5, Epoch 155/160 (LR 0.00024) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.60
2023-10-17 20:05:49,787 [podnet.py] => Task 5, Epoch 156/160 (LR 0.00015) => LSC_loss 0.24, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 95.59, Test_acc 58.45
2023-10-17 20:05:51,974 [podnet.py] => Task 5, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 95.87, Test_acc 58.43
2023-10-17 20:05:54,204 [podnet.py] => Task 5, Epoch 158/160 (LR 0.00004) => LSC_loss 0.22, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 96.13, Test_acc 58.72
2023-10-17 20:05:56,393 [podnet.py] => Task 5, Epoch 159/160 (LR 0.00001) => LSC_loss 0.23, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 95.87, Test_acc 58.33
2023-10-17 20:05:58,615 [podnet.py] => Task 5, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 96.23, Test_acc 57.91
2023-10-17 20:05:58,616 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:05:58,616 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:06:29,687 [podnet.py] => The size of finetune dataset: 1500
2023-10-17 20:06:31,268 [podnet.py] => Task 5, Epoch 1/20 (LR 0.00497) => LSC_loss 0.19, Spatial_loss 1.61, Flat_loss 0.15, Train_acc 95.13, Test_acc 59.21
2023-10-17 20:06:32,838 [podnet.py] => Task 5, Epoch 2/20 (LR 0.00488) => LSC_loss 0.11, Spatial_loss 1.40, Flat_loss 0.08, Train_acc 98.27, Test_acc 59.88
2023-10-17 20:06:34,390 [podnet.py] => Task 5, Epoch 3/20 (LR 0.00473) => LSC_loss 0.10, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.13, Test_acc 60.91
2023-10-17 20:06:35,919 [podnet.py] => Task 5, Epoch 4/20 (LR 0.00452) => LSC_loss 0.10, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.07, Test_acc 61.09
2023-10-17 20:06:37,448 [podnet.py] => Task 5, Epoch 5/20 (LR 0.00427) => LSC_loss 0.09, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.80, Test_acc 60.59
2023-10-17 20:06:38,998 [podnet.py] => Task 5, Epoch 6/20 (LR 0.00397) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 97.80, Test_acc 60.59
2023-10-17 20:06:40,550 [podnet.py] => Task 5, Epoch 7/20 (LR 0.00363) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.27, Test_acc 60.75
2023-10-17 20:06:42,147 [podnet.py] => Task 5, Epoch 8/20 (LR 0.00327) => LSC_loss 0.09, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.87, Test_acc 61.01
2023-10-17 20:06:43,667 [podnet.py] => Task 5, Epoch 9/20 (LR 0.00289) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.05, Train_acc 98.27, Test_acc 60.72
2023-10-17 20:06:45,227 [podnet.py] => Task 5, Epoch 10/20 (LR 0.00250) => LSC_loss 0.09, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.60, Test_acc 60.61
2023-10-17 20:06:46,791 [podnet.py] => Task 5, Epoch 11/20 (LR 0.00211) => LSC_loss 0.10, Spatial_loss 1.30, Flat_loss 0.05, Train_acc 98.87, Test_acc 60.69
2023-10-17 20:06:48,370 [podnet.py] => Task 5, Epoch 12/20 (LR 0.00173) => LSC_loss 0.09, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 98.40, Test_acc 60.87
2023-10-17 20:06:49,893 [podnet.py] => Task 5, Epoch 13/20 (LR 0.00137) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.73, Test_acc 60.85
2023-10-17 20:06:51,461 [podnet.py] => Task 5, Epoch 14/20 (LR 0.00103) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.06, Train_acc 98.67, Test_acc 60.81
2023-10-17 20:06:53,027 [podnet.py] => Task 5, Epoch 15/20 (LR 0.00073) => LSC_loss 0.09, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.27, Test_acc 61.09
2023-10-17 20:06:54,585 [podnet.py] => Task 5, Epoch 16/20 (LR 0.00048) => LSC_loss 0.09, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.40, Test_acc 61.08
2023-10-17 20:06:56,175 [podnet.py] => Task 5, Epoch 17/20 (LR 0.00027) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.53, Test_acc 60.87
2023-10-17 20:06:57,763 [podnet.py] => Task 5, Epoch 18/20 (LR 0.00012) => LSC_loss 0.09, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.77
2023-10-17 20:06:59,292 [podnet.py] => Task 5, Epoch 19/20 (LR 0.00003) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.88
2023-10-17 20:07:00,898 [podnet.py] => Task 5, Epoch 20/20 (LR 0.00000) => LSC_loss 0.09, Spatial_loss 1.18, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.75
2023-10-17 20:07:00,901 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:07:32,875 [podnet.py] => Exemplar size: 1500
2023-10-17 20:07:32,876 [trainer.py] => CNN: {'total': 60.75, '00-09': 68.8, '10-19': 50.9, '20-29': 67.7, '30-39': 59.6, '40-49': 65.2, '50-59': 48.2, '60-69': 59.2, '70-79': 72.0, 'old': 59.94, 'new': 72.0}
2023-10-17 20:07:32,876 [trainer.py] => NME: {'total': 61.48, '00-09': 72.1, '10-19': 56.6, '20-29': 71.3, '30-39': 62.6, '40-49': 67.6, '50-59': 44.1, '60-69': 55.2, '70-79': 63.2, 'old': 61.36, 'new': 63.2}
2023-10-17 20:07:32,877 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75]
2023-10-17 20:07:32,877 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55]
2023-10-17 20:07:32,877 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48]
2023-10-17 20:07:32,877 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52]

2023-10-17 20:07:32,878 [trainer.py] => Average Accuracy (CNN): 68.54833333333333
2023-10-17 20:07:32,878 [trainer.py] => Average Accuracy (NME): 68.635
2023-10-17 20:07:32,879 [trainer.py] => All params: 514257
2023-10-17 20:07:32,879 [trainer.py] => Trainable params: 514257
2023-10-17 20:07:32,880 [podnet.py] => Learning on 75-80
2023-10-17 20:07:32,914 [podnet.py] => Adaptive factor: 4.0
2023-10-17 20:07:35,169 [podnet.py] => Task 6, Epoch 1/160 (LR 0.09999) => LSC_loss 1.68, Spatial_loss 4.23, Flat_loss 0.95, Train_acc 67.78, Test_acc 37.98
2023-10-17 20:07:37,326 [podnet.py] => Task 6, Epoch 2/160 (LR 0.09996) => LSC_loss 0.82, Spatial_loss 4.53, Flat_loss 0.77, Train_acc 78.22, Test_acc 39.15
2023-10-17 20:07:39,484 [podnet.py] => Task 6, Epoch 3/160 (LR 0.09991) => LSC_loss 0.64, Spatial_loss 4.24, Flat_loss 0.65, Train_acc 82.52, Test_acc 45.81
2023-10-17 20:07:41,582 [podnet.py] => Task 6, Epoch 4/160 (LR 0.09985) => LSC_loss 0.55, Spatial_loss 4.12, Flat_loss 0.59, Train_acc 84.90, Test_acc 41.86
2023-10-17 20:07:43,790 [podnet.py] => Task 6, Epoch 5/160 (LR 0.09976) => LSC_loss 0.49, Spatial_loss 3.76, Flat_loss 0.49, Train_acc 87.75, Test_acc 49.94
2023-10-17 20:07:45,966 [podnet.py] => Task 6, Epoch 6/160 (LR 0.09965) => LSC_loss 0.44, Spatial_loss 3.56, Flat_loss 0.44, Train_acc 88.72, Test_acc 49.80
2023-10-17 20:07:48,136 [podnet.py] => Task 6, Epoch 7/160 (LR 0.09953) => LSC_loss 0.43, Spatial_loss 3.66, Flat_loss 0.45, Train_acc 88.95, Test_acc 49.42
2023-10-17 20:07:50,315 [podnet.py] => Task 6, Epoch 8/160 (LR 0.09938) => LSC_loss 0.40, Spatial_loss 3.41, Flat_loss 0.40, Train_acc 89.58, Test_acc 49.40
2023-10-17 20:07:52,495 [podnet.py] => Task 6, Epoch 9/160 (LR 0.09922) => LSC_loss 0.39, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.32, Test_acc 46.72
2023-10-17 20:07:54,691 [podnet.py] => Task 6, Epoch 10/160 (LR 0.09904) => LSC_loss 0.40, Spatial_loss 3.39, Flat_loss 0.41, Train_acc 89.48, Test_acc 48.94
2023-10-17 20:07:56,798 [podnet.py] => Task 6, Epoch 11/160 (LR 0.09884) => LSC_loss 0.39, Spatial_loss 3.25, Flat_loss 0.37, Train_acc 90.65, Test_acc 50.08
2023-10-17 20:07:59,027 [podnet.py] => Task 6, Epoch 12/160 (LR 0.09862) => LSC_loss 0.40, Spatial_loss 3.37, Flat_loss 0.39, Train_acc 90.20, Test_acc 48.80
2023-10-17 20:08:01,240 [podnet.py] => Task 6, Epoch 13/160 (LR 0.09838) => LSC_loss 0.38, Spatial_loss 3.38, Flat_loss 0.38, Train_acc 89.68, Test_acc 48.35
2023-10-17 20:08:03,387 [podnet.py] => Task 6, Epoch 14/160 (LR 0.09812) => LSC_loss 0.34, Spatial_loss 3.33, Flat_loss 0.37, Train_acc 92.00, Test_acc 45.21
2023-10-17 20:08:05,587 [podnet.py] => Task 6, Epoch 15/160 (LR 0.09785) => LSC_loss 0.36, Spatial_loss 3.21, Flat_loss 0.36, Train_acc 91.10, Test_acc 48.52
2023-10-17 20:08:07,769 [podnet.py] => Task 6, Epoch 16/160 (LR 0.09755) => LSC_loss 0.38, Spatial_loss 3.39, Flat_loss 0.37, Train_acc 91.20, Test_acc 50.16
2023-10-17 20:08:10,026 [podnet.py] => Task 6, Epoch 17/160 (LR 0.09724) => LSC_loss 0.36, Spatial_loss 3.23, Flat_loss 0.36, Train_acc 91.42, Test_acc 47.85
2023-10-17 20:08:12,210 [podnet.py] => Task 6, Epoch 18/160 (LR 0.09691) => LSC_loss 0.33, Spatial_loss 3.05, Flat_loss 0.33, Train_acc 92.42, Test_acc 49.22
2023-10-17 20:08:14,386 [podnet.py] => Task 6, Epoch 19/160 (LR 0.09656) => LSC_loss 0.35, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 91.68, Test_acc 46.36
2023-10-17 20:08:16,591 [podnet.py] => Task 6, Epoch 20/160 (LR 0.09619) => LSC_loss 0.39, Spatial_loss 3.30, Flat_loss 0.37, Train_acc 90.85, Test_acc 45.68
2023-10-17 20:08:18,754 [podnet.py] => Task 6, Epoch 21/160 (LR 0.09581) => LSC_loss 0.34, Spatial_loss 3.08, Flat_loss 0.34, Train_acc 92.40, Test_acc 52.49
2023-10-17 20:08:20,908 [podnet.py] => Task 6, Epoch 22/160 (LR 0.09541) => LSC_loss 0.34, Spatial_loss 3.02, Flat_loss 0.33, Train_acc 91.92, Test_acc 51.28
2023-10-17 20:08:23,121 [podnet.py] => Task 6, Epoch 23/160 (LR 0.09499) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.33, Train_acc 92.02, Test_acc 48.49
2023-10-17 20:08:25,256 [podnet.py] => Task 6, Epoch 24/160 (LR 0.09455) => LSC_loss 0.35, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.88, Test_acc 51.12
2023-10-17 20:08:27,411 [podnet.py] => Task 6, Epoch 25/160 (LR 0.09410) => LSC_loss 0.33, Spatial_loss 3.19, Flat_loss 0.35, Train_acc 91.68, Test_acc 46.69
2023-10-17 20:08:29,621 [podnet.py] => Task 6, Epoch 26/160 (LR 0.09362) => LSC_loss 0.34, Spatial_loss 3.15, Flat_loss 0.34, Train_acc 91.35, Test_acc 46.10
2023-10-17 20:08:31,806 [podnet.py] => Task 6, Epoch 27/160 (LR 0.09314) => LSC_loss 0.32, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.35, Test_acc 45.64
2023-10-17 20:08:34,023 [podnet.py] => Task 6, Epoch 28/160 (LR 0.09263) => LSC_loss 0.31, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 92.90, Test_acc 49.25
2023-10-17 20:08:36,213 [podnet.py] => Task 6, Epoch 29/160 (LR 0.09211) => LSC_loss 0.34, Spatial_loss 3.04, Flat_loss 0.33, Train_acc 92.15, Test_acc 48.71
2023-10-17 20:08:38,385 [podnet.py] => Task 6, Epoch 30/160 (LR 0.09157) => LSC_loss 0.33, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 92.05, Test_acc 49.08
2023-10-17 20:08:40,533 [podnet.py] => Task 6, Epoch 31/160 (LR 0.09102) => LSC_loss 0.32, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 92.62, Test_acc 49.14
2023-10-17 20:08:42,736 [podnet.py] => Task 6, Epoch 32/160 (LR 0.09045) => LSC_loss 0.33, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 92.08, Test_acc 49.32
2023-10-17 20:08:45,029 [podnet.py] => Task 6, Epoch 33/160 (LR 0.08987) => LSC_loss 0.32, Spatial_loss 3.03, Flat_loss 0.33, Train_acc 92.92, Test_acc 52.25
2023-10-17 20:08:47,213 [podnet.py] => Task 6, Epoch 34/160 (LR 0.08927) => LSC_loss 0.31, Spatial_loss 2.99, Flat_loss 0.32, Train_acc 92.52, Test_acc 49.48
2023-10-17 20:08:49,445 [podnet.py] => Task 6, Epoch 35/160 (LR 0.08865) => LSC_loss 0.31, Spatial_loss 2.95, Flat_loss 0.31, Train_acc 92.95, Test_acc 49.85
2023-10-17 20:08:51,627 [podnet.py] => Task 6, Epoch 36/160 (LR 0.08802) => LSC_loss 0.31, Spatial_loss 2.92, Flat_loss 0.31, Train_acc 92.88, Test_acc 51.04
2023-10-17 20:08:53,805 [podnet.py] => Task 6, Epoch 37/160 (LR 0.08738) => LSC_loss 0.33, Spatial_loss 2.93, Flat_loss 0.32, Train_acc 92.48, Test_acc 42.90
2023-10-17 20:08:55,948 [podnet.py] => Task 6, Epoch 38/160 (LR 0.08672) => LSC_loss 0.36, Spatial_loss 3.17, Flat_loss 0.36, Train_acc 91.30, Test_acc 51.41
2023-10-17 20:08:58,174 [podnet.py] => Task 6, Epoch 39/160 (LR 0.08604) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 93.22, Test_acc 49.25
2023-10-17 20:09:00,405 [podnet.py] => Task 6, Epoch 40/160 (LR 0.08536) => LSC_loss 0.30, Spatial_loss 2.92, Flat_loss 0.30, Train_acc 93.28, Test_acc 51.05
2023-10-17 20:09:02,609 [podnet.py] => Task 6, Epoch 41/160 (LR 0.08465) => LSC_loss 0.29, Spatial_loss 2.83, Flat_loss 0.29, Train_acc 93.72, Test_acc 52.51
2023-10-17 20:09:04,825 [podnet.py] => Task 6, Epoch 42/160 (LR 0.08394) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 93.58, Test_acc 52.72
2023-10-17 20:09:07,058 [podnet.py] => Task 6, Epoch 43/160 (LR 0.08321) => LSC_loss 0.31, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 92.65, Test_acc 51.80
2023-10-17 20:09:09,316 [podnet.py] => Task 6, Epoch 44/160 (LR 0.08247) => LSC_loss 0.30, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 93.22, Test_acc 50.41
2023-10-17 20:09:11,479 [podnet.py] => Task 6, Epoch 45/160 (LR 0.08172) => LSC_loss 0.31, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 93.15, Test_acc 47.22
2023-10-17 20:09:13,661 [podnet.py] => Task 6, Epoch 46/160 (LR 0.08095) => LSC_loss 0.29, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 93.20, Test_acc 51.00
2023-10-17 20:09:15,854 [podnet.py] => Task 6, Epoch 47/160 (LR 0.08018) => LSC_loss 0.30, Spatial_loss 2.73, Flat_loss 0.29, Train_acc 93.25, Test_acc 49.92
2023-10-17 20:09:18,062 [podnet.py] => Task 6, Epoch 48/160 (LR 0.07939) => LSC_loss 0.28, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 93.80, Test_acc 50.38
2023-10-17 20:09:20,246 [podnet.py] => Task 6, Epoch 49/160 (LR 0.07859) => LSC_loss 0.30, Spatial_loss 2.87, Flat_loss 0.29, Train_acc 93.15, Test_acc 47.50
2023-10-17 20:09:22,484 [podnet.py] => Task 6, Epoch 50/160 (LR 0.07778) => LSC_loss 0.28, Spatial_loss 2.89, Flat_loss 0.30, Train_acc 93.78, Test_acc 51.12
2023-10-17 20:09:24,696 [podnet.py] => Task 6, Epoch 51/160 (LR 0.07696) => LSC_loss 0.29, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 93.05, Test_acc 53.30
2023-10-17 20:09:26,870 [podnet.py] => Task 6, Epoch 52/160 (LR 0.07612) => LSC_loss 0.28, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 93.92, Test_acc 50.61
2023-10-17 20:09:29,032 [podnet.py] => Task 6, Epoch 53/160 (LR 0.07528) => LSC_loss 0.29, Spatial_loss 2.71, Flat_loss 0.27, Train_acc 93.40, Test_acc 47.81
2023-10-17 20:09:31,244 [podnet.py] => Task 6, Epoch 54/160 (LR 0.07443) => LSC_loss 0.28, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 93.65, Test_acc 50.91
2023-10-17 20:09:33,339 [podnet.py] => Task 6, Epoch 55/160 (LR 0.07357) => LSC_loss 0.27, Spatial_loss 2.66, Flat_loss 0.27, Train_acc 93.82, Test_acc 49.72
2023-10-17 20:09:35,553 [podnet.py] => Task 6, Epoch 56/160 (LR 0.07270) => LSC_loss 0.29, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 93.50, Test_acc 50.26
2023-10-17 20:09:37,740 [podnet.py] => Task 6, Epoch 57/160 (LR 0.07182) => LSC_loss 0.30, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 93.42, Test_acc 49.06
2023-10-17 20:09:39,952 [podnet.py] => Task 6, Epoch 58/160 (LR 0.07093) => LSC_loss 0.29, Spatial_loss 2.76, Flat_loss 0.29, Train_acc 93.55, Test_acc 51.70
2023-10-17 20:09:42,172 [podnet.py] => Task 6, Epoch 59/160 (LR 0.07004) => LSC_loss 0.28, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 93.85, Test_acc 53.24
2023-10-17 20:09:44,395 [podnet.py] => Task 6, Epoch 60/160 (LR 0.06913) => LSC_loss 0.28, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.78, Test_acc 50.49
2023-10-17 20:09:46,579 [podnet.py] => Task 6, Epoch 61/160 (LR 0.06822) => LSC_loss 0.27, Spatial_loss 2.77, Flat_loss 0.28, Train_acc 93.80, Test_acc 52.34
2023-10-17 20:09:48,794 [podnet.py] => Task 6, Epoch 62/160 (LR 0.06731) => LSC_loss 0.28, Spatial_loss 2.70, Flat_loss 0.27, Train_acc 94.22, Test_acc 48.96
2023-10-17 20:09:50,994 [podnet.py] => Task 6, Epoch 63/160 (LR 0.06638) => LSC_loss 0.31, Spatial_loss 2.75, Flat_loss 0.29, Train_acc 93.58, Test_acc 51.55
2023-10-17 20:09:53,205 [podnet.py] => Task 6, Epoch 64/160 (LR 0.06545) => LSC_loss 0.27, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.38, Test_acc 52.38
2023-10-17 20:09:55,421 [podnet.py] => Task 6, Epoch 65/160 (LR 0.06451) => LSC_loss 0.29, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.85, Test_acc 53.05
2023-10-17 20:09:57,563 [podnet.py] => Task 6, Epoch 66/160 (LR 0.06357) => LSC_loss 0.29, Spatial_loss 2.61, Flat_loss 0.27, Train_acc 93.30, Test_acc 49.65
2023-10-17 20:09:59,791 [podnet.py] => Task 6, Epoch 67/160 (LR 0.06262) => LSC_loss 0.28, Spatial_loss 2.63, Flat_loss 0.27, Train_acc 93.68, Test_acc 51.39
2023-10-17 20:10:01,990 [podnet.py] => Task 6, Epoch 68/160 (LR 0.06167) => LSC_loss 0.28, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.70, Test_acc 52.14
2023-10-17 20:10:04,185 [podnet.py] => Task 6, Epoch 69/160 (LR 0.06072) => LSC_loss 0.27, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 93.52, Test_acc 51.85
2023-10-17 20:10:06,364 [podnet.py] => Task 6, Epoch 70/160 (LR 0.05975) => LSC_loss 0.27, Spatial_loss 2.51, Flat_loss 0.25, Train_acc 94.15, Test_acc 52.21
2023-10-17 20:10:08,603 [podnet.py] => Task 6, Epoch 71/160 (LR 0.05879) => LSC_loss 0.26, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 94.88, Test_acc 52.81
2023-10-17 20:10:10,819 [podnet.py] => Task 6, Epoch 72/160 (LR 0.05782) => LSC_loss 0.26, Spatial_loss 2.39, Flat_loss 0.23, Train_acc 94.82, Test_acc 48.99
2023-10-17 20:10:13,009 [podnet.py] => Task 6, Epoch 73/160 (LR 0.05685) => LSC_loss 0.27, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.12, Test_acc 49.22
2023-10-17 20:10:15,191 [podnet.py] => Task 6, Epoch 74/160 (LR 0.05588) => LSC_loss 0.27, Spatial_loss 2.46, Flat_loss 0.25, Train_acc 93.85, Test_acc 48.69
2023-10-17 20:10:17,391 [podnet.py] => Task 6, Epoch 75/160 (LR 0.05490) => LSC_loss 0.27, Spatial_loss 2.51, Flat_loss 0.25, Train_acc 94.12, Test_acc 52.68
2023-10-17 20:10:19,567 [podnet.py] => Task 6, Epoch 76/160 (LR 0.05392) => LSC_loss 0.27, Spatial_loss 2.53, Flat_loss 0.26, Train_acc 94.20, Test_acc 51.74
2023-10-17 20:10:21,762 [podnet.py] => Task 6, Epoch 77/160 (LR 0.05294) => LSC_loss 0.27, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 93.88, Test_acc 48.98
2023-10-17 20:10:23,960 [podnet.py] => Task 6, Epoch 78/160 (LR 0.05196) => LSC_loss 0.27, Spatial_loss 2.55, Flat_loss 0.25, Train_acc 94.32, Test_acc 50.98
2023-10-17 20:10:26,191 [podnet.py] => Task 6, Epoch 79/160 (LR 0.05098) => LSC_loss 0.27, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.20, Test_acc 51.79
2023-10-17 20:10:28,438 [podnet.py] => Task 6, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 93.35, Test_acc 44.65
2023-10-17 20:10:30,635 [podnet.py] => Task 6, Epoch 81/160 (LR 0.04902) => LSC_loss 0.28, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.25, Test_acc 52.62
2023-10-17 20:10:32,792 [podnet.py] => Task 6, Epoch 82/160 (LR 0.04804) => LSC_loss 0.26, Spatial_loss 2.40, Flat_loss 0.24, Train_acc 94.78, Test_acc 52.24
2023-10-17 20:10:35,076 [podnet.py] => Task 6, Epoch 83/160 (LR 0.04706) => LSC_loss 0.27, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 94.48, Test_acc 50.19
2023-10-17 20:10:37,325 [podnet.py] => Task 6, Epoch 84/160 (LR 0.04608) => LSC_loss 0.25, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 94.72, Test_acc 54.92
2023-10-17 20:10:39,467 [podnet.py] => Task 6, Epoch 85/160 (LR 0.04510) => LSC_loss 0.26, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.75, Test_acc 52.16
2023-10-17 20:10:41,596 [podnet.py] => Task 6, Epoch 86/160 (LR 0.04412) => LSC_loss 0.27, Spatial_loss 2.35, Flat_loss 0.23, Train_acc 93.92, Test_acc 52.69
2023-10-17 20:10:43,806 [podnet.py] => Task 6, Epoch 87/160 (LR 0.04315) => LSC_loss 0.25, Spatial_loss 2.36, Flat_loss 0.23, Train_acc 94.92, Test_acc 52.98
2023-10-17 20:10:45,988 [podnet.py] => Task 6, Epoch 88/160 (LR 0.04218) => LSC_loss 0.25, Spatial_loss 2.22, Flat_loss 0.22, Train_acc 94.95, Test_acc 53.74
2023-10-17 20:10:48,238 [podnet.py] => Task 6, Epoch 89/160 (LR 0.04121) => LSC_loss 0.27, Spatial_loss 2.29, Flat_loss 0.23, Train_acc 94.45, Test_acc 53.60
2023-10-17 20:10:50,449 [podnet.py] => Task 6, Epoch 90/160 (LR 0.04025) => LSC_loss 0.25, Spatial_loss 2.26, Flat_loss 0.22, Train_acc 94.50, Test_acc 51.32
2023-10-17 20:10:52,656 [podnet.py] => Task 6, Epoch 91/160 (LR 0.03928) => LSC_loss 0.26, Spatial_loss 2.25, Flat_loss 0.22, Train_acc 94.82, Test_acc 55.26
2023-10-17 20:10:54,815 [podnet.py] => Task 6, Epoch 92/160 (LR 0.03833) => LSC_loss 0.25, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.82, Test_acc 53.14
2023-10-17 20:10:57,044 [podnet.py] => Task 6, Epoch 93/160 (LR 0.03738) => LSC_loss 0.26, Spatial_loss 2.16, Flat_loss 0.21, Train_acc 94.25, Test_acc 54.69
2023-10-17 20:10:59,338 [podnet.py] => Task 6, Epoch 94/160 (LR 0.03643) => LSC_loss 0.26, Spatial_loss 2.20, Flat_loss 0.21, Train_acc 95.08, Test_acc 54.14
2023-10-17 20:11:01,516 [podnet.py] => Task 6, Epoch 95/160 (LR 0.03549) => LSC_loss 0.27, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 94.80, Test_acc 54.36
2023-10-17 20:11:03,723 [podnet.py] => Task 6, Epoch 96/160 (LR 0.03455) => LSC_loss 0.25, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.72, Test_acc 53.19
2023-10-17 20:11:05,922 [podnet.py] => Task 6, Epoch 97/160 (LR 0.03362) => LSC_loss 0.26, Spatial_loss 2.11, Flat_loss 0.20, Train_acc 94.98, Test_acc 53.65
2023-10-17 20:11:08,064 [podnet.py] => Task 6, Epoch 98/160 (LR 0.03269) => LSC_loss 0.25, Spatial_loss 2.17, Flat_loss 0.21, Train_acc 95.10, Test_acc 51.91
2023-10-17 20:11:10,304 [podnet.py] => Task 6, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.12, Flat_loss 0.21, Train_acc 94.48, Test_acc 53.18
2023-10-17 20:11:12,471 [podnet.py] => Task 6, Epoch 100/160 (LR 0.03087) => LSC_loss 0.24, Spatial_loss 2.04, Flat_loss 0.20, Train_acc 95.10, Test_acc 53.54
2023-10-17 20:11:14,684 [podnet.py] => Task 6, Epoch 101/160 (LR 0.02996) => LSC_loss 0.26, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.00, Test_acc 53.72
2023-10-17 20:11:16,878 [podnet.py] => Task 6, Epoch 102/160 (LR 0.02907) => LSC_loss 0.26, Spatial_loss 2.09, Flat_loss 0.20, Train_acc 94.58, Test_acc 53.41
2023-10-17 20:11:18,997 [podnet.py] => Task 6, Epoch 103/160 (LR 0.02818) => LSC_loss 0.26, Spatial_loss 2.04, Flat_loss 0.19, Train_acc 94.88, Test_acc 52.31
2023-10-17 20:11:21,222 [podnet.py] => Task 6, Epoch 104/160 (LR 0.02730) => LSC_loss 0.24, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 95.15, Test_acc 53.26
2023-10-17 20:11:23,417 [podnet.py] => Task 6, Epoch 105/160 (LR 0.02643) => LSC_loss 0.25, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 95.48, Test_acc 54.04
2023-10-17 20:11:25,597 [podnet.py] => Task 6, Epoch 106/160 (LR 0.02557) => LSC_loss 0.24, Spatial_loss 1.95, Flat_loss 0.19, Train_acc 95.10, Test_acc 56.41
2023-10-17 20:11:27,837 [podnet.py] => Task 6, Epoch 107/160 (LR 0.02472) => LSC_loss 0.25, Spatial_loss 1.91, Flat_loss 0.19, Train_acc 95.10, Test_acc 54.14
2023-10-17 20:11:29,988 [podnet.py] => Task 6, Epoch 108/160 (LR 0.02388) => LSC_loss 0.25, Spatial_loss 1.96, Flat_loss 0.19, Train_acc 94.92, Test_acc 54.38
2023-10-17 20:11:32,187 [podnet.py] => Task 6, Epoch 109/160 (LR 0.02304) => LSC_loss 0.25, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.15, Test_acc 54.19
2023-10-17 20:11:34,317 [podnet.py] => Task 6, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.19, Train_acc 95.48, Test_acc 53.59
2023-10-17 20:11:36,487 [podnet.py] => Task 6, Epoch 111/160 (LR 0.02141) => LSC_loss 0.26, Spatial_loss 1.87, Flat_loss 0.18, Train_acc 94.98, Test_acc 53.71
2023-10-17 20:11:38,661 [podnet.py] => Task 6, Epoch 112/160 (LR 0.02061) => LSC_loss 0.25, Spatial_loss 1.88, Flat_loss 0.19, Train_acc 95.08, Test_acc 51.39
2023-10-17 20:11:40,821 [podnet.py] => Task 6, Epoch 113/160 (LR 0.01982) => LSC_loss 0.25, Spatial_loss 1.88, Flat_loss 0.18, Train_acc 95.30, Test_acc 54.18
2023-10-17 20:11:43,043 [podnet.py] => Task 6, Epoch 114/160 (LR 0.01905) => LSC_loss 0.25, Spatial_loss 1.86, Flat_loss 0.18, Train_acc 94.68, Test_acc 54.90
2023-10-17 20:11:45,239 [podnet.py] => Task 6, Epoch 115/160 (LR 0.01828) => LSC_loss 0.24, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 95.82, Test_acc 55.21
2023-10-17 20:11:47,491 [podnet.py] => Task 6, Epoch 116/160 (LR 0.01753) => LSC_loss 0.25, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 95.18, Test_acc 55.60
2023-10-17 20:11:49,610 [podnet.py] => Task 6, Epoch 117/160 (LR 0.01679) => LSC_loss 0.25, Spatial_loss 1.86, Flat_loss 0.18, Train_acc 95.40, Test_acc 55.30
2023-10-17 20:11:51,776 [podnet.py] => Task 6, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.76, Flat_loss 0.16, Train_acc 95.40, Test_acc 54.62
2023-10-17 20:11:53,966 [podnet.py] => Task 6, Epoch 119/160 (LR 0.01535) => LSC_loss 0.24, Spatial_loss 1.70, Flat_loss 0.16, Train_acc 95.18, Test_acc 55.01
2023-10-17 20:11:56,128 [podnet.py] => Task 6, Epoch 120/160 (LR 0.01464) => LSC_loss 0.25, Spatial_loss 1.77, Flat_loss 0.17, Train_acc 94.92, Test_acc 54.29
2023-10-17 20:11:58,328 [podnet.py] => Task 6, Epoch 121/160 (LR 0.01396) => LSC_loss 0.25, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.58, Test_acc 55.62
2023-10-17 20:12:00,497 [podnet.py] => Task 6, Epoch 122/160 (LR 0.01328) => LSC_loss 0.24, Spatial_loss 1.73, Flat_loss 0.16, Train_acc 95.12, Test_acc 53.92
2023-10-17 20:12:02,695 [podnet.py] => Task 6, Epoch 123/160 (LR 0.01262) => LSC_loss 0.24, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.45, Test_acc 53.56
2023-10-17 20:12:04,868 [podnet.py] => Task 6, Epoch 124/160 (LR 0.01198) => LSC_loss 0.24, Spatial_loss 1.64, Flat_loss 0.15, Train_acc 95.48, Test_acc 52.75
2023-10-17 20:12:07,027 [podnet.py] => Task 6, Epoch 125/160 (LR 0.01135) => LSC_loss 0.25, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 95.12, Test_acc 54.74
2023-10-17 20:12:09,248 [podnet.py] => Task 6, Epoch 126/160 (LR 0.01073) => LSC_loss 0.25, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 94.95, Test_acc 55.00
2023-10-17 20:12:11,447 [podnet.py] => Task 6, Epoch 127/160 (LR 0.01013) => LSC_loss 0.24, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 95.38, Test_acc 54.28
2023-10-17 20:12:13,644 [podnet.py] => Task 6, Epoch 128/160 (LR 0.00955) => LSC_loss 0.25, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 94.85, Test_acc 56.50
2023-10-17 20:12:15,850 [podnet.py] => Task 6, Epoch 129/160 (LR 0.00898) => LSC_loss 0.24, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 95.40, Test_acc 54.65
2023-10-17 20:12:18,015 [podnet.py] => Task 6, Epoch 130/160 (LR 0.00843) => LSC_loss 0.25, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 94.92, Test_acc 56.30
2023-10-17 20:12:20,210 [podnet.py] => Task 6, Epoch 131/160 (LR 0.00789) => LSC_loss 0.25, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 95.20, Test_acc 55.51
2023-10-17 20:12:22,444 [podnet.py] => Task 6, Epoch 132/160 (LR 0.00737) => LSC_loss 0.23, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.62, Test_acc 55.39
2023-10-17 20:12:24,587 [podnet.py] => Task 6, Epoch 133/160 (LR 0.00686) => LSC_loss 0.25, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 95.82, Test_acc 56.21
2023-10-17 20:12:26,804 [podnet.py] => Task 6, Epoch 134/160 (LR 0.00638) => LSC_loss 0.25, Spatial_loss 1.55, Flat_loss 0.15, Train_acc 95.10, Test_acc 55.79
2023-10-17 20:12:28,982 [podnet.py] => Task 6, Epoch 135/160 (LR 0.00590) => LSC_loss 0.24, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 95.52, Test_acc 56.05
2023-10-17 20:12:31,160 [podnet.py] => Task 6, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 95.48, Test_acc 55.72
2023-10-17 20:12:33,314 [podnet.py] => Task 6, Epoch 137/160 (LR 0.00501) => LSC_loss 0.25, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 94.78, Test_acc 54.99
2023-10-17 20:12:35,529 [podnet.py] => Task 6, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.82, Test_acc 56.80
2023-10-17 20:12:37,726 [podnet.py] => Task 6, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.42, Test_acc 56.14
2023-10-17 20:12:39,881 [podnet.py] => Task 6, Epoch 140/160 (LR 0.00381) => LSC_loss 0.25, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 95.55, Test_acc 55.98
2023-10-17 20:12:42,040 [podnet.py] => Task 6, Epoch 141/160 (LR 0.00344) => LSC_loss 0.25, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.40, Test_acc 56.01
2023-10-17 20:12:44,174 [podnet.py] => Task 6, Epoch 142/160 (LR 0.00309) => LSC_loss 0.24, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.68, Test_acc 56.02
2023-10-17 20:12:46,330 [podnet.py] => Task 6, Epoch 143/160 (LR 0.00276) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 94.85, Test_acc 56.31
2023-10-17 20:12:48,468 [podnet.py] => Task 6, Epoch 144/160 (LR 0.00245) => LSC_loss 0.24, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 95.68, Test_acc 56.06
2023-10-17 20:12:50,601 [podnet.py] => Task 6, Epoch 145/160 (LR 0.00215) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 95.05, Test_acc 56.10
2023-10-17 20:12:52,739 [podnet.py] => Task 6, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 95.55, Test_acc 55.70
2023-10-17 20:12:54,853 [podnet.py] => Task 6, Epoch 147/160 (LR 0.00162) => LSC_loss 0.24, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.42, Test_acc 56.14
2023-10-17 20:12:56,997 [podnet.py] => Task 6, Epoch 148/160 (LR 0.00138) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.28, Test_acc 56.28
2023-10-17 20:12:59,159 [podnet.py] => Task 6, Epoch 149/160 (LR 0.00116) => LSC_loss 0.24, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 94.90, Test_acc 56.26
2023-10-17 20:13:01,435 [podnet.py] => Task 6, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 96.32, Test_acc 55.90
2023-10-17 20:13:03,692 [podnet.py] => Task 6, Epoch 151/160 (LR 0.00078) => LSC_loss 0.24, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 95.25, Test_acc 56.45
2023-10-17 20:13:05,914 [podnet.py] => Task 6, Epoch 152/160 (LR 0.00062) => LSC_loss 0.26, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.42, Test_acc 56.14
2023-10-17 20:13:08,146 [podnet.py] => Task 6, Epoch 153/160 (LR 0.00047) => LSC_loss 0.24, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 95.65, Test_acc 56.15
2023-10-17 20:13:10,398 [podnet.py] => Task 6, Epoch 154/160 (LR 0.00035) => LSC_loss 0.25, Spatial_loss 1.41, Flat_loss 0.14, Train_acc 95.45, Test_acc 56.25
2023-10-17 20:13:12,615 [podnet.py] => Task 6, Epoch 155/160 (LR 0.00024) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.80, Test_acc 56.28
2023-10-17 20:13:14,844 [podnet.py] => Task 6, Epoch 156/160 (LR 0.00015) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 95.75, Test_acc 56.54
2023-10-17 20:13:17,121 [podnet.py] => Task 6, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 95.78, Test_acc 56.44
2023-10-17 20:13:19,406 [podnet.py] => Task 6, Epoch 158/160 (LR 0.00004) => LSC_loss 0.24, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.32, Test_acc 56.24
2023-10-17 20:13:21,678 [podnet.py] => Task 6, Epoch 159/160 (LR 0.00001) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.22, Test_acc 56.36
2023-10-17 20:13:23,873 [podnet.py] => Task 6, Epoch 160/160 (LR 0.00000) => LSC_loss 0.25, Spatial_loss 1.33, Flat_loss 0.14, Train_acc 95.58, Test_acc 56.41
2023-10-17 20:13:23,874 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:13:23,874 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:13:56,529 [podnet.py] => The size of finetune dataset: 1600
2023-10-17 20:13:58,190 [podnet.py] => Task 6, Epoch 1/20 (LR 0.00497) => LSC_loss 0.20, Spatial_loss 1.85, Flat_loss 0.15, Train_acc 94.75, Test_acc 54.84
2023-10-17 20:13:59,795 [podnet.py] => Task 6, Epoch 2/20 (LR 0.00488) => LSC_loss 0.13, Spatial_loss 1.47, Flat_loss 0.08, Train_acc 97.56, Test_acc 57.60
2023-10-17 20:14:01,421 [podnet.py] => Task 6, Epoch 3/20 (LR 0.00473) => LSC_loss 0.12, Spatial_loss 1.45, Flat_loss 0.07, Train_acc 98.19, Test_acc 58.51
2023-10-17 20:14:03,038 [podnet.py] => Task 6, Epoch 4/20 (LR 0.00452) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.00, Test_acc 58.24
2023-10-17 20:14:04,638 [podnet.py] => Task 6, Epoch 5/20 (LR 0.00427) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.25, Test_acc 58.04
2023-10-17 20:14:06,273 [podnet.py] => Task 6, Epoch 6/20 (LR 0.00397) => LSC_loss 0.11, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.75, Test_acc 57.84
2023-10-17 20:14:07,917 [podnet.py] => Task 6, Epoch 7/20 (LR 0.00363) => LSC_loss 0.11, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.69, Test_acc 58.12
2023-10-17 20:14:09,543 [podnet.py] => Task 6, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.19, Test_acc 58.31
2023-10-17 20:14:11,131 [podnet.py] => Task 6, Epoch 9/20 (LR 0.00289) => LSC_loss 0.11, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.69, Test_acc 58.12
2023-10-17 20:14:12,786 [podnet.py] => Task 6, Epoch 10/20 (LR 0.00250) => LSC_loss 0.11, Spatial_loss 1.26, Flat_loss 0.06, Train_acc 98.25, Test_acc 58.10
2023-10-17 20:14:14,402 [podnet.py] => Task 6, Epoch 11/20 (LR 0.00211) => LSC_loss 0.11, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.62, Test_acc 58.22
2023-10-17 20:14:16,016 [podnet.py] => Task 6, Epoch 12/20 (LR 0.00173) => LSC_loss 0.11, Spatial_loss 1.30, Flat_loss 0.05, Train_acc 98.44, Test_acc 58.18
2023-10-17 20:14:17,639 [podnet.py] => Task 6, Epoch 13/20 (LR 0.00137) => LSC_loss 0.11, Spatial_loss 1.25, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.28
2023-10-17 20:14:19,280 [podnet.py] => Task 6, Epoch 14/20 (LR 0.00103) => LSC_loss 0.10, Spatial_loss 1.31, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.16
2023-10-17 20:14:20,920 [podnet.py] => Task 6, Epoch 15/20 (LR 0.00073) => LSC_loss 0.10, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.69, Test_acc 58.04
2023-10-17 20:14:22,495 [podnet.py] => Task 6, Epoch 16/20 (LR 0.00048) => LSC_loss 0.11, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.56, Test_acc 58.24
2023-10-17 20:14:24,115 [podnet.py] => Task 6, Epoch 17/20 (LR 0.00027) => LSC_loss 0.11, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.69, Test_acc 58.21
2023-10-17 20:14:25,703 [podnet.py] => Task 6, Epoch 18/20 (LR 0.00012) => LSC_loss 0.11, Spatial_loss 1.23, Flat_loss 0.05, Train_acc 98.38, Test_acc 58.10
2023-10-17 20:14:27,351 [podnet.py] => Task 6, Epoch 19/20 (LR 0.00003) => LSC_loss 0.11, Spatial_loss 1.21, Flat_loss 0.05, Train_acc 98.81, Test_acc 58.36
2023-10-17 20:14:28,991 [podnet.py] => Task 6, Epoch 20/20 (LR 0.00000) => LSC_loss 0.11, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.02
2023-10-17 20:14:28,992 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:15:02,960 [podnet.py] => Exemplar size: 1600
2023-10-17 20:15:02,960 [trainer.py] => CNN: {'total': 58.02, '00-09': 66.0, '10-19': 50.2, '20-29': 66.3, '30-39': 57.3, '40-49': 61.8, '50-59': 41.7, '60-69': 57.8, '70-79': 63.1, 'old': 57.73, 'new': 62.4}
2023-10-17 20:15:02,960 [trainer.py] => NME: {'total': 59.02, '00-09': 70.5, '10-19': 56.0, '20-29': 70.4, '30-39': 60.2, '40-49': 64.8, '50-59': 38.9, '60-69': 53.4, '70-79': 58.0, 'old': 59.15, 'new': 57.2}
2023-10-17 20:15:02,960 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02]
2023-10-17 20:15:02,960 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08]
2023-10-17 20:15:02,960 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02]
2023-10-17 20:15:02,960 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16]

2023-10-17 20:15:02,960 [trainer.py] => Average Accuracy (CNN): 67.0442857142857
2023-10-17 20:15:02,960 [trainer.py] => Average Accuracy (NME): 67.26142857142858
2023-10-17 20:15:02,961 [trainer.py] => All params: 517457
2023-10-17 20:15:02,961 [trainer.py] => Trainable params: 517457
2023-10-17 20:15:02,962 [podnet.py] => Learning on 80-85
2023-10-17 20:15:02,993 [podnet.py] => Adaptive factor: 4.123105625617661
2023-10-17 20:15:05,293 [podnet.py] => Task 7, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 4.29, Flat_loss 0.89, Train_acc 69.02, Test_acc 15.26
2023-10-17 20:15:07,569 [podnet.py] => Task 7, Epoch 2/160 (LR 0.09996) => LSC_loss 1.96, Spatial_loss 7.49, Flat_loss 1.64, Train_acc 60.10, Test_acc 23.33
2023-10-17 20:15:09,804 [podnet.py] => Task 7, Epoch 3/160 (LR 0.09991) => LSC_loss 1.31, Spatial_loss 6.18, Flat_loss 1.27, Train_acc 68.98, Test_acc 22.01
2023-10-17 20:15:12,035 [podnet.py] => Task 7, Epoch 4/160 (LR 0.09985) => LSC_loss 1.18, Spatial_loss 6.33, Flat_loss 1.20, Train_acc 70.41, Test_acc 40.25
2023-10-17 20:15:14,263 [podnet.py] => Task 7, Epoch 5/160 (LR 0.09976) => LSC_loss 0.88, Spatial_loss 5.42, Flat_loss 0.97, Train_acc 76.98, Test_acc 36.62
2023-10-17 20:15:16,509 [podnet.py] => Task 7, Epoch 6/160 (LR 0.09965) => LSC_loss 0.82, Spatial_loss 5.14, Flat_loss 0.87, Train_acc 78.90, Test_acc 41.51
2023-10-17 20:15:18,746 [podnet.py] => Task 7, Epoch 7/160 (LR 0.09953) => LSC_loss 0.74, Spatial_loss 5.02, Flat_loss 0.83, Train_acc 80.34, Test_acc 44.76
2023-10-17 20:15:20,998 [podnet.py] => Task 7, Epoch 8/160 (LR 0.09938) => LSC_loss 0.71, Spatial_loss 4.81, Flat_loss 0.76, Train_acc 82.22, Test_acc 39.46
2023-10-17 20:15:23,194 [podnet.py] => Task 7, Epoch 9/160 (LR 0.09922) => LSC_loss 0.69, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 81.51, Test_acc 38.56
2023-10-17 20:15:25,521 [podnet.py] => Task 7, Epoch 10/160 (LR 0.09904) => LSC_loss 0.77, Spatial_loss 5.19, Flat_loss 0.84, Train_acc 80.05, Test_acc 40.32
2023-10-17 20:15:27,785 [podnet.py] => Task 7, Epoch 11/160 (LR 0.09884) => LSC_loss 0.82, Spatial_loss 5.41, Flat_loss 0.86, Train_acc 79.68, Test_acc 24.84
2023-10-17 20:15:30,039 [podnet.py] => Task 7, Epoch 12/160 (LR 0.09862) => LSC_loss 0.83, Spatial_loss 5.33, Flat_loss 0.93, Train_acc 79.41, Test_acc 37.15
2023-10-17 20:15:32,367 [podnet.py] => Task 7, Epoch 13/160 (LR 0.09838) => LSC_loss 0.67, Spatial_loss 5.03, Flat_loss 0.78, Train_acc 82.49, Test_acc 42.59
2023-10-17 20:15:34,640 [podnet.py] => Task 7, Epoch 14/160 (LR 0.09812) => LSC_loss 0.54, Spatial_loss 4.44, Flat_loss 0.66, Train_acc 86.27, Test_acc 43.21
2023-10-17 20:15:36,903 [podnet.py] => Task 7, Epoch 15/160 (LR 0.09785) => LSC_loss 0.48, Spatial_loss 4.30, Flat_loss 0.60, Train_acc 87.88, Test_acc 45.96
2023-10-17 20:15:39,152 [podnet.py] => Task 7, Epoch 16/160 (LR 0.09755) => LSC_loss 0.51, Spatial_loss 4.36, Flat_loss 0.61, Train_acc 87.61, Test_acc 30.49
2023-10-17 20:15:41,432 [podnet.py] => Task 7, Epoch 17/160 (LR 0.09724) => LSC_loss 0.76, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 82.07, Test_acc 44.40
2023-10-17 20:15:43,703 [podnet.py] => Task 7, Epoch 18/160 (LR 0.09691) => LSC_loss 0.66, Spatial_loss 4.70, Flat_loss 0.69, Train_acc 85.22, Test_acc 44.59
2023-10-17 20:15:45,998 [podnet.py] => Task 7, Epoch 19/160 (LR 0.09656) => LSC_loss 0.63, Spatial_loss 4.74, Flat_loss 0.71, Train_acc 83.98, Test_acc 43.38
2023-10-17 20:15:48,219 [podnet.py] => Task 7, Epoch 20/160 (LR 0.09619) => LSC_loss 0.59, Spatial_loss 4.65, Flat_loss 0.68, Train_acc 85.49, Test_acc 44.56
2023-10-17 20:15:50,471 [podnet.py] => Task 7, Epoch 21/160 (LR 0.09581) => LSC_loss 0.44, Spatial_loss 4.15, Flat_loss 0.56, Train_acc 88.90, Test_acc 48.98
2023-10-17 20:15:52,762 [podnet.py] => Task 7, Epoch 22/160 (LR 0.09541) => LSC_loss 0.39, Spatial_loss 3.92, Flat_loss 0.49, Train_acc 91.05, Test_acc 46.92
2023-10-17 20:15:55,087 [podnet.py] => Task 7, Epoch 23/160 (LR 0.09499) => LSC_loss 0.40, Spatial_loss 3.90, Flat_loss 0.49, Train_acc 90.41, Test_acc 49.24
2023-10-17 20:15:57,352 [podnet.py] => Task 7, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.80, Flat_loss 0.48, Train_acc 91.39, Test_acc 51.48
2023-10-17 20:15:59,628 [podnet.py] => Task 7, Epoch 25/160 (LR 0.09410) => LSC_loss 0.39, Spatial_loss 3.68, Flat_loss 0.44, Train_acc 92.10, Test_acc 43.84
2023-10-17 20:16:01,877 [podnet.py] => Task 7, Epoch 26/160 (LR 0.09362) => LSC_loss 0.43, Spatial_loss 3.98, Flat_loss 0.52, Train_acc 88.78, Test_acc 50.72
2023-10-17 20:16:04,060 [podnet.py] => Task 7, Epoch 27/160 (LR 0.09314) => LSC_loss 0.37, Spatial_loss 3.60, Flat_loss 0.43, Train_acc 92.59, Test_acc 46.45
2023-10-17 20:16:06,353 [podnet.py] => Task 7, Epoch 28/160 (LR 0.09263) => LSC_loss 0.44, Spatial_loss 3.88, Flat_loss 0.50, Train_acc 89.71, Test_acc 48.05
2023-10-17 20:16:08,621 [podnet.py] => Task 7, Epoch 29/160 (LR 0.09211) => LSC_loss 0.53, Spatial_loss 4.46, Flat_loss 0.59, Train_acc 86.80, Test_acc 44.42
2023-10-17 20:16:10,876 [podnet.py] => Task 7, Epoch 30/160 (LR 0.09157) => LSC_loss 0.51, Spatial_loss 4.26, Flat_loss 0.56, Train_acc 86.51, Test_acc 43.78
2023-10-17 20:16:13,180 [podnet.py] => Task 7, Epoch 31/160 (LR 0.09102) => LSC_loss 0.53, Spatial_loss 4.39, Flat_loss 0.64, Train_acc 86.17, Test_acc 37.99
2023-10-17 20:16:15,474 [podnet.py] => Task 7, Epoch 32/160 (LR 0.09045) => LSC_loss 0.48, Spatial_loss 4.36, Flat_loss 0.60, Train_acc 88.44, Test_acc 41.18
2023-10-17 20:16:17,650 [podnet.py] => Task 7, Epoch 33/160 (LR 0.08987) => LSC_loss 0.41, Spatial_loss 3.90, Flat_loss 0.50, Train_acc 90.85, Test_acc 49.82
2023-10-17 20:16:19,970 [podnet.py] => Task 7, Epoch 34/160 (LR 0.08927) => LSC_loss 0.45, Spatial_loss 4.08, Flat_loss 0.54, Train_acc 88.83, Test_acc 47.73
2023-10-17 20:16:22,211 [podnet.py] => Task 7, Epoch 35/160 (LR 0.08865) => LSC_loss 0.42, Spatial_loss 3.85, Flat_loss 0.51, Train_acc 90.95, Test_acc 39.74
2023-10-17 20:16:24,418 [podnet.py] => Task 7, Epoch 36/160 (LR 0.08802) => LSC_loss 0.61, Spatial_loss 4.70, Flat_loss 0.68, Train_acc 85.93, Test_acc 44.15
2023-10-17 20:16:26,715 [podnet.py] => Task 7, Epoch 37/160 (LR 0.08738) => LSC_loss 0.55, Spatial_loss 4.51, Flat_loss 0.63, Train_acc 85.29, Test_acc 42.72
2023-10-17 20:16:28,961 [podnet.py] => Task 7, Epoch 38/160 (LR 0.08672) => LSC_loss 0.50, Spatial_loss 4.35, Flat_loss 0.60, Train_acc 88.93, Test_acc 40.95
2023-10-17 20:16:31,126 [podnet.py] => Task 7, Epoch 39/160 (LR 0.08604) => LSC_loss 0.46, Spatial_loss 4.20, Flat_loss 0.58, Train_acc 88.27, Test_acc 47.59
2023-10-17 20:16:33,372 [podnet.py] => Task 7, Epoch 40/160 (LR 0.08536) => LSC_loss 0.43, Spatial_loss 3.91, Flat_loss 0.50, Train_acc 91.20, Test_acc 46.71
2023-10-17 20:16:35,650 [podnet.py] => Task 7, Epoch 41/160 (LR 0.08465) => LSC_loss 0.44, Spatial_loss 4.08, Flat_loss 0.53, Train_acc 89.44, Test_acc 45.46
2023-10-17 20:16:37,931 [podnet.py] => Task 7, Epoch 42/160 (LR 0.08394) => LSC_loss 0.48, Spatial_loss 4.20, Flat_loss 0.56, Train_acc 88.51, Test_acc 48.08
2023-10-17 20:16:40,171 [podnet.py] => Task 7, Epoch 43/160 (LR 0.08321) => LSC_loss 0.67, Spatial_loss 4.81, Flat_loss 0.69, Train_acc 82.63, Test_acc 41.80
2023-10-17 20:16:42,438 [podnet.py] => Task 7, Epoch 44/160 (LR 0.08247) => LSC_loss 0.53, Spatial_loss 4.42, Flat_loss 0.63, Train_acc 86.51, Test_acc 45.07
2023-10-17 20:16:44,716 [podnet.py] => Task 7, Epoch 45/160 (LR 0.08172) => LSC_loss 0.50, Spatial_loss 4.19, Flat_loss 0.58, Train_acc 89.24, Test_acc 44.51
2023-10-17 20:16:47,038 [podnet.py] => Task 7, Epoch 46/160 (LR 0.08095) => LSC_loss 0.50, Spatial_loss 4.58, Flat_loss 0.64, Train_acc 87.49, Test_acc 44.64
2023-10-17 20:16:49,276 [podnet.py] => Task 7, Epoch 47/160 (LR 0.08018) => LSC_loss 0.42, Spatial_loss 4.13, Flat_loss 0.55, Train_acc 89.80, Test_acc 49.64
2023-10-17 20:16:51,446 [podnet.py] => Task 7, Epoch 48/160 (LR 0.07939) => LSC_loss 0.43, Spatial_loss 3.79, Flat_loss 0.49, Train_acc 92.34, Test_acc 34.29
2023-10-17 20:16:53,722 [podnet.py] => Task 7, Epoch 49/160 (LR 0.07859) => LSC_loss 0.76, Spatial_loss 5.15, Flat_loss 0.78, Train_acc 82.00, Test_acc 34.24
2023-10-17 20:16:55,983 [podnet.py] => Task 7, Epoch 50/160 (LR 0.07778) => LSC_loss 0.70, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 82.66, Test_acc 42.85
2023-10-17 20:16:58,302 [podnet.py] => Task 7, Epoch 51/160 (LR 0.07696) => LSC_loss 0.51, Spatial_loss 4.52, Flat_loss 0.65, Train_acc 87.49, Test_acc 47.14
2023-10-17 20:17:00,548 [podnet.py] => Task 7, Epoch 52/160 (LR 0.07612) => LSC_loss 0.47, Spatial_loss 4.28, Flat_loss 0.59, Train_acc 88.95, Test_acc 41.54
2023-10-17 20:17:02,799 [podnet.py] => Task 7, Epoch 53/160 (LR 0.07528) => LSC_loss 0.52, Spatial_loss 4.29, Flat_loss 0.62, Train_acc 88.39, Test_acc 43.86
2023-10-17 20:17:05,086 [podnet.py] => Task 7, Epoch 54/160 (LR 0.07443) => LSC_loss 0.54, Spatial_loss 4.51, Flat_loss 0.64, Train_acc 86.61, Test_acc 45.40
2023-10-17 20:17:07,344 [podnet.py] => Task 7, Epoch 55/160 (LR 0.07357) => LSC_loss 0.46, Spatial_loss 4.31, Flat_loss 0.62, Train_acc 88.66, Test_acc 49.44
2023-10-17 20:17:09,559 [podnet.py] => Task 7, Epoch 56/160 (LR 0.07270) => LSC_loss 0.36, Spatial_loss 4.06, Flat_loss 0.53, Train_acc 91.68, Test_acc 48.33
2023-10-17 20:17:11,802 [podnet.py] => Task 7, Epoch 57/160 (LR 0.07182) => LSC_loss 0.38, Spatial_loss 3.83, Flat_loss 0.48, Train_acc 91.78, Test_acc 47.21
2023-10-17 20:17:13,989 [podnet.py] => Task 7, Epoch 58/160 (LR 0.07093) => LSC_loss 0.39, Spatial_loss 3.94, Flat_loss 0.49, Train_acc 91.51, Test_acc 49.44
2023-10-17 20:17:16,227 [podnet.py] => Task 7, Epoch 59/160 (LR 0.07004) => LSC_loss 0.41, Spatial_loss 3.97, Flat_loss 0.51, Train_acc 90.98, Test_acc 41.04
2023-10-17 20:17:18,474 [podnet.py] => Task 7, Epoch 60/160 (LR 0.06913) => LSC_loss 0.47, Spatial_loss 4.08, Flat_loss 0.54, Train_acc 89.88, Test_acc 48.69
2023-10-17 20:17:20,741 [podnet.py] => Task 7, Epoch 61/160 (LR 0.06822) => LSC_loss 0.52, Spatial_loss 4.36, Flat_loss 0.60, Train_acc 87.32, Test_acc 47.47
2023-10-17 20:17:22,994 [podnet.py] => Task 7, Epoch 62/160 (LR 0.06731) => LSC_loss 0.38, Spatial_loss 3.99, Flat_loss 0.51, Train_acc 91.34, Test_acc 49.22
2023-10-17 20:17:25,220 [podnet.py] => Task 7, Epoch 63/160 (LR 0.06638) => LSC_loss 0.41, Spatial_loss 4.02, Flat_loss 0.52, Train_acc 90.61, Test_acc 47.80
2023-10-17 20:17:27,449 [podnet.py] => Task 7, Epoch 64/160 (LR 0.06545) => LSC_loss 0.48, Spatial_loss 3.80, Flat_loss 0.49, Train_acc 92.22, Test_acc 42.99
2023-10-17 20:17:29,652 [podnet.py] => Task 7, Epoch 65/160 (LR 0.06451) => LSC_loss 1.15, Spatial_loss 6.15, Flat_loss 1.00, Train_acc 73.61, Test_acc 42.86
2023-10-17 20:17:31,927 [podnet.py] => Task 7, Epoch 66/160 (LR 0.06357) => LSC_loss 0.66, Spatial_loss 4.87, Flat_loss 0.74, Train_acc 82.88, Test_acc 44.31
2023-10-17 20:17:34,162 [podnet.py] => Task 7, Epoch 67/160 (LR 0.06262) => LSC_loss 0.46, Spatial_loss 4.32, Flat_loss 0.60, Train_acc 88.78, Test_acc 48.94
2023-10-17 20:17:36,423 [podnet.py] => Task 7, Epoch 68/160 (LR 0.06167) => LSC_loss 0.51, Spatial_loss 4.13, Flat_loss 0.58, Train_acc 89.29, Test_acc 50.01
2023-10-17 20:17:38,651 [podnet.py] => Task 7, Epoch 69/160 (LR 0.06072) => LSC_loss 0.39, Spatial_loss 3.96, Flat_loss 0.54, Train_acc 90.88, Test_acc 47.35
2023-10-17 20:17:40,933 [podnet.py] => Task 7, Epoch 70/160 (LR 0.05975) => LSC_loss 0.36, Spatial_loss 3.66, Flat_loss 0.45, Train_acc 92.83, Test_acc 52.21
2023-10-17 20:17:43,198 [podnet.py] => Task 7, Epoch 71/160 (LR 0.05879) => LSC_loss 0.37, Spatial_loss 3.75, Flat_loss 0.47, Train_acc 92.68, Test_acc 44.64
2023-10-17 20:17:45,410 [podnet.py] => Task 7, Epoch 72/160 (LR 0.05782) => LSC_loss 0.50, Spatial_loss 4.37, Flat_loss 0.60, Train_acc 87.98, Test_acc 49.16
2023-10-17 20:17:47,657 [podnet.py] => Task 7, Epoch 73/160 (LR 0.05685) => LSC_loss 0.39, Spatial_loss 3.84, Flat_loss 0.52, Train_acc 91.49, Test_acc 49.32
2023-10-17 20:17:49,969 [podnet.py] => Task 7, Epoch 74/160 (LR 0.05588) => LSC_loss 0.35, Spatial_loss 3.64, Flat_loss 0.45, Train_acc 93.27, Test_acc 48.75
2023-10-17 20:17:52,223 [podnet.py] => Task 7, Epoch 75/160 (LR 0.05490) => LSC_loss 0.49, Spatial_loss 4.17, Flat_loss 0.56, Train_acc 89.00, Test_acc 49.36
2023-10-17 20:17:54,464 [podnet.py] => Task 7, Epoch 76/160 (LR 0.05392) => LSC_loss 0.44, Spatial_loss 3.92, Flat_loss 0.53, Train_acc 90.54, Test_acc 46.65
2023-10-17 20:17:56,750 [podnet.py] => Task 7, Epoch 77/160 (LR 0.05294) => LSC_loss 0.62, Spatial_loss 4.83, Flat_loss 0.68, Train_acc 84.32, Test_acc 45.33
2023-10-17 20:17:59,024 [podnet.py] => Task 7, Epoch 78/160 (LR 0.05196) => LSC_loss 0.55, Spatial_loss 4.34, Flat_loss 0.62, Train_acc 86.76, Test_acc 49.84
2023-10-17 20:18:01,223 [podnet.py] => Task 7, Epoch 79/160 (LR 0.05098) => LSC_loss 0.33, Spatial_loss 3.63, Flat_loss 0.46, Train_acc 92.39, Test_acc 52.07
2023-10-17 20:18:03,519 [podnet.py] => Task 7, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 3.44, Flat_loss 0.42, Train_acc 94.66, Test_acc 50.52
2023-10-17 20:18:05,733 [podnet.py] => Task 7, Epoch 81/160 (LR 0.04902) => LSC_loss 0.38, Spatial_loss 3.73, Flat_loss 0.46, Train_acc 92.66, Test_acc 50.02
2023-10-17 20:18:07,984 [podnet.py] => Task 7, Epoch 82/160 (LR 0.04804) => LSC_loss 0.41, Spatial_loss 3.78, Flat_loss 0.49, Train_acc 91.46, Test_acc 50.60
2023-10-17 20:18:10,237 [podnet.py] => Task 7, Epoch 83/160 (LR 0.04706) => LSC_loss 0.36, Spatial_loss 3.76, Flat_loss 0.46, Train_acc 92.32, Test_acc 49.78
2023-10-17 20:18:12,488 [podnet.py] => Task 7, Epoch 84/160 (LR 0.04608) => LSC_loss 0.33, Spatial_loss 3.77, Flat_loss 0.46, Train_acc 92.73, Test_acc 51.22
2023-10-17 20:18:14,763 [podnet.py] => Task 7, Epoch 85/160 (LR 0.04510) => LSC_loss 0.28, Spatial_loss 3.34, Flat_loss 0.40, Train_acc 94.59, Test_acc 51.74
2023-10-17 20:18:16,982 [podnet.py] => Task 7, Epoch 86/160 (LR 0.04412) => LSC_loss 0.27, Spatial_loss 3.20, Flat_loss 0.38, Train_acc 95.71, Test_acc 53.61
2023-10-17 20:18:19,224 [podnet.py] => Task 7, Epoch 87/160 (LR 0.04315) => LSC_loss 0.32, Spatial_loss 3.36, Flat_loss 0.41, Train_acc 93.37, Test_acc 50.39
2023-10-17 20:18:21,524 [podnet.py] => Task 7, Epoch 88/160 (LR 0.04218) => LSC_loss 0.32, Spatial_loss 3.42, Flat_loss 0.42, Train_acc 93.10, Test_acc 52.01
2023-10-17 20:18:23,808 [podnet.py] => Task 7, Epoch 89/160 (LR 0.04121) => LSC_loss 0.25, Spatial_loss 3.20, Flat_loss 0.36, Train_acc 95.27, Test_acc 53.24
2023-10-17 20:18:26,046 [podnet.py] => Task 7, Epoch 90/160 (LR 0.04025) => LSC_loss 0.29, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 96.12, Test_acc 51.39
2023-10-17 20:18:28,268 [podnet.py] => Task 7, Epoch 91/160 (LR 0.03928) => LSC_loss 0.30, Spatial_loss 3.26, Flat_loss 0.40, Train_acc 93.07, Test_acc 52.34
2023-10-17 20:18:30,579 [podnet.py] => Task 7, Epoch 92/160 (LR 0.03833) => LSC_loss 0.27, Spatial_loss 3.17, Flat_loss 0.36, Train_acc 95.41, Test_acc 52.45
2023-10-17 20:18:32,797 [podnet.py] => Task 7, Epoch 93/160 (LR 0.03738) => LSC_loss 0.32, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 95.17, Test_acc 53.09
2023-10-17 20:18:35,113 [podnet.py] => Task 7, Epoch 94/160 (LR 0.03643) => LSC_loss 0.27, Spatial_loss 3.23, Flat_loss 0.37, Train_acc 94.68, Test_acc 51.40
2023-10-17 20:18:37,364 [podnet.py] => Task 7, Epoch 95/160 (LR 0.03549) => LSC_loss 0.25, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 95.41, Test_acc 53.54
2023-10-17 20:18:39,614 [podnet.py] => Task 7, Epoch 96/160 (LR 0.03455) => LSC_loss 0.26, Spatial_loss 3.04, Flat_loss 0.34, Train_acc 95.00, Test_acc 53.20
2023-10-17 20:18:41,933 [podnet.py] => Task 7, Epoch 97/160 (LR 0.03362) => LSC_loss 0.26, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 95.71, Test_acc 51.09
2023-10-17 20:18:44,192 [podnet.py] => Task 7, Epoch 98/160 (LR 0.03269) => LSC_loss 0.43, Spatial_loss 3.59, Flat_loss 0.46, Train_acc 90.88, Test_acc 50.60
2023-10-17 20:18:46,478 [podnet.py] => Task 7, Epoch 99/160 (LR 0.03178) => LSC_loss 0.31, Spatial_loss 3.16, Flat_loss 0.37, Train_acc 94.56, Test_acc 54.12
2023-10-17 20:18:48,760 [podnet.py] => Task 7, Epoch 100/160 (LR 0.03087) => LSC_loss 0.28, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 95.05, Test_acc 52.33
2023-10-17 20:18:51,066 [podnet.py] => Task 7, Epoch 101/160 (LR 0.02996) => LSC_loss 0.28, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 95.29, Test_acc 54.06
2023-10-17 20:18:53,303 [podnet.py] => Task 7, Epoch 102/160 (LR 0.02907) => LSC_loss 0.31, Spatial_loss 3.23, Flat_loss 0.39, Train_acc 94.46, Test_acc 52.58
2023-10-17 20:18:55,511 [podnet.py] => Task 7, Epoch 103/160 (LR 0.02818) => LSC_loss 0.30, Spatial_loss 3.05, Flat_loss 0.36, Train_acc 95.41, Test_acc 51.40
2023-10-17 20:18:57,820 [podnet.py] => Task 7, Epoch 104/160 (LR 0.02730) => LSC_loss 0.27, Spatial_loss 3.01, Flat_loss 0.34, Train_acc 94.90, Test_acc 52.41
2023-10-17 20:19:00,054 [podnet.py] => Task 7, Epoch 105/160 (LR 0.02643) => LSC_loss 0.25, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 95.71, Test_acc 53.94
2023-10-17 20:19:02,386 [podnet.py] => Task 7, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 2.88, Flat_loss 0.32, Train_acc 95.49, Test_acc 53.28
2023-10-17 20:19:04,664 [podnet.py] => Task 7, Epoch 107/160 (LR 0.02472) => LSC_loss 0.30, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 94.83, Test_acc 52.71
2023-10-17 20:19:06,929 [podnet.py] => Task 7, Epoch 108/160 (LR 0.02388) => LSC_loss 0.30, Spatial_loss 2.98, Flat_loss 0.35, Train_acc 95.34, Test_acc 54.69
2023-10-17 20:19:09,235 [podnet.py] => Task 7, Epoch 109/160 (LR 0.02304) => LSC_loss 0.24, Spatial_loss 2.83, Flat_loss 0.32, Train_acc 94.83, Test_acc 53.49
2023-10-17 20:19:11,525 [podnet.py] => Task 7, Epoch 110/160 (LR 0.02222) => LSC_loss 0.23, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 96.44, Test_acc 54.87
2023-10-17 20:19:13,822 [podnet.py] => Task 7, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 95.98, Test_acc 53.78
2023-10-17 20:19:16,095 [podnet.py] => Task 7, Epoch 112/160 (LR 0.02061) => LSC_loss 0.28, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 94.54, Test_acc 53.36
2023-10-17 20:19:18,320 [podnet.py] => Task 7, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 96.39, Test_acc 53.47
2023-10-17 20:19:20,612 [podnet.py] => Task 7, Epoch 114/160 (LR 0.01905) => LSC_loss 0.26, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 95.76, Test_acc 52.72
2023-10-17 20:19:22,896 [podnet.py] => Task 7, Epoch 115/160 (LR 0.01828) => LSC_loss 0.28, Spatial_loss 2.73, Flat_loss 0.31, Train_acc 95.85, Test_acc 52.55
2023-10-17 20:19:25,172 [podnet.py] => Task 7, Epoch 116/160 (LR 0.01753) => LSC_loss 0.33, Spatial_loss 2.93, Flat_loss 0.33, Train_acc 95.32, Test_acc 54.80
2023-10-17 20:19:27,445 [podnet.py] => Task 7, Epoch 117/160 (LR 0.01679) => LSC_loss 0.34, Spatial_loss 2.93, Flat_loss 0.33, Train_acc 94.80, Test_acc 53.72
2023-10-17 20:19:29,739 [podnet.py] => Task 7, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 2.75, Flat_loss 0.31, Train_acc 96.51, Test_acc 54.67
2023-10-17 20:19:32,020 [podnet.py] => Task 7, Epoch 119/160 (LR 0.01535) => LSC_loss 0.22, Spatial_loss 2.60, Flat_loss 0.28, Train_acc 96.07, Test_acc 54.28
2023-10-17 20:19:34,321 [podnet.py] => Task 7, Epoch 120/160 (LR 0.01464) => LSC_loss 0.21, Spatial_loss 2.51, Flat_loss 0.26, Train_acc 96.78, Test_acc 54.22
2023-10-17 20:19:36,587 [podnet.py] => Task 7, Epoch 121/160 (LR 0.01396) => LSC_loss 0.22, Spatial_loss 2.50, Flat_loss 0.25, Train_acc 96.71, Test_acc 53.88
2023-10-17 20:19:38,844 [podnet.py] => Task 7, Epoch 122/160 (LR 0.01328) => LSC_loss 0.23, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 97.20, Test_acc 53.39
2023-10-17 20:19:41,059 [podnet.py] => Task 7, Epoch 123/160 (LR 0.01262) => LSC_loss 0.31, Spatial_loss 2.69, Flat_loss 0.30, Train_acc 96.05, Test_acc 53.76
2023-10-17 20:19:43,281 [podnet.py] => Task 7, Epoch 124/160 (LR 0.01198) => LSC_loss 0.26, Spatial_loss 2.84, Flat_loss 0.32, Train_acc 94.73, Test_acc 54.73
2023-10-17 20:19:45,512 [podnet.py] => Task 7, Epoch 125/160 (LR 0.01135) => LSC_loss 0.23, Spatial_loss 2.48, Flat_loss 0.28, Train_acc 96.80, Test_acc 54.73
2023-10-17 20:19:47,765 [podnet.py] => Task 7, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 2.60, Flat_loss 0.30, Train_acc 96.12, Test_acc 54.49
2023-10-17 20:19:50,026 [podnet.py] => Task 7, Epoch 127/160 (LR 0.01013) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.30, Train_acc 96.10, Test_acc 54.92
2023-10-17 20:19:52,276 [podnet.py] => Task 7, Epoch 128/160 (LR 0.00955) => LSC_loss 0.28, Spatial_loss 2.60, Flat_loss 0.28, Train_acc 96.02, Test_acc 54.36
2023-10-17 20:19:54,556 [podnet.py] => Task 7, Epoch 129/160 (LR 0.00898) => LSC_loss 0.22, Spatial_loss 2.53, Flat_loss 0.28, Train_acc 96.78, Test_acc 55.42
2023-10-17 20:19:56,793 [podnet.py] => Task 7, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 96.76, Test_acc 54.71
2023-10-17 20:19:59,071 [podnet.py] => Task 7, Epoch 131/160 (LR 0.00789) => LSC_loss 0.20, Spatial_loss 2.42, Flat_loss 0.25, Train_acc 97.59, Test_acc 55.09
2023-10-17 20:20:01,353 [podnet.py] => Task 7, Epoch 132/160 (LR 0.00737) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.26, Train_acc 96.68, Test_acc 55.24
2023-10-17 20:20:03,632 [podnet.py] => Task 7, Epoch 133/160 (LR 0.00686) => LSC_loss 0.30, Spatial_loss 2.49, Flat_loss 0.28, Train_acc 96.66, Test_acc 55.89
2023-10-17 20:20:05,923 [podnet.py] => Task 7, Epoch 134/160 (LR 0.00638) => LSC_loss 0.24, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 95.54, Test_acc 53.99
2023-10-17 20:20:08,164 [podnet.py] => Task 7, Epoch 135/160 (LR 0.00590) => LSC_loss 0.21, Spatial_loss 2.46, Flat_loss 0.25, Train_acc 96.32, Test_acc 55.29
2023-10-17 20:20:10,447 [podnet.py] => Task 7, Epoch 136/160 (LR 0.00545) => LSC_loss 0.21, Spatial_loss 2.40, Flat_loss 0.24, Train_acc 97.00, Test_acc 55.22
2023-10-17 20:20:12,692 [podnet.py] => Task 7, Epoch 137/160 (LR 0.00501) => LSC_loss 0.24, Spatial_loss 2.32, Flat_loss 0.25, Train_acc 96.93, Test_acc 55.01
2023-10-17 20:20:14,930 [podnet.py] => Task 7, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 97.02, Test_acc 54.39
2023-10-17 20:20:17,130 [podnet.py] => Task 7, Epoch 139/160 (LR 0.00419) => LSC_loss 0.24, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 97.05, Test_acc 55.38
2023-10-17 20:20:19,322 [podnet.py] => Task 7, Epoch 140/160 (LR 0.00381) => LSC_loss 0.21, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.15, Test_acc 55.06
2023-10-17 20:20:21,520 [podnet.py] => Task 7, Epoch 141/160 (LR 0.00344) => LSC_loss 0.20, Spatial_loss 2.26, Flat_loss 0.24, Train_acc 97.27, Test_acc 55.28
2023-10-17 20:20:23,791 [podnet.py] => Task 7, Epoch 142/160 (LR 0.00309) => LSC_loss 0.21, Spatial_loss 2.26, Flat_loss 0.24, Train_acc 97.27, Test_acc 55.12
2023-10-17 20:20:26,041 [podnet.py] => Task 7, Epoch 143/160 (LR 0.00276) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.24, Train_acc 97.29, Test_acc 55.33
2023-10-17 20:20:28,259 [podnet.py] => Task 7, Epoch 144/160 (LR 0.00245) => LSC_loss 0.22, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 96.78, Test_acc 55.39
2023-10-17 20:20:30,464 [podnet.py] => Task 7, Epoch 145/160 (LR 0.00215) => LSC_loss 0.20, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.46, Test_acc 55.34
2023-10-17 20:20:32,670 [podnet.py] => Task 7, Epoch 146/160 (LR 0.00188) => LSC_loss 0.20, Spatial_loss 2.23, Flat_loss 0.24, Train_acc 97.37, Test_acc 55.46
2023-10-17 20:20:34,940 [podnet.py] => Task 7, Epoch 147/160 (LR 0.00162) => LSC_loss 0.21, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.12, Test_acc 54.87
2023-10-17 20:20:37,268 [podnet.py] => Task 7, Epoch 148/160 (LR 0.00138) => LSC_loss 0.21, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 97.44, Test_acc 55.00
2023-10-17 20:20:39,473 [podnet.py] => Task 7, Epoch 149/160 (LR 0.00116) => LSC_loss 0.22, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 97.10, Test_acc 55.29
2023-10-17 20:20:41,816 [podnet.py] => Task 7, Epoch 150/160 (LR 0.00096) => LSC_loss 0.20, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 97.83, Test_acc 55.80
2023-10-17 20:20:44,110 [podnet.py] => Task 7, Epoch 151/160 (LR 0.00078) => LSC_loss 0.21, Spatial_loss 2.25, Flat_loss 0.24, Train_acc 97.05, Test_acc 55.58
2023-10-17 20:20:46,398 [podnet.py] => Task 7, Epoch 152/160 (LR 0.00062) => LSC_loss 0.19, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 97.05, Test_acc 54.82
2023-10-17 20:20:48,647 [podnet.py] => Task 7, Epoch 153/160 (LR 0.00047) => LSC_loss 0.22, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.22, Test_acc 55.28
2023-10-17 20:20:50,876 [podnet.py] => Task 7, Epoch 154/160 (LR 0.00035) => LSC_loss 0.20, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 97.00, Test_acc 55.18
2023-10-17 20:20:53,142 [podnet.py] => Task 7, Epoch 155/160 (LR 0.00024) => LSC_loss 0.23, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.56, Test_acc 54.87
2023-10-17 20:20:55,453 [podnet.py] => Task 7, Epoch 156/160 (LR 0.00015) => LSC_loss 0.19, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 97.59, Test_acc 55.01
2023-10-17 20:20:57,746 [podnet.py] => Task 7, Epoch 157/160 (LR 0.00009) => LSC_loss 0.22, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 97.39, Test_acc 55.33
2023-10-17 20:21:00,048 [podnet.py] => Task 7, Epoch 158/160 (LR 0.00004) => LSC_loss 0.21, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 97.41, Test_acc 54.96
2023-10-17 20:21:02,347 [podnet.py] => Task 7, Epoch 159/160 (LR 0.00001) => LSC_loss 0.22, Spatial_loss 2.19, Flat_loss 0.24, Train_acc 97.00, Test_acc 55.09
2023-10-17 20:21:04,621 [podnet.py] => Task 7, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.37, Test_acc 54.89
2023-10-17 20:21:04,625 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:21:04,625 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:21:39,749 [podnet.py] => The size of finetune dataset: 1700
2023-10-17 20:21:41,463 [podnet.py] => Task 7, Epoch 1/20 (LR 0.00497) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 93.59, Test_acc 55.02
2023-10-17 20:21:43,093 [podnet.py] => Task 7, Epoch 2/20 (LR 0.00488) => LSC_loss 0.16, Spatial_loss 2.23, Flat_loss 0.18, Train_acc 98.06, Test_acc 55.56
2023-10-17 20:21:44,754 [podnet.py] => Task 7, Epoch 3/20 (LR 0.00473) => LSC_loss 0.14, Spatial_loss 2.21, Flat_loss 0.17, Train_acc 98.65, Test_acc 55.80
2023-10-17 20:21:46,433 [podnet.py] => Task 7, Epoch 4/20 (LR 0.00452) => LSC_loss 0.14, Spatial_loss 2.20, Flat_loss 0.17, Train_acc 98.94, Test_acc 56.45
2023-10-17 20:21:48,109 [podnet.py] => Task 7, Epoch 5/20 (LR 0.00427) => LSC_loss 0.14, Spatial_loss 2.13, Flat_loss 0.16, Train_acc 98.71, Test_acc 56.25
2023-10-17 20:21:49,800 [podnet.py] => Task 7, Epoch 6/20 (LR 0.00397) => LSC_loss 0.14, Spatial_loss 2.15, Flat_loss 0.16, Train_acc 98.76, Test_acc 56.18
2023-10-17 20:21:51,465 [podnet.py] => Task 7, Epoch 7/20 (LR 0.00363) => LSC_loss 0.14, Spatial_loss 2.15, Flat_loss 0.16, Train_acc 98.94, Test_acc 56.21
2023-10-17 20:21:53,147 [podnet.py] => Task 7, Epoch 8/20 (LR 0.00327) => LSC_loss 0.14, Spatial_loss 2.14, Flat_loss 0.16, Train_acc 98.65, Test_acc 56.00
2023-10-17 20:21:54,834 [podnet.py] => Task 7, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 2.19, Flat_loss 0.16, Train_acc 98.94, Test_acc 56.29
2023-10-17 20:21:56,474 [podnet.py] => Task 7, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 2.14, Flat_loss 0.16, Train_acc 99.06, Test_acc 56.73
2023-10-17 20:21:58,137 [podnet.py] => Task 7, Epoch 11/20 (LR 0.00211) => LSC_loss 0.13, Spatial_loss 2.11, Flat_loss 0.16, Train_acc 99.00, Test_acc 56.44
2023-10-17 20:21:59,844 [podnet.py] => Task 7, Epoch 12/20 (LR 0.00173) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.53, Test_acc 56.40
2023-10-17 20:22:01,542 [podnet.py] => Task 7, Epoch 13/20 (LR 0.00137) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.88, Test_acc 56.26
2023-10-17 20:22:03,185 [podnet.py] => Task 7, Epoch 14/20 (LR 0.00103) => LSC_loss 0.15, Spatial_loss 2.14, Flat_loss 0.15, Train_acc 98.35, Test_acc 56.45
2023-10-17 20:22:04,824 [podnet.py] => Task 7, Epoch 15/20 (LR 0.00073) => LSC_loss 0.15, Spatial_loss 2.09, Flat_loss 0.16, Train_acc 98.47, Test_acc 56.54
2023-10-17 20:22:06,522 [podnet.py] => Task 7, Epoch 16/20 (LR 0.00048) => LSC_loss 0.14, Spatial_loss 2.11, Flat_loss 0.15, Train_acc 98.76, Test_acc 56.31
2023-10-17 20:22:08,176 [podnet.py] => Task 7, Epoch 17/20 (LR 0.00027) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.71, Test_acc 56.24
2023-10-17 20:22:09,898 [podnet.py] => Task 7, Epoch 18/20 (LR 0.00012) => LSC_loss 0.13, Spatial_loss 2.10, Flat_loss 0.15, Train_acc 99.29, Test_acc 56.40
2023-10-17 20:22:11,598 [podnet.py] => Task 7, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 2.09, Flat_loss 0.15, Train_acc 99.06, Test_acc 56.40
2023-10-17 20:22:13,253 [podnet.py] => Task 7, Epoch 20/20 (LR 0.00000) => LSC_loss 0.14, Spatial_loss 2.11, Flat_loss 0.16, Train_acc 99.06, Test_acc 56.36
2023-10-17 20:22:13,254 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:22:48,872 [podnet.py] => Exemplar size: 1700
2023-10-17 20:22:48,872 [trainer.py] => CNN: {'total': 56.36, '00-09': 65.4, '10-19': 47.1, '20-29': 62.9, '30-39': 55.8, '40-49': 61.9, '50-59': 42.3, '60-69': 52.2, '70-79': 54.8, '80-89': 73.4, 'old': 55.3, 'new': 73.4}
2023-10-17 20:22:48,872 [trainer.py] => NME: {'total': 57.14, '00-09': 69.9, '10-19': 52.0, '20-29': 68.1, '30-39': 58.8, '40-49': 63.6, '50-59': 38.7, '60-69': 50.1, '70-79': 51.3, '80-89': 66.4, 'old': 56.56, 'new': 66.4}
2023-10-17 20:22:48,873 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36]
2023-10-17 20:22:48,876 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08]
2023-10-17 20:22:48,876 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14]
2023-10-17 20:22:48,876 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48]

2023-10-17 20:22:48,876 [trainer.py] => Average Accuracy (CNN): 65.70875
2023-10-17 20:22:48,876 [trainer.py] => Average Accuracy (NME): 65.99625
2023-10-17 20:22:48,877 [trainer.py] => All params: 520657
2023-10-17 20:22:48,877 [trainer.py] => Trainable params: 520657
2023-10-17 20:22:48,878 [podnet.py] => Learning on 85-90
2023-10-17 20:22:48,915 [podnet.py] => Adaptive factor: 4.242640687119285
2023-10-17 20:22:51,261 [podnet.py] => Task 8, Epoch 1/160 (LR 0.09999) => LSC_loss 1.82, Spatial_loss 3.17, Flat_loss 0.61, Train_acc 65.36, Test_acc 44.04
2023-10-17 20:22:53,562 [podnet.py] => Task 8, Epoch 2/160 (LR 0.09996) => LSC_loss 0.72, Spatial_loss 3.10, Flat_loss 0.40, Train_acc 80.60, Test_acc 48.79
2023-10-17 20:22:55,860 [podnet.py] => Task 8, Epoch 3/160 (LR 0.09991) => LSC_loss 0.62, Spatial_loss 3.04, Flat_loss 0.36, Train_acc 83.93, Test_acc 46.53
2023-10-17 20:22:58,106 [podnet.py] => Task 8, Epoch 4/160 (LR 0.09985) => LSC_loss 0.62, Spatial_loss 3.01, Flat_loss 0.35, Train_acc 83.74, Test_acc 46.76
2023-10-17 20:23:00,362 [podnet.py] => Task 8, Epoch 5/160 (LR 0.09976) => LSC_loss 0.57, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 85.07, Test_acc 47.01
2023-10-17 20:23:02,626 [podnet.py] => Task 8, Epoch 6/160 (LR 0.09965) => LSC_loss 0.56, Spatial_loss 3.01, Flat_loss 0.33, Train_acc 85.76, Test_acc 49.01
2023-10-17 20:23:04,925 [podnet.py] => Task 8, Epoch 7/160 (LR 0.09953) => LSC_loss 0.52, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 85.95, Test_acc 47.14
2023-10-17 20:23:07,182 [podnet.py] => Task 8, Epoch 8/160 (LR 0.09938) => LSC_loss 0.52, Spatial_loss 3.14, Flat_loss 0.34, Train_acc 86.33, Test_acc 43.11
2023-10-17 20:23:09,483 [podnet.py] => Task 8, Epoch 9/160 (LR 0.09922) => LSC_loss 0.51, Spatial_loss 2.94, Flat_loss 0.32, Train_acc 86.67, Test_acc 49.08
2023-10-17 20:23:11,688 [podnet.py] => Task 8, Epoch 10/160 (LR 0.09904) => LSC_loss 0.47, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 88.38, Test_acc 47.50
2023-10-17 20:23:13,962 [podnet.py] => Task 8, Epoch 11/160 (LR 0.09884) => LSC_loss 0.48, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 88.29, Test_acc 49.18
2023-10-17 20:23:16,225 [podnet.py] => Task 8, Epoch 12/160 (LR 0.09862) => LSC_loss 0.47, Spatial_loss 2.88, Flat_loss 0.31, Train_acc 88.31, Test_acc 48.18
2023-10-17 20:23:18,546 [podnet.py] => Task 8, Epoch 13/160 (LR 0.09838) => LSC_loss 0.47, Spatial_loss 2.96, Flat_loss 0.32, Train_acc 88.12, Test_acc 47.87
2023-10-17 20:23:20,855 [podnet.py] => Task 8, Epoch 14/160 (LR 0.09812) => LSC_loss 0.46, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 88.57, Test_acc 48.57
2023-10-17 20:23:23,191 [podnet.py] => Task 8, Epoch 15/160 (LR 0.09785) => LSC_loss 0.45, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 88.86, Test_acc 47.09
2023-10-17 20:23:25,472 [podnet.py] => Task 8, Epoch 16/160 (LR 0.09755) => LSC_loss 0.44, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 88.83, Test_acc 49.42
2023-10-17 20:23:27,787 [podnet.py] => Task 8, Epoch 17/160 (LR 0.09724) => LSC_loss 0.43, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 89.36, Test_acc 48.12
2023-10-17 20:23:30,072 [podnet.py] => Task 8, Epoch 18/160 (LR 0.09691) => LSC_loss 0.43, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 89.69, Test_acc 49.40
2023-10-17 20:23:32,403 [podnet.py] => Task 8, Epoch 19/160 (LR 0.09656) => LSC_loss 0.41, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 90.24, Test_acc 46.47
2023-10-17 20:23:34,629 [podnet.py] => Task 8, Epoch 20/160 (LR 0.09619) => LSC_loss 0.40, Spatial_loss 2.92, Flat_loss 0.31, Train_acc 90.00, Test_acc 46.10
2023-10-17 20:23:36,907 [podnet.py] => Task 8, Epoch 21/160 (LR 0.09581) => LSC_loss 0.43, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 89.10, Test_acc 45.21
2023-10-17 20:23:39,141 [podnet.py] => Task 8, Epoch 22/160 (LR 0.09541) => LSC_loss 0.42, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 89.31, Test_acc 49.02
2023-10-17 20:23:41,458 [podnet.py] => Task 8, Epoch 23/160 (LR 0.09499) => LSC_loss 0.41, Spatial_loss 2.95, Flat_loss 0.33, Train_acc 90.10, Test_acc 45.64
2023-10-17 20:23:43,791 [podnet.py] => Task 8, Epoch 24/160 (LR 0.09455) => LSC_loss 0.40, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 90.38, Test_acc 46.77
2023-10-17 20:23:46,069 [podnet.py] => Task 8, Epoch 25/160 (LR 0.09410) => LSC_loss 0.40, Spatial_loss 2.91, Flat_loss 0.32, Train_acc 90.45, Test_acc 48.62
2023-10-17 20:23:48,382 [podnet.py] => Task 8, Epoch 26/160 (LR 0.09362) => LSC_loss 0.39, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 90.55, Test_acc 47.79
2023-10-17 20:23:50,684 [podnet.py] => Task 8, Epoch 27/160 (LR 0.09314) => LSC_loss 0.38, Spatial_loss 2.89, Flat_loss 0.32, Train_acc 90.45, Test_acc 47.33
2023-10-17 20:23:52,970 [podnet.py] => Task 8, Epoch 28/160 (LR 0.09263) => LSC_loss 0.36, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 91.79, Test_acc 48.02
2023-10-17 20:23:55,292 [podnet.py] => Task 8, Epoch 29/160 (LR 0.09211) => LSC_loss 0.39, Spatial_loss 2.83, Flat_loss 0.31, Train_acc 90.93, Test_acc 46.43
2023-10-17 20:23:57,517 [podnet.py] => Task 8, Epoch 30/160 (LR 0.09157) => LSC_loss 0.37, Spatial_loss 2.89, Flat_loss 0.32, Train_acc 90.86, Test_acc 49.57
2023-10-17 20:23:59,791 [podnet.py] => Task 8, Epoch 31/160 (LR 0.09102) => LSC_loss 0.38, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 91.02, Test_acc 45.46
2023-10-17 20:24:02,056 [podnet.py] => Task 8, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 2.91, Flat_loss 0.32, Train_acc 91.45, Test_acc 48.24
2023-10-17 20:24:04,394 [podnet.py] => Task 8, Epoch 33/160 (LR 0.08987) => LSC_loss 0.36, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 91.67, Test_acc 46.87
2023-10-17 20:24:06,667 [podnet.py] => Task 8, Epoch 34/160 (LR 0.08927) => LSC_loss 0.37, Spatial_loss 2.82, Flat_loss 0.30, Train_acc 91.50, Test_acc 43.30
2023-10-17 20:24:08,972 [podnet.py] => Task 8, Epoch 35/160 (LR 0.08865) => LSC_loss 0.38, Spatial_loss 2.83, Flat_loss 0.31, Train_acc 90.98, Test_acc 45.80
2023-10-17 20:24:11,270 [podnet.py] => Task 8, Epoch 36/160 (LR 0.08802) => LSC_loss 0.36, Spatial_loss 2.84, Flat_loss 0.31, Train_acc 91.76, Test_acc 45.57
2023-10-17 20:24:13,595 [podnet.py] => Task 8, Epoch 37/160 (LR 0.08738) => LSC_loss 0.37, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 91.40, Test_acc 48.17
2023-10-17 20:24:15,883 [podnet.py] => Task 8, Epoch 38/160 (LR 0.08672) => LSC_loss 0.37, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 91.05, Test_acc 49.16
2023-10-17 20:24:18,154 [podnet.py] => Task 8, Epoch 39/160 (LR 0.08604) => LSC_loss 0.35, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 91.74, Test_acc 46.08
2023-10-17 20:24:20,382 [podnet.py] => Task 8, Epoch 40/160 (LR 0.08536) => LSC_loss 0.37, Spatial_loss 2.80, Flat_loss 0.30, Train_acc 91.21, Test_acc 50.04
2023-10-17 20:24:22,660 [podnet.py] => Task 8, Epoch 41/160 (LR 0.08465) => LSC_loss 0.36, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 91.05, Test_acc 47.06
2023-10-17 20:24:25,008 [podnet.py] => Task 8, Epoch 42/160 (LR 0.08394) => LSC_loss 0.36, Spatial_loss 2.78, Flat_loss 0.30, Train_acc 91.62, Test_acc 49.96
2023-10-17 20:24:27,312 [podnet.py] => Task 8, Epoch 43/160 (LR 0.08321) => LSC_loss 0.35, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 91.88, Test_acc 45.87
2023-10-17 20:24:29,596 [podnet.py] => Task 8, Epoch 44/160 (LR 0.08247) => LSC_loss 0.34, Spatial_loss 2.81, Flat_loss 0.30, Train_acc 92.55, Test_acc 50.60
2023-10-17 20:24:31,919 [podnet.py] => Task 8, Epoch 45/160 (LR 0.08172) => LSC_loss 0.34, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 92.24, Test_acc 49.17
2023-10-17 20:24:34,179 [podnet.py] => Task 8, Epoch 46/160 (LR 0.08095) => LSC_loss 0.36, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 91.60, Test_acc 47.06
2023-10-17 20:24:36,471 [podnet.py] => Task 8, Epoch 47/160 (LR 0.08018) => LSC_loss 0.36, Spatial_loss 2.88, Flat_loss 0.32, Train_acc 90.83, Test_acc 49.03
2023-10-17 20:24:38,739 [podnet.py] => Task 8, Epoch 48/160 (LR 0.07939) => LSC_loss 0.35, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 91.76, Test_acc 46.63
2023-10-17 20:24:40,993 [podnet.py] => Task 8, Epoch 49/160 (LR 0.07859) => LSC_loss 0.34, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 91.90, Test_acc 48.92
2023-10-17 20:24:43,294 [podnet.py] => Task 8, Epoch 50/160 (LR 0.07778) => LSC_loss 0.33, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 92.60, Test_acc 48.77
2023-10-17 20:24:45,643 [podnet.py] => Task 8, Epoch 51/160 (LR 0.07696) => LSC_loss 0.34, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 92.19, Test_acc 48.19
2023-10-17 20:24:47,957 [podnet.py] => Task 8, Epoch 52/160 (LR 0.07612) => LSC_loss 0.35, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 92.17, Test_acc 49.22
2023-10-17 20:24:50,251 [podnet.py] => Task 8, Epoch 53/160 (LR 0.07528) => LSC_loss 0.35, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 91.69, Test_acc 48.12
2023-10-17 20:24:52,512 [podnet.py] => Task 8, Epoch 54/160 (LR 0.07443) => LSC_loss 0.33, Spatial_loss 2.65, Flat_loss 0.29, Train_acc 92.21, Test_acc 48.44
2023-10-17 20:24:54,828 [podnet.py] => Task 8, Epoch 55/160 (LR 0.07357) => LSC_loss 0.34, Spatial_loss 2.70, Flat_loss 0.29, Train_acc 91.64, Test_acc 47.43
2023-10-17 20:24:57,133 [podnet.py] => Task 8, Epoch 56/160 (LR 0.07270) => LSC_loss 0.33, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 92.29, Test_acc 48.10
2023-10-17 20:24:59,459 [podnet.py] => Task 8, Epoch 57/160 (LR 0.07182) => LSC_loss 0.33, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 92.48, Test_acc 49.66
2023-10-17 20:25:01,730 [podnet.py] => Task 8, Epoch 58/160 (LR 0.07093) => LSC_loss 0.34, Spatial_loss 2.67, Flat_loss 0.29, Train_acc 92.17, Test_acc 48.26
2023-10-17 20:25:03,992 [podnet.py] => Task 8, Epoch 59/160 (LR 0.07004) => LSC_loss 0.36, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 91.64, Test_acc 48.12
2023-10-17 20:25:06,264 [podnet.py] => Task 8, Epoch 60/160 (LR 0.06913) => LSC_loss 0.35, Spatial_loss 2.64, Flat_loss 0.29, Train_acc 91.83, Test_acc 46.61
2023-10-17 20:25:08,596 [podnet.py] => Task 8, Epoch 61/160 (LR 0.06822) => LSC_loss 0.33, Spatial_loss 2.55, Flat_loss 0.27, Train_acc 92.40, Test_acc 48.58
2023-10-17 20:25:10,929 [podnet.py] => Task 8, Epoch 62/160 (LR 0.06731) => LSC_loss 0.33, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 92.48, Test_acc 48.37
2023-10-17 20:25:13,187 [podnet.py] => Task 8, Epoch 63/160 (LR 0.06638) => LSC_loss 0.32, Spatial_loss 2.51, Flat_loss 0.27, Train_acc 92.83, Test_acc 47.61
2023-10-17 20:25:15,464 [podnet.py] => Task 8, Epoch 64/160 (LR 0.06545) => LSC_loss 0.32, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 92.74, Test_acc 51.29
2023-10-17 20:25:17,707 [podnet.py] => Task 8, Epoch 65/160 (LR 0.06451) => LSC_loss 0.32, Spatial_loss 2.52, Flat_loss 0.26, Train_acc 92.74, Test_acc 48.62
2023-10-17 20:25:20,015 [podnet.py] => Task 8, Epoch 66/160 (LR 0.06357) => LSC_loss 0.33, Spatial_loss 2.57, Flat_loss 0.27, Train_acc 92.21, Test_acc 46.66
2023-10-17 20:25:22,279 [podnet.py] => Task 8, Epoch 67/160 (LR 0.06262) => LSC_loss 0.32, Spatial_loss 2.50, Flat_loss 0.26, Train_acc 92.48, Test_acc 48.14
2023-10-17 20:25:24,607 [podnet.py] => Task 8, Epoch 68/160 (LR 0.06167) => LSC_loss 0.31, Spatial_loss 2.47, Flat_loss 0.26, Train_acc 93.21, Test_acc 48.03
2023-10-17 20:25:26,938 [podnet.py] => Task 8, Epoch 69/160 (LR 0.06072) => LSC_loss 0.31, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 93.21, Test_acc 48.61
2023-10-17 20:25:29,211 [podnet.py] => Task 8, Epoch 70/160 (LR 0.05975) => LSC_loss 0.33, Spatial_loss 2.50, Flat_loss 0.27, Train_acc 92.52, Test_acc 50.64
2023-10-17 20:25:31,520 [podnet.py] => Task 8, Epoch 71/160 (LR 0.05879) => LSC_loss 0.32, Spatial_loss 2.49, Flat_loss 0.26, Train_acc 93.02, Test_acc 47.59
2023-10-17 20:25:33,808 [podnet.py] => Task 8, Epoch 72/160 (LR 0.05782) => LSC_loss 0.32, Spatial_loss 2.40, Flat_loss 0.26, Train_acc 92.43, Test_acc 48.72
2023-10-17 20:25:36,092 [podnet.py] => Task 8, Epoch 73/160 (LR 0.05685) => LSC_loss 0.33, Spatial_loss 2.46, Flat_loss 0.26, Train_acc 91.93, Test_acc 50.58
2023-10-17 20:25:38,374 [podnet.py] => Task 8, Epoch 74/160 (LR 0.05588) => LSC_loss 0.32, Spatial_loss 2.48, Flat_loss 0.26, Train_acc 92.36, Test_acc 48.72
2023-10-17 20:25:40,715 [podnet.py] => Task 8, Epoch 75/160 (LR 0.05490) => LSC_loss 0.32, Spatial_loss 2.45, Flat_loss 0.26, Train_acc 93.40, Test_acc 48.51
2023-10-17 20:25:43,014 [podnet.py] => Task 8, Epoch 76/160 (LR 0.05392) => LSC_loss 0.31, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 93.38, Test_acc 50.18
2023-10-17 20:25:45,349 [podnet.py] => Task 8, Epoch 77/160 (LR 0.05294) => LSC_loss 0.30, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 93.00, Test_acc 48.88
2023-10-17 20:25:47,587 [podnet.py] => Task 8, Epoch 78/160 (LR 0.05196) => LSC_loss 0.32, Spatial_loss 2.43, Flat_loss 0.26, Train_acc 93.10, Test_acc 44.89
2023-10-17 20:25:49,872 [podnet.py] => Task 8, Epoch 79/160 (LR 0.05098) => LSC_loss 0.32, Spatial_loss 2.43, Flat_loss 0.25, Train_acc 92.88, Test_acc 49.38
2023-10-17 20:25:52,215 [podnet.py] => Task 8, Epoch 80/160 (LR 0.05000) => LSC_loss 0.31, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 93.14, Test_acc 48.68
2023-10-17 20:25:54,529 [podnet.py] => Task 8, Epoch 81/160 (LR 0.04902) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 93.76, Test_acc 49.51
2023-10-17 20:25:56,876 [podnet.py] => Task 8, Epoch 82/160 (LR 0.04804) => LSC_loss 0.30, Spatial_loss 2.29, Flat_loss 0.24, Train_acc 94.05, Test_acc 49.77
2023-10-17 20:25:59,191 [podnet.py] => Task 8, Epoch 83/160 (LR 0.04706) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 93.62, Test_acc 51.00
2023-10-17 20:26:01,446 [podnet.py] => Task 8, Epoch 84/160 (LR 0.04608) => LSC_loss 0.31, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 93.43, Test_acc 50.08
2023-10-17 20:26:03,774 [podnet.py] => Task 8, Epoch 85/160 (LR 0.04510) => LSC_loss 0.31, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 93.57, Test_acc 50.27
2023-10-17 20:26:06,099 [podnet.py] => Task 8, Epoch 86/160 (LR 0.04412) => LSC_loss 0.31, Spatial_loss 2.24, Flat_loss 0.24, Train_acc 93.00, Test_acc 49.49
2023-10-17 20:26:08,371 [podnet.py] => Task 8, Epoch 87/160 (LR 0.04315) => LSC_loss 0.31, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 93.17, Test_acc 48.32
2023-10-17 20:26:10,672 [podnet.py] => Task 8, Epoch 88/160 (LR 0.04218) => LSC_loss 0.30, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 93.86, Test_acc 51.67
2023-10-17 20:26:12,898 [podnet.py] => Task 8, Epoch 89/160 (LR 0.04121) => LSC_loss 0.29, Spatial_loss 2.16, Flat_loss 0.23, Train_acc 93.52, Test_acc 51.44
2023-10-17 20:26:15,189 [podnet.py] => Task 8, Epoch 90/160 (LR 0.04025) => LSC_loss 0.30, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 93.48, Test_acc 51.60
2023-10-17 20:26:17,481 [podnet.py] => Task 8, Epoch 91/160 (LR 0.03928) => LSC_loss 0.30, Spatial_loss 2.21, Flat_loss 0.22, Train_acc 93.98, Test_acc 47.09
2023-10-17 20:26:19,813 [podnet.py] => Task 8, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.19, Flat_loss 0.22, Train_acc 93.81, Test_acc 50.74
2023-10-17 20:26:22,039 [podnet.py] => Task 8, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.12, Flat_loss 0.22, Train_acc 93.69, Test_acc 48.10
2023-10-17 20:26:24,340 [podnet.py] => Task 8, Epoch 94/160 (LR 0.03643) => LSC_loss 0.29, Spatial_loss 2.19, Flat_loss 0.22, Train_acc 94.00, Test_acc 49.51
2023-10-17 20:26:26,640 [podnet.py] => Task 8, Epoch 95/160 (LR 0.03549) => LSC_loss 0.30, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 93.57, Test_acc 50.60
2023-10-17 20:26:28,948 [podnet.py] => Task 8, Epoch 96/160 (LR 0.03455) => LSC_loss 0.29, Spatial_loss 2.10, Flat_loss 0.21, Train_acc 93.74, Test_acc 50.83
2023-10-17 20:26:31,274 [podnet.py] => Task 8, Epoch 97/160 (LR 0.03362) => LSC_loss 0.29, Spatial_loss 2.08, Flat_loss 0.21, Train_acc 93.95, Test_acc 49.63
2023-10-17 20:26:33,532 [podnet.py] => Task 8, Epoch 98/160 (LR 0.03269) => LSC_loss 0.29, Spatial_loss 2.09, Flat_loss 0.22, Train_acc 93.24, Test_acc 50.10
2023-10-17 20:26:35,881 [podnet.py] => Task 8, Epoch 99/160 (LR 0.03178) => LSC_loss 0.29, Spatial_loss 2.08, Flat_loss 0.21, Train_acc 94.48, Test_acc 50.67
2023-10-17 20:26:38,156 [podnet.py] => Task 8, Epoch 100/160 (LR 0.03087) => LSC_loss 0.30, Spatial_loss 2.05, Flat_loss 0.21, Train_acc 93.83, Test_acc 51.98
2023-10-17 20:26:40,397 [podnet.py] => Task 8, Epoch 101/160 (LR 0.02996) => LSC_loss 0.29, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 93.98, Test_acc 51.01
2023-10-17 20:26:42,687 [podnet.py] => Task 8, Epoch 102/160 (LR 0.02907) => LSC_loss 0.30, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 93.40, Test_acc 50.08
2023-10-17 20:26:44,940 [podnet.py] => Task 8, Epoch 103/160 (LR 0.02818) => LSC_loss 0.28, Spatial_loss 1.96, Flat_loss 0.20, Train_acc 94.33, Test_acc 51.22
2023-10-17 20:26:47,247 [podnet.py] => Task 8, Epoch 104/160 (LR 0.02730) => LSC_loss 0.29, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 93.98, Test_acc 49.80
2023-10-17 20:26:49,493 [podnet.py] => Task 8, Epoch 105/160 (LR 0.02643) => LSC_loss 0.28, Spatial_loss 1.99, Flat_loss 0.20, Train_acc 94.55, Test_acc 50.86
2023-10-17 20:26:51,788 [podnet.py] => Task 8, Epoch 106/160 (LR 0.02557) => LSC_loss 0.28, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 94.48, Test_acc 48.04
2023-10-17 20:26:54,073 [podnet.py] => Task 8, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 1.95, Flat_loss 0.20, Train_acc 94.71, Test_acc 49.62
2023-10-17 20:26:56,392 [podnet.py] => Task 8, Epoch 108/160 (LR 0.02388) => LSC_loss 0.30, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 93.62, Test_acc 52.53
2023-10-17 20:26:58,728 [podnet.py] => Task 8, Epoch 109/160 (LR 0.02304) => LSC_loss 0.29, Spatial_loss 1.86, Flat_loss 0.19, Train_acc 94.29, Test_acc 51.02
2023-10-17 20:27:00,991 [podnet.py] => Task 8, Epoch 110/160 (LR 0.02222) => LSC_loss 0.28, Spatial_loss 1.86, Flat_loss 0.19, Train_acc 94.48, Test_acc 52.48
2023-10-17 20:27:03,286 [podnet.py] => Task 8, Epoch 111/160 (LR 0.02141) => LSC_loss 0.28, Spatial_loss 1.87, Flat_loss 0.19, Train_acc 94.14, Test_acc 52.02
2023-10-17 20:27:05,645 [podnet.py] => Task 8, Epoch 112/160 (LR 0.02061) => LSC_loss 0.29, Spatial_loss 1.85, Flat_loss 0.19, Train_acc 93.98, Test_acc 51.96
2023-10-17 20:27:07,921 [podnet.py] => Task 8, Epoch 113/160 (LR 0.01982) => LSC_loss 0.29, Spatial_loss 1.89, Flat_loss 0.20, Train_acc 94.10, Test_acc 51.64
2023-10-17 20:27:10,243 [podnet.py] => Task 8, Epoch 114/160 (LR 0.01905) => LSC_loss 0.28, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 94.76, Test_acc 52.94
2023-10-17 20:27:12,553 [podnet.py] => Task 8, Epoch 115/160 (LR 0.01828) => LSC_loss 0.28, Spatial_loss 1.82, Flat_loss 0.18, Train_acc 94.45, Test_acc 51.96
2023-10-17 20:27:14,864 [podnet.py] => Task 8, Epoch 116/160 (LR 0.01753) => LSC_loss 0.28, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 94.50, Test_acc 52.09
2023-10-17 20:27:17,166 [podnet.py] => Task 8, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 94.64, Test_acc 50.70
2023-10-17 20:27:19,445 [podnet.py] => Task 8, Epoch 118/160 (LR 0.01606) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 94.24, Test_acc 50.90
2023-10-17 20:27:21,747 [podnet.py] => Task 8, Epoch 119/160 (LR 0.01535) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 94.50, Test_acc 52.28
2023-10-17 20:27:24,002 [podnet.py] => Task 8, Epoch 120/160 (LR 0.01464) => LSC_loss 0.28, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 94.43, Test_acc 52.29
2023-10-17 20:27:26,256 [podnet.py] => Task 8, Epoch 121/160 (LR 0.01396) => LSC_loss 0.28, Spatial_loss 1.74, Flat_loss 0.18, Train_acc 93.88, Test_acc 51.54
2023-10-17 20:27:28,541 [podnet.py] => Task 8, Epoch 122/160 (LR 0.01328) => LSC_loss 0.27, Spatial_loss 1.65, Flat_loss 0.17, Train_acc 94.88, Test_acc 51.88
2023-10-17 20:27:30,825 [podnet.py] => Task 8, Epoch 123/160 (LR 0.01262) => LSC_loss 0.28, Spatial_loss 1.68, Flat_loss 0.17, Train_acc 94.79, Test_acc 52.22
2023-10-17 20:27:33,123 [podnet.py] => Task 8, Epoch 124/160 (LR 0.01198) => LSC_loss 0.27, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 94.45, Test_acc 51.09
2023-10-17 20:27:35,382 [podnet.py] => Task 8, Epoch 125/160 (LR 0.01135) => LSC_loss 0.29, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 94.24, Test_acc 50.83
2023-10-17 20:27:37,699 [podnet.py] => Task 8, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.70, Flat_loss 0.17, Train_acc 95.00, Test_acc 50.50
2023-10-17 20:27:39,999 [podnet.py] => Task 8, Epoch 127/160 (LR 0.01013) => LSC_loss 0.29, Spatial_loss 1.62, Flat_loss 0.17, Train_acc 94.05, Test_acc 50.77
2023-10-17 20:27:42,292 [podnet.py] => Task 8, Epoch 128/160 (LR 0.00955) => LSC_loss 0.27, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 94.76, Test_acc 51.28
2023-10-17 20:27:44,615 [podnet.py] => Task 8, Epoch 129/160 (LR 0.00898) => LSC_loss 0.28, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 94.83, Test_acc 51.83
2023-10-17 20:27:46,902 [podnet.py] => Task 8, Epoch 130/160 (LR 0.00843) => LSC_loss 0.28, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 94.19, Test_acc 52.67
2023-10-17 20:27:49,193 [podnet.py] => Task 8, Epoch 131/160 (LR 0.00789) => LSC_loss 0.28, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 94.19, Test_acc 52.22
2023-10-17 20:27:51,459 [podnet.py] => Task 8, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.57, Flat_loss 0.17, Train_acc 94.98, Test_acc 52.48
2023-10-17 20:27:53,811 [podnet.py] => Task 8, Epoch 133/160 (LR 0.00686) => LSC_loss 0.29, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 93.64, Test_acc 51.79
2023-10-17 20:27:56,098 [podnet.py] => Task 8, Epoch 134/160 (LR 0.00638) => LSC_loss 0.27, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 95.10, Test_acc 52.19
2023-10-17 20:27:58,427 [podnet.py] => Task 8, Epoch 135/160 (LR 0.00590) => LSC_loss 0.28, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 94.67, Test_acc 51.07
2023-10-17 20:28:00,662 [podnet.py] => Task 8, Epoch 136/160 (LR 0.00545) => LSC_loss 0.28, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 94.62, Test_acc 52.61
2023-10-17 20:28:02,933 [podnet.py] => Task 8, Epoch 137/160 (LR 0.00501) => LSC_loss 0.28, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 94.48, Test_acc 52.73
2023-10-17 20:28:05,167 [podnet.py] => Task 8, Epoch 138/160 (LR 0.00459) => LSC_loss 0.28, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 94.14, Test_acc 52.30
2023-10-17 20:28:07,400 [podnet.py] => Task 8, Epoch 139/160 (LR 0.00419) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 94.83, Test_acc 52.57
2023-10-17 20:28:09,621 [podnet.py] => Task 8, Epoch 140/160 (LR 0.00381) => LSC_loss 0.28, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 94.67, Test_acc 51.70
2023-10-17 20:28:11,862 [podnet.py] => Task 8, Epoch 141/160 (LR 0.00344) => LSC_loss 0.28, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 94.50, Test_acc 52.09
2023-10-17 20:28:14,049 [podnet.py] => Task 8, Epoch 142/160 (LR 0.00309) => LSC_loss 0.28, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 94.55, Test_acc 51.90
2023-10-17 20:28:16,318 [podnet.py] => Task 8, Epoch 143/160 (LR 0.00276) => LSC_loss 0.27, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 95.17, Test_acc 53.01
2023-10-17 20:28:18,635 [podnet.py] => Task 8, Epoch 144/160 (LR 0.00245) => LSC_loss 0.28, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 94.64, Test_acc 52.67
2023-10-17 20:28:20,991 [podnet.py] => Task 8, Epoch 145/160 (LR 0.00215) => LSC_loss 0.27, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 95.40, Test_acc 52.09
2023-10-17 20:28:23,390 [podnet.py] => Task 8, Epoch 146/160 (LR 0.00188) => LSC_loss 0.28, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 94.74, Test_acc 52.56
2023-10-17 20:28:25,718 [podnet.py] => Task 8, Epoch 147/160 (LR 0.00162) => LSC_loss 0.28, Spatial_loss 1.39, Flat_loss 0.16, Train_acc 94.64, Test_acc 52.16
2023-10-17 20:28:28,044 [podnet.py] => Task 8, Epoch 148/160 (LR 0.00138) => LSC_loss 0.27, Spatial_loss 1.41, Flat_loss 0.15, Train_acc 95.05, Test_acc 52.51
2023-10-17 20:28:30,380 [podnet.py] => Task 8, Epoch 149/160 (LR 0.00116) => LSC_loss 0.27, Spatial_loss 1.39, Flat_loss 0.16, Train_acc 94.95, Test_acc 52.69
2023-10-17 20:28:32,752 [podnet.py] => Task 8, Epoch 150/160 (LR 0.00096) => LSC_loss 0.27, Spatial_loss 1.38, Flat_loss 0.15, Train_acc 94.52, Test_acc 52.22
2023-10-17 20:28:35,121 [podnet.py] => Task 8, Epoch 151/160 (LR 0.00078) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.62, Test_acc 52.80
2023-10-17 20:28:37,488 [podnet.py] => Task 8, Epoch 152/160 (LR 0.00062) => LSC_loss 0.27, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.88, Test_acc 52.89
2023-10-17 20:28:39,805 [podnet.py] => Task 8, Epoch 153/160 (LR 0.00047) => LSC_loss 0.28, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 94.31, Test_acc 52.76
2023-10-17 20:28:42,189 [podnet.py] => Task 8, Epoch 154/160 (LR 0.00035) => LSC_loss 0.27, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 94.98, Test_acc 52.32
2023-10-17 20:28:44,510 [podnet.py] => Task 8, Epoch 155/160 (LR 0.00024) => LSC_loss 0.27, Spatial_loss 1.34, Flat_loss 0.15, Train_acc 95.12, Test_acc 52.87
2023-10-17 20:28:46,827 [podnet.py] => Task 8, Epoch 156/160 (LR 0.00015) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.24, Test_acc 52.69
2023-10-17 20:28:49,163 [podnet.py] => Task 8, Epoch 157/160 (LR 0.00009) => LSC_loss 0.27, Spatial_loss 1.33, Flat_loss 0.15, Train_acc 94.79, Test_acc 52.47
2023-10-17 20:28:51,423 [podnet.py] => Task 8, Epoch 158/160 (LR 0.00004) => LSC_loss 0.28, Spatial_loss 1.33, Flat_loss 0.15, Train_acc 94.76, Test_acc 52.76
2023-10-17 20:28:53,780 [podnet.py] => Task 8, Epoch 159/160 (LR 0.00001) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.90, Test_acc 52.56
2023-10-17 20:28:56,116 [podnet.py] => Task 8, Epoch 160/160 (LR 0.00000) => LSC_loss 0.27, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 94.69, Test_acc 52.68
2023-10-17 20:28:56,117 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:28:56,117 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:29:32,071 [podnet.py] => The size of finetune dataset: 1800
2023-10-17 20:29:33,827 [podnet.py] => Task 8, Epoch 1/20 (LR 0.00497) => LSC_loss 0.23, Spatial_loss 1.93, Flat_loss 0.19, Train_acc 95.28, Test_acc 52.59
2023-10-17 20:29:35,566 [podnet.py] => Task 8, Epoch 2/20 (LR 0.00488) => LSC_loss 0.14, Spatial_loss 1.74, Flat_loss 0.11, Train_acc 98.22, Test_acc 53.37
2023-10-17 20:29:37,291 [podnet.py] => Task 8, Epoch 3/20 (LR 0.00473) => LSC_loss 0.12, Spatial_loss 1.62, Flat_loss 0.09, Train_acc 98.39, Test_acc 53.96
2023-10-17 20:29:39,019 [podnet.py] => Task 8, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.70, Flat_loss 0.10, Train_acc 98.33, Test_acc 53.49
2023-10-17 20:29:40,737 [podnet.py] => Task 8, Epoch 5/20 (LR 0.00427) => LSC_loss 0.12, Spatial_loss 1.65, Flat_loss 0.09, Train_acc 98.50, Test_acc 54.52
2023-10-17 20:29:42,453 [podnet.py] => Task 8, Epoch 6/20 (LR 0.00397) => LSC_loss 0.16, Spatial_loss 1.56, Flat_loss 0.08, Train_acc 98.33, Test_acc 53.92
2023-10-17 20:29:44,191 [podnet.py] => Task 8, Epoch 7/20 (LR 0.00363) => LSC_loss 0.13, Spatial_loss 1.63, Flat_loss 0.09, Train_acc 98.83, Test_acc 54.50
2023-10-17 20:29:45,887 [podnet.py] => Task 8, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.55, Flat_loss 0.09, Train_acc 98.67, Test_acc 54.11
2023-10-17 20:29:47,657 [podnet.py] => Task 8, Epoch 9/20 (LR 0.00289) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.09, Train_acc 98.67, Test_acc 54.29
2023-10-17 20:29:49,385 [podnet.py] => Task 8, Epoch 10/20 (LR 0.00250) => LSC_loss 0.12, Spatial_loss 1.66, Flat_loss 0.09, Train_acc 98.44, Test_acc 54.61
2023-10-17 20:29:51,067 [podnet.py] => Task 8, Epoch 11/20 (LR 0.00211) => LSC_loss 0.16, Spatial_loss 1.68, Flat_loss 0.11, Train_acc 98.94, Test_acc 54.43
2023-10-17 20:29:52,800 [podnet.py] => Task 8, Epoch 12/20 (LR 0.00173) => LSC_loss 0.16, Spatial_loss 1.61, Flat_loss 0.09, Train_acc 98.17, Test_acc 54.24
2023-10-17 20:29:54,560 [podnet.py] => Task 8, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.65, Flat_loss 0.09, Train_acc 98.39, Test_acc 54.16
2023-10-17 20:29:56,355 [podnet.py] => Task 8, Epoch 14/20 (LR 0.00103) => LSC_loss 0.21, Spatial_loss 1.62, Flat_loss 0.10, Train_acc 98.28, Test_acc 54.54
2023-10-17 20:29:58,031 [podnet.py] => Task 8, Epoch 15/20 (LR 0.00073) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.10, Train_acc 98.67, Test_acc 54.70
2023-10-17 20:29:59,715 [podnet.py] => Task 8, Epoch 16/20 (LR 0.00048) => LSC_loss 0.17, Spatial_loss 1.62, Flat_loss 0.10, Train_acc 98.06, Test_acc 54.76
2023-10-17 20:30:01,438 [podnet.py] => Task 8, Epoch 17/20 (LR 0.00027) => LSC_loss 0.14, Spatial_loss 1.56, Flat_loss 0.09, Train_acc 98.94, Test_acc 55.16
2023-10-17 20:30:03,135 [podnet.py] => Task 8, Epoch 18/20 (LR 0.00012) => LSC_loss 0.17, Spatial_loss 1.49, Flat_loss 0.08, Train_acc 98.44, Test_acc 54.54
2023-10-17 20:30:04,877 [podnet.py] => Task 8, Epoch 19/20 (LR 0.00003) => LSC_loss 0.11, Spatial_loss 1.53, Flat_loss 0.09, Train_acc 99.00, Test_acc 54.56
2023-10-17 20:30:06,550 [podnet.py] => Task 8, Epoch 20/20 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.55, Flat_loss 0.09, Train_acc 98.11, Test_acc 54.70
2023-10-17 20:30:06,553 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:30:44,661 [podnet.py] => Exemplar size: 1800
2023-10-17 20:30:44,661 [trainer.py] => CNN: {'total': 54.7, '00-09': 63.4, '10-19': 46.3, '20-29': 63.7, '30-39': 52.7, '40-49': 63.0, '50-59': 41.5, '60-69': 49.4, '70-79': 50.0, '80-89': 62.3, 'old': 54.74, 'new': 54.0}
2023-10-17 20:30:44,662 [trainer.py] => NME: {'total': 55.1, '00-09': 67.3, '10-19': 50.8, '20-29': 66.6, '30-39': 57.0, '40-49': 63.7, '50-59': 37.4, '60-69': 47.3, '70-79': 48.2, '80-89': 57.6, 'old': 55.38, 'new': 50.4}
2023-10-17 20:30:44,662 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7]
2023-10-17 20:30:44,662 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5]
2023-10-17 20:30:44,663 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1]
2023-10-17 20:30:44,663 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73]

2023-10-17 20:30:44,663 [trainer.py] => Average Accuracy (CNN): 64.48555555555555
2023-10-17 20:30:44,663 [trainer.py] => Average Accuracy (NME): 64.78555555555556
2023-10-17 20:30:44,664 [trainer.py] => All params: 523857
2023-10-17 20:30:44,664 [trainer.py] => Trainable params: 523857
2023-10-17 20:30:44,665 [podnet.py] => Learning on 90-95
2023-10-17 20:30:44,700 [podnet.py] => Adaptive factor: 4.358898943540674
2023-10-17 20:30:47,052 [podnet.py] => Task 9, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 4.52, Flat_loss 0.94, Train_acc 64.49, Test_acc 32.48
2023-10-17 20:30:49,403 [podnet.py] => Task 9, Epoch 2/160 (LR 0.09996) => LSC_loss 0.84, Spatial_loss 4.69, Flat_loss 0.77, Train_acc 76.51, Test_acc 39.77
2023-10-17 20:30:51,769 [podnet.py] => Task 9, Epoch 3/160 (LR 0.09991) => LSC_loss 0.68, Spatial_loss 4.22, Flat_loss 0.63, Train_acc 81.37, Test_acc 41.41
2023-10-17 20:30:54,114 [podnet.py] => Task 9, Epoch 4/160 (LR 0.09985) => LSC_loss 0.64, Spatial_loss 4.08, Flat_loss 0.57, Train_acc 82.63, Test_acc 47.07
2023-10-17 20:30:56,459 [podnet.py] => Task 9, Epoch 5/160 (LR 0.09976) => LSC_loss 0.55, Spatial_loss 3.79, Flat_loss 0.49, Train_acc 85.70, Test_acc 42.02
2023-10-17 20:30:58,786 [podnet.py] => Task 9, Epoch 6/160 (LR 0.09965) => LSC_loss 0.52, Spatial_loss 3.66, Flat_loss 0.46, Train_acc 86.63, Test_acc 41.42
2023-10-17 20:31:01,122 [podnet.py] => Task 9, Epoch 7/160 (LR 0.09953) => LSC_loss 0.51, Spatial_loss 3.72, Flat_loss 0.46, Train_acc 85.84, Test_acc 37.24
2023-10-17 20:31:03,438 [podnet.py] => Task 9, Epoch 8/160 (LR 0.09938) => LSC_loss 0.50, Spatial_loss 3.73, Flat_loss 0.46, Train_acc 86.88, Test_acc 43.61
2023-10-17 20:31:05,790 [podnet.py] => Task 9, Epoch 9/160 (LR 0.09922) => LSC_loss 0.48, Spatial_loss 3.59, Flat_loss 0.44, Train_acc 87.70, Test_acc 41.72
2023-10-17 20:31:08,067 [podnet.py] => Task 9, Epoch 10/160 (LR 0.09904) => LSC_loss 0.49, Spatial_loss 3.54, Flat_loss 0.43, Train_acc 87.70, Test_acc 38.02
2023-10-17 20:31:10,403 [podnet.py] => Task 9, Epoch 11/160 (LR 0.09884) => LSC_loss 0.44, Spatial_loss 3.51, Flat_loss 0.42, Train_acc 88.70, Test_acc 43.38
2023-10-17 20:31:12,749 [podnet.py] => Task 9, Epoch 12/160 (LR 0.09862) => LSC_loss 0.43, Spatial_loss 3.42, Flat_loss 0.39, Train_acc 89.53, Test_acc 38.67
2023-10-17 20:31:15,105 [podnet.py] => Task 9, Epoch 13/160 (LR 0.09838) => LSC_loss 0.41, Spatial_loss 3.38, Flat_loss 0.39, Train_acc 90.23, Test_acc 44.20
2023-10-17 20:31:17,397 [podnet.py] => Task 9, Epoch 14/160 (LR 0.09812) => LSC_loss 0.41, Spatial_loss 3.41, Flat_loss 0.38, Train_acc 89.74, Test_acc 45.32
2023-10-17 20:31:19,748 [podnet.py] => Task 9, Epoch 15/160 (LR 0.09785) => LSC_loss 0.44, Spatial_loss 3.38, Flat_loss 0.40, Train_acc 89.21, Test_acc 46.37
2023-10-17 20:31:22,041 [podnet.py] => Task 9, Epoch 16/160 (LR 0.09755) => LSC_loss 0.40, Spatial_loss 3.32, Flat_loss 0.39, Train_acc 90.09, Test_acc 46.40
2023-10-17 20:31:24,365 [podnet.py] => Task 9, Epoch 17/160 (LR 0.09724) => LSC_loss 0.42, Spatial_loss 3.34, Flat_loss 0.38, Train_acc 89.05, Test_acc 46.04
2023-10-17 20:31:26,729 [podnet.py] => Task 9, Epoch 18/160 (LR 0.09691) => LSC_loss 0.40, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.51, Test_acc 41.19
2023-10-17 20:31:29,068 [podnet.py] => Task 9, Epoch 19/160 (LR 0.09656) => LSC_loss 0.38, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.07, Test_acc 42.76
2023-10-17 20:31:31,386 [podnet.py] => Task 9, Epoch 20/160 (LR 0.09619) => LSC_loss 0.37, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 91.33, Test_acc 44.77
2023-10-17 20:31:33,764 [podnet.py] => Task 9, Epoch 21/160 (LR 0.09581) => LSC_loss 0.38, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.81, Test_acc 41.54
2023-10-17 20:31:36,099 [podnet.py] => Task 9, Epoch 22/160 (LR 0.09541) => LSC_loss 0.37, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 91.35, Test_acc 48.21
2023-10-17 20:31:38,476 [podnet.py] => Task 9, Epoch 23/160 (LR 0.09499) => LSC_loss 0.39, Spatial_loss 3.18, Flat_loss 0.37, Train_acc 90.60, Test_acc 42.69
2023-10-17 20:31:40,837 [podnet.py] => Task 9, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.38, Train_acc 90.47, Test_acc 42.45
2023-10-17 20:31:43,186 [podnet.py] => Task 9, Epoch 25/160 (LR 0.09410) => LSC_loss 0.38, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.26, Test_acc 43.03
2023-10-17 20:31:45,508 [podnet.py] => Task 9, Epoch 26/160 (LR 0.09362) => LSC_loss 0.37, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.47, Test_acc 42.88
2023-10-17 20:31:47,817 [podnet.py] => Task 9, Epoch 27/160 (LR 0.09314) => LSC_loss 0.37, Spatial_loss 3.19, Flat_loss 0.36, Train_acc 91.05, Test_acc 46.63
2023-10-17 20:31:50,076 [podnet.py] => Task 9, Epoch 28/160 (LR 0.09263) => LSC_loss 0.38, Spatial_loss 3.19, Flat_loss 0.36, Train_acc 90.77, Test_acc 47.05
2023-10-17 20:31:52,431 [podnet.py] => Task 9, Epoch 29/160 (LR 0.09211) => LSC_loss 0.38, Spatial_loss 3.19, Flat_loss 0.37, Train_acc 90.58, Test_acc 45.05
2023-10-17 20:31:54,744 [podnet.py] => Task 9, Epoch 30/160 (LR 0.09157) => LSC_loss 0.36, Spatial_loss 3.09, Flat_loss 0.35, Train_acc 91.67, Test_acc 44.35
2023-10-17 20:31:57,094 [podnet.py] => Task 9, Epoch 31/160 (LR 0.09102) => LSC_loss 0.37, Spatial_loss 3.20, Flat_loss 0.36, Train_acc 91.58, Test_acc 44.32
2023-10-17 20:31:59,411 [podnet.py] => Task 9, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 3.17, Flat_loss 0.35, Train_acc 91.16, Test_acc 44.05
2023-10-17 20:32:01,763 [podnet.py] => Task 9, Epoch 33/160 (LR 0.08987) => LSC_loss 0.36, Spatial_loss 3.12, Flat_loss 0.34, Train_acc 91.88, Test_acc 44.64
2023-10-17 20:32:04,056 [podnet.py] => Task 9, Epoch 34/160 (LR 0.08927) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.37, Train_acc 90.51, Test_acc 47.61
2023-10-17 20:32:06,362 [podnet.py] => Task 9, Epoch 35/160 (LR 0.08865) => LSC_loss 0.36, Spatial_loss 3.09, Flat_loss 0.35, Train_acc 91.40, Test_acc 46.53
2023-10-17 20:32:08,711 [podnet.py] => Task 9, Epoch 36/160 (LR 0.08802) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.77, Test_acc 41.42
2023-10-17 20:32:11,027 [podnet.py] => Task 9, Epoch 37/160 (LR 0.08738) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.07, Test_acc 45.86
2023-10-17 20:32:13,373 [podnet.py] => Task 9, Epoch 38/160 (LR 0.08672) => LSC_loss 0.35, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 91.91, Test_acc 45.98
2023-10-17 20:32:15,709 [podnet.py] => Task 9, Epoch 39/160 (LR 0.08604) => LSC_loss 0.36, Spatial_loss 3.14, Flat_loss 0.35, Train_acc 91.60, Test_acc 44.42
2023-10-17 20:32:18,067 [podnet.py] => Task 9, Epoch 40/160 (LR 0.08536) => LSC_loss 0.33, Spatial_loss 3.05, Flat_loss 0.33, Train_acc 92.70, Test_acc 44.45
2023-10-17 20:32:20,431 [podnet.py] => Task 9, Epoch 41/160 (LR 0.08465) => LSC_loss 0.34, Spatial_loss 3.10, Flat_loss 0.33, Train_acc 92.37, Test_acc 42.42
2023-10-17 20:32:22,782 [podnet.py] => Task 9, Epoch 42/160 (LR 0.08394) => LSC_loss 0.33, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 92.44, Test_acc 43.42
2023-10-17 20:32:25,129 [podnet.py] => Task 9, Epoch 43/160 (LR 0.08321) => LSC_loss 0.33, Spatial_loss 3.01, Flat_loss 0.34, Train_acc 92.09, Test_acc 44.57
2023-10-17 20:32:27,466 [podnet.py] => Task 9, Epoch 44/160 (LR 0.08247) => LSC_loss 0.36, Spatial_loss 3.04, Flat_loss 0.34, Train_acc 91.42, Test_acc 40.09
2023-10-17 20:32:29,801 [podnet.py] => Task 9, Epoch 45/160 (LR 0.08172) => LSC_loss 0.33, Spatial_loss 3.00, Flat_loss 0.33, Train_acc 92.16, Test_acc 46.06
2023-10-17 20:32:32,125 [podnet.py] => Task 9, Epoch 46/160 (LR 0.08095) => LSC_loss 0.32, Spatial_loss 2.95, Flat_loss 0.32, Train_acc 93.07, Test_acc 46.45
2023-10-17 20:32:34,470 [podnet.py] => Task 9, Epoch 47/160 (LR 0.08018) => LSC_loss 0.32, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 92.47, Test_acc 45.80
2023-10-17 20:32:36,768 [podnet.py] => Task 9, Epoch 48/160 (LR 0.07939) => LSC_loss 0.35, Spatial_loss 3.03, Flat_loss 0.34, Train_acc 91.81, Test_acc 41.78
2023-10-17 20:32:39,131 [podnet.py] => Task 9, Epoch 49/160 (LR 0.07859) => LSC_loss 0.34, Spatial_loss 3.06, Flat_loss 0.34, Train_acc 92.42, Test_acc 45.22
2023-10-17 20:32:41,506 [podnet.py] => Task 9, Epoch 50/160 (LR 0.07778) => LSC_loss 0.33, Spatial_loss 2.98, Flat_loss 0.33, Train_acc 92.56, Test_acc 47.04
2023-10-17 20:32:43,817 [podnet.py] => Task 9, Epoch 51/160 (LR 0.07696) => LSC_loss 0.33, Spatial_loss 2.95, Flat_loss 0.33, Train_acc 92.51, Test_acc 46.07
2023-10-17 20:32:46,102 [podnet.py] => Task 9, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.00, Flat_loss 0.33, Train_acc 92.16, Test_acc 47.26
2023-10-17 20:32:48,433 [podnet.py] => Task 9, Epoch 53/160 (LR 0.07528) => LSC_loss 0.31, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 92.77, Test_acc 47.01
2023-10-17 20:32:50,784 [podnet.py] => Task 9, Epoch 54/160 (LR 0.07443) => LSC_loss 0.32, Spatial_loss 2.87, Flat_loss 0.32, Train_acc 92.88, Test_acc 45.25
2023-10-17 20:32:53,097 [podnet.py] => Task 9, Epoch 55/160 (LR 0.07357) => LSC_loss 0.32, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 92.65, Test_acc 47.73
2023-10-17 20:32:55,448 [podnet.py] => Task 9, Epoch 56/160 (LR 0.07270) => LSC_loss 0.30, Spatial_loss 2.76, Flat_loss 0.30, Train_acc 93.63, Test_acc 46.46
2023-10-17 20:32:57,776 [podnet.py] => Task 9, Epoch 57/160 (LR 0.07182) => LSC_loss 0.32, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 92.35, Test_acc 49.14
2023-10-17 20:33:00,112 [podnet.py] => Task 9, Epoch 58/160 (LR 0.07093) => LSC_loss 0.33, Spatial_loss 2.81, Flat_loss 0.31, Train_acc 92.37, Test_acc 48.19
2023-10-17 20:33:02,497 [podnet.py] => Task 9, Epoch 59/160 (LR 0.07004) => LSC_loss 0.31, Spatial_loss 2.82, Flat_loss 0.30, Train_acc 93.05, Test_acc 45.09
2023-10-17 20:33:04,820 [podnet.py] => Task 9, Epoch 60/160 (LR 0.06913) => LSC_loss 0.32, Spatial_loss 2.81, Flat_loss 0.30, Train_acc 92.79, Test_acc 49.66
2023-10-17 20:33:07,110 [podnet.py] => Task 9, Epoch 61/160 (LR 0.06822) => LSC_loss 0.33, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 92.56, Test_acc 48.17
2023-10-17 20:33:09,491 [podnet.py] => Task 9, Epoch 62/160 (LR 0.06731) => LSC_loss 0.31, Spatial_loss 2.78, Flat_loss 0.30, Train_acc 93.09, Test_acc 48.58
2023-10-17 20:33:11,781 [podnet.py] => Task 9, Epoch 63/160 (LR 0.06638) => LSC_loss 0.31, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 93.65, Test_acc 46.38
2023-10-17 20:33:14,167 [podnet.py] => Task 9, Epoch 64/160 (LR 0.06545) => LSC_loss 0.32, Spatial_loss 2.84, Flat_loss 0.31, Train_acc 92.35, Test_acc 46.98
2023-10-17 20:33:16,533 [podnet.py] => Task 9, Epoch 65/160 (LR 0.06451) => LSC_loss 0.32, Spatial_loss 2.73, Flat_loss 0.29, Train_acc 93.16, Test_acc 44.45
2023-10-17 20:33:18,807 [podnet.py] => Task 9, Epoch 66/160 (LR 0.06357) => LSC_loss 0.30, Spatial_loss 2.71, Flat_loss 0.29, Train_acc 93.21, Test_acc 46.84
2023-10-17 20:33:21,187 [podnet.py] => Task 9, Epoch 67/160 (LR 0.06262) => LSC_loss 0.31, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 92.79, Test_acc 46.07
2023-10-17 20:33:23,575 [podnet.py] => Task 9, Epoch 68/160 (LR 0.06167) => LSC_loss 0.32, Spatial_loss 2.64, Flat_loss 0.28, Train_acc 92.91, Test_acc 48.80
2023-10-17 20:33:25,903 [podnet.py] => Task 9, Epoch 69/160 (LR 0.06072) => LSC_loss 0.30, Spatial_loss 2.72, Flat_loss 0.29, Train_acc 93.63, Test_acc 46.13
2023-10-17 20:33:28,173 [podnet.py] => Task 9, Epoch 70/160 (LR 0.05975) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.30, Train_acc 93.00, Test_acc 47.11
2023-10-17 20:33:30,527 [podnet.py] => Task 9, Epoch 71/160 (LR 0.05879) => LSC_loss 0.29, Spatial_loss 2.67, Flat_loss 0.28, Train_acc 93.56, Test_acc 48.26
2023-10-17 20:33:32,848 [podnet.py] => Task 9, Epoch 72/160 (LR 0.05782) => LSC_loss 0.29, Spatial_loss 2.67, Flat_loss 0.28, Train_acc 93.37, Test_acc 48.38
2023-10-17 20:33:35,187 [podnet.py] => Task 9, Epoch 73/160 (LR 0.05685) => LSC_loss 0.30, Spatial_loss 2.62, Flat_loss 0.28, Train_acc 93.70, Test_acc 50.04
2023-10-17 20:33:37,504 [podnet.py] => Task 9, Epoch 74/160 (LR 0.05588) => LSC_loss 0.29, Spatial_loss 2.59, Flat_loss 0.27, Train_acc 93.56, Test_acc 48.39
2023-10-17 20:33:39,838 [podnet.py] => Task 9, Epoch 75/160 (LR 0.05490) => LSC_loss 0.30, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 93.74, Test_acc 46.37
2023-10-17 20:33:42,113 [podnet.py] => Task 9, Epoch 76/160 (LR 0.05392) => LSC_loss 0.29, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 93.74, Test_acc 49.20
2023-10-17 20:33:44,466 [podnet.py] => Task 9, Epoch 77/160 (LR 0.05294) => LSC_loss 0.30, Spatial_loss 2.62, Flat_loss 0.27, Train_acc 93.07, Test_acc 46.84
2023-10-17 20:33:46,817 [podnet.py] => Task 9, Epoch 78/160 (LR 0.05196) => LSC_loss 0.29, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 93.74, Test_acc 47.74
2023-10-17 20:33:49,154 [podnet.py] => Task 9, Epoch 79/160 (LR 0.05098) => LSC_loss 0.30, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 93.91, Test_acc 47.74
2023-10-17 20:33:51,440 [podnet.py] => Task 9, Epoch 80/160 (LR 0.05000) => LSC_loss 0.30, Spatial_loss 2.50, Flat_loss 0.27, Train_acc 93.51, Test_acc 48.44
2023-10-17 20:33:53,705 [podnet.py] => Task 9, Epoch 81/160 (LR 0.04902) => LSC_loss 0.28, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 93.98, Test_acc 48.49
2023-10-17 20:33:56,033 [podnet.py] => Task 9, Epoch 82/160 (LR 0.04804) => LSC_loss 0.29, Spatial_loss 2.50, Flat_loss 0.26, Train_acc 93.70, Test_acc 48.35
2023-10-17 20:33:58,351 [podnet.py] => Task 9, Epoch 83/160 (LR 0.04706) => LSC_loss 0.28, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.28, Test_acc 49.09
2023-10-17 20:34:00,667 [podnet.py] => Task 9, Epoch 84/160 (LR 0.04608) => LSC_loss 0.29, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 94.02, Test_acc 47.54
2023-10-17 20:34:03,009 [podnet.py] => Task 9, Epoch 85/160 (LR 0.04510) => LSC_loss 0.29, Spatial_loss 2.44, Flat_loss 0.26, Train_acc 93.91, Test_acc 47.00
2023-10-17 20:34:05,375 [podnet.py] => Task 9, Epoch 86/160 (LR 0.04412) => LSC_loss 0.28, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 94.23, Test_acc 49.13
2023-10-17 20:34:07,689 [podnet.py] => Task 9, Epoch 87/160 (LR 0.04315) => LSC_loss 0.29, Spatial_loss 2.47, Flat_loss 0.25, Train_acc 94.28, Test_acc 50.33
2023-10-17 20:34:10,046 [podnet.py] => Task 9, Epoch 88/160 (LR 0.04218) => LSC_loss 0.29, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.51, Test_acc 44.69
2023-10-17 20:34:12,363 [podnet.py] => Task 9, Epoch 89/160 (LR 0.04121) => LSC_loss 0.29, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 94.00, Test_acc 46.02
2023-10-17 20:34:14,741 [podnet.py] => Task 9, Epoch 90/160 (LR 0.04025) => LSC_loss 0.29, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 93.63, Test_acc 47.88
2023-10-17 20:34:17,096 [podnet.py] => Task 9, Epoch 91/160 (LR 0.03928) => LSC_loss 0.27, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 94.70, Test_acc 47.86
2023-10-17 20:34:19,455 [podnet.py] => Task 9, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.25, Train_acc 93.72, Test_acc 48.15
2023-10-17 20:34:21,789 [podnet.py] => Task 9, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.37, Flat_loss 0.24, Train_acc 94.14, Test_acc 50.44
2023-10-17 20:34:24,182 [podnet.py] => Task 9, Epoch 94/160 (LR 0.03643) => LSC_loss 0.28, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 94.28, Test_acc 49.05
2023-10-17 20:34:26,575 [podnet.py] => Task 9, Epoch 95/160 (LR 0.03549) => LSC_loss 0.28, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 94.35, Test_acc 48.66
2023-10-17 20:34:28,921 [podnet.py] => Task 9, Epoch 96/160 (LR 0.03455) => LSC_loss 0.28, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 94.37, Test_acc 49.40
2023-10-17 20:34:31,296 [podnet.py] => Task 9, Epoch 97/160 (LR 0.03362) => LSC_loss 0.27, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 94.40, Test_acc 47.07
2023-10-17 20:34:33,662 [podnet.py] => Task 9, Epoch 98/160 (LR 0.03269) => LSC_loss 0.27, Spatial_loss 2.23, Flat_loss 0.22, Train_acc 94.49, Test_acc 48.38
2023-10-17 20:34:36,002 [podnet.py] => Task 9, Epoch 99/160 (LR 0.03178) => LSC_loss 0.26, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 95.00, Test_acc 49.46
2023-10-17 20:34:38,312 [podnet.py] => Task 9, Epoch 100/160 (LR 0.03087) => LSC_loss 0.27, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 95.14, Test_acc 50.07
2023-10-17 20:34:40,716 [podnet.py] => Task 9, Epoch 101/160 (LR 0.02996) => LSC_loss 0.27, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 94.74, Test_acc 48.64
2023-10-17 20:34:43,086 [podnet.py] => Task 9, Epoch 102/160 (LR 0.02907) => LSC_loss 0.27, Spatial_loss 2.26, Flat_loss 0.23, Train_acc 94.79, Test_acc 50.13
2023-10-17 20:34:45,426 [podnet.py] => Task 9, Epoch 103/160 (LR 0.02818) => LSC_loss 0.26, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 94.77, Test_acc 49.16
2023-10-17 20:34:47,810 [podnet.py] => Task 9, Epoch 104/160 (LR 0.02730) => LSC_loss 0.27, Spatial_loss 2.17, Flat_loss 0.22, Train_acc 94.56, Test_acc 48.56
2023-10-17 20:34:50,169 [podnet.py] => Task 9, Epoch 105/160 (LR 0.02643) => LSC_loss 0.26, Spatial_loss 2.12, Flat_loss 0.22, Train_acc 94.60, Test_acc 49.57
2023-10-17 20:34:52,582 [podnet.py] => Task 9, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 2.07, Flat_loss 0.21, Train_acc 94.21, Test_acc 50.65
2023-10-17 20:34:54,909 [podnet.py] => Task 9, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 2.09, Flat_loss 0.21, Train_acc 94.56, Test_acc 49.59
2023-10-17 20:34:57,181 [podnet.py] => Task 9, Epoch 108/160 (LR 0.02388) => LSC_loss 0.28, Spatial_loss 2.10, Flat_loss 0.22, Train_acc 94.16, Test_acc 50.08
2023-10-17 20:34:59,556 [podnet.py] => Task 9, Epoch 109/160 (LR 0.02304) => LSC_loss 0.27, Spatial_loss 2.07, Flat_loss 0.21, Train_acc 94.40, Test_acc 50.66
2023-10-17 20:35:01,841 [podnet.py] => Task 9, Epoch 110/160 (LR 0.02222) => LSC_loss 0.27, Spatial_loss 1.99, Flat_loss 0.21, Train_acc 94.58, Test_acc 49.76
2023-10-17 20:35:04,140 [podnet.py] => Task 9, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 1.98, Flat_loss 0.21, Train_acc 94.65, Test_acc 50.74
2023-10-17 20:35:06,459 [podnet.py] => Task 9, Epoch 112/160 (LR 0.02061) => LSC_loss 0.26, Spatial_loss 1.99, Flat_loss 0.20, Train_acc 94.67, Test_acc 51.11
2023-10-17 20:35:08,777 [podnet.py] => Task 9, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 1.90, Flat_loss 0.20, Train_acc 95.42, Test_acc 49.39
2023-10-17 20:35:11,091 [podnet.py] => Task 9, Epoch 114/160 (LR 0.01905) => LSC_loss 0.28, Spatial_loss 2.05, Flat_loss 0.21, Train_acc 94.02, Test_acc 49.80
2023-10-17 20:35:13,432 [podnet.py] => Task 9, Epoch 115/160 (LR 0.01828) => LSC_loss 0.27, Spatial_loss 1.97, Flat_loss 0.21, Train_acc 94.42, Test_acc 49.83
2023-10-17 20:35:15,844 [podnet.py] => Task 9, Epoch 116/160 (LR 0.01753) => LSC_loss 0.26, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 95.21, Test_acc 49.72
2023-10-17 20:35:18,176 [podnet.py] => Task 9, Epoch 117/160 (LR 0.01679) => LSC_loss 0.26, Spatial_loss 1.95, Flat_loss 0.20, Train_acc 94.70, Test_acc 49.36
2023-10-17 20:35:20,489 [podnet.py] => Task 9, Epoch 118/160 (LR 0.01606) => LSC_loss 0.27, Spatial_loss 1.87, Flat_loss 0.20, Train_acc 94.74, Test_acc 50.75
2023-10-17 20:35:22,893 [podnet.py] => Task 9, Epoch 119/160 (LR 0.01535) => LSC_loss 0.27, Spatial_loss 1.91, Flat_loss 0.20, Train_acc 94.35, Test_acc 50.07
2023-10-17 20:35:25,261 [podnet.py] => Task 9, Epoch 120/160 (LR 0.01464) => LSC_loss 0.26, Spatial_loss 1.83, Flat_loss 0.20, Train_acc 94.49, Test_acc 51.34
2023-10-17 20:35:27,615 [podnet.py] => Task 9, Epoch 121/160 (LR 0.01396) => LSC_loss 0.27, Spatial_loss 1.89, Flat_loss 0.19, Train_acc 94.81, Test_acc 50.56
2023-10-17 20:35:29,921 [podnet.py] => Task 9, Epoch 122/160 (LR 0.01328) => LSC_loss 0.26, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 95.21, Test_acc 51.21
2023-10-17 20:35:32,277 [podnet.py] => Task 9, Epoch 123/160 (LR 0.01262) => LSC_loss 0.25, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 95.00, Test_acc 51.27
2023-10-17 20:35:34,660 [podnet.py] => Task 9, Epoch 124/160 (LR 0.01198) => LSC_loss 0.26, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 95.09, Test_acc 50.41
2023-10-17 20:35:37,000 [podnet.py] => Task 9, Epoch 125/160 (LR 0.01135) => LSC_loss 0.26, Spatial_loss 1.84, Flat_loss 0.19, Train_acc 95.35, Test_acc 51.87
2023-10-17 20:35:39,305 [podnet.py] => Task 9, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 94.91, Test_acc 50.85
2023-10-17 20:35:41,643 [podnet.py] => Task 9, Epoch 127/160 (LR 0.01013) => LSC_loss 0.27, Spatial_loss 1.79, Flat_loss 0.19, Train_acc 94.88, Test_acc 50.92
2023-10-17 20:35:43,995 [podnet.py] => Task 9, Epoch 128/160 (LR 0.00955) => LSC_loss 0.26, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 95.07, Test_acc 51.48
2023-10-17 20:35:46,307 [podnet.py] => Task 9, Epoch 129/160 (LR 0.00898) => LSC_loss 0.27, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 94.81, Test_acc 50.86
2023-10-17 20:35:48,542 [podnet.py] => Task 9, Epoch 130/160 (LR 0.00843) => LSC_loss 0.26, Spatial_loss 1.73, Flat_loss 0.18, Train_acc 94.98, Test_acc 51.11
2023-10-17 20:35:50,866 [podnet.py] => Task 9, Epoch 131/160 (LR 0.00789) => LSC_loss 0.26, Spatial_loss 1.71, Flat_loss 0.18, Train_acc 94.63, Test_acc 51.98
2023-10-17 20:35:53,130 [podnet.py] => Task 9, Epoch 132/160 (LR 0.00737) => LSC_loss 0.26, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 95.14, Test_acc 50.80
2023-10-17 20:35:55,373 [podnet.py] => Task 9, Epoch 133/160 (LR 0.00686) => LSC_loss 0.26, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 94.95, Test_acc 50.63
2023-10-17 20:35:57,709 [podnet.py] => Task 9, Epoch 134/160 (LR 0.00638) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 94.81, Test_acc 50.56
2023-10-17 20:36:00,058 [podnet.py] => Task 9, Epoch 135/160 (LR 0.00590) => LSC_loss 0.26, Spatial_loss 1.63, Flat_loss 0.18, Train_acc 95.63, Test_acc 51.15
2023-10-17 20:36:02,414 [podnet.py] => Task 9, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.17, Train_acc 95.37, Test_acc 51.62
2023-10-17 20:36:04,765 [podnet.py] => Task 9, Epoch 137/160 (LR 0.00501) => LSC_loss 0.26, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 94.93, Test_acc 51.38
2023-10-17 20:36:07,091 [podnet.py] => Task 9, Epoch 138/160 (LR 0.00459) => LSC_loss 0.26, Spatial_loss 1.61, Flat_loss 0.17, Train_acc 94.81, Test_acc 52.18
2023-10-17 20:36:09,412 [podnet.py] => Task 9, Epoch 139/160 (LR 0.00419) => LSC_loss 0.26, Spatial_loss 1.65, Flat_loss 0.18, Train_acc 95.28, Test_acc 51.48
2023-10-17 20:36:11,731 [podnet.py] => Task 9, Epoch 140/160 (LR 0.00381) => LSC_loss 0.26, Spatial_loss 1.57, Flat_loss 0.17, Train_acc 95.05, Test_acc 51.84
2023-10-17 20:36:14,061 [podnet.py] => Task 9, Epoch 141/160 (LR 0.00344) => LSC_loss 0.26, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 95.35, Test_acc 51.56
2023-10-17 20:36:16,417 [podnet.py] => Task 9, Epoch 142/160 (LR 0.00309) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 95.21, Test_acc 51.99
2023-10-17 20:36:18,767 [podnet.py] => Task 9, Epoch 143/160 (LR 0.00276) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.21, Test_acc 51.74
2023-10-17 20:36:21,100 [podnet.py] => Task 9, Epoch 144/160 (LR 0.00245) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 95.05, Test_acc 51.52
2023-10-17 20:36:23,426 [podnet.py] => Task 9, Epoch 145/160 (LR 0.00215) => LSC_loss 0.25, Spatial_loss 1.56, Flat_loss 0.17, Train_acc 95.44, Test_acc 51.47
2023-10-17 20:36:25,761 [podnet.py] => Task 9, Epoch 146/160 (LR 0.00188) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.16, Train_acc 95.44, Test_acc 51.85
2023-10-17 20:36:28,071 [podnet.py] => Task 9, Epoch 147/160 (LR 0.00162) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.37, Test_acc 51.82
2023-10-17 20:36:30,415 [podnet.py] => Task 9, Epoch 148/160 (LR 0.00138) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.16, Train_acc 95.58, Test_acc 51.93
2023-10-17 20:36:32,752 [podnet.py] => Task 9, Epoch 149/160 (LR 0.00116) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.51, Test_acc 51.68
2023-10-17 20:36:35,111 [podnet.py] => Task 9, Epoch 150/160 (LR 0.00096) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.19, Test_acc 52.05
2023-10-17 20:36:37,434 [podnet.py] => Task 9, Epoch 151/160 (LR 0.00078) => LSC_loss 0.25, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 95.44, Test_acc 51.84
2023-10-17 20:36:39,723 [podnet.py] => Task 9, Epoch 152/160 (LR 0.00062) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.51, Test_acc 51.97
2023-10-17 20:36:42,019 [podnet.py] => Task 9, Epoch 153/160 (LR 0.00047) => LSC_loss 0.26, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 95.26, Test_acc 51.73
2023-10-17 20:36:44,392 [podnet.py] => Task 9, Epoch 154/160 (LR 0.00035) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.37, Test_acc 51.63
2023-10-17 20:36:46,693 [podnet.py] => Task 9, Epoch 155/160 (LR 0.00024) => LSC_loss 0.25, Spatial_loss 1.43, Flat_loss 0.16, Train_acc 95.16, Test_acc 51.58
2023-10-17 20:36:48,919 [podnet.py] => Task 9, Epoch 156/160 (LR 0.00015) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.60, Test_acc 52.23
2023-10-17 20:36:51,256 [podnet.py] => Task 9, Epoch 157/160 (LR 0.00009) => LSC_loss 0.26, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 95.23, Test_acc 51.91
2023-10-17 20:36:53,613 [podnet.py] => Task 9, Epoch 158/160 (LR 0.00004) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 95.02, Test_acc 52.00
2023-10-17 20:36:55,905 [podnet.py] => Task 9, Epoch 159/160 (LR 0.00001) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.47, Test_acc 51.71
2023-10-17 20:36:58,220 [podnet.py] => Task 9, Epoch 160/160 (LR 0.00000) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 95.33, Test_acc 51.62
2023-10-17 20:36:58,220 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:36:58,220 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:37:36,488 [podnet.py] => The size of finetune dataset: 1900
2023-10-17 20:37:38,283 [podnet.py] => Task 9, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 1.91, Flat_loss 0.15, Train_acc 96.95, Test_acc 51.20
2023-10-17 20:37:40,019 [podnet.py] => Task 9, Epoch 2/20 (LR 0.00488) => LSC_loss 0.13, Spatial_loss 1.55, Flat_loss 0.08, Train_acc 98.37, Test_acc 53.00
2023-10-17 20:37:41,797 [podnet.py] => Task 9, Epoch 3/20 (LR 0.00473) => LSC_loss 0.13, Spatial_loss 1.44, Flat_loss 0.07, Train_acc 98.26, Test_acc 52.92
2023-10-17 20:37:43,556 [podnet.py] => Task 9, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.47, Flat_loss 0.07, Train_acc 98.68, Test_acc 53.24
2023-10-17 20:37:45,316 [podnet.py] => Task 9, Epoch 5/20 (LR 0.00427) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.16, Test_acc 52.92
2023-10-17 20:37:47,117 [podnet.py] => Task 9, Epoch 6/20 (LR 0.00397) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.32, Test_acc 53.14
2023-10-17 20:37:48,867 [podnet.py] => Task 9, Epoch 7/20 (LR 0.00363) => LSC_loss 0.12, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.32, Test_acc 52.76
2023-10-17 20:37:50,563 [podnet.py] => Task 9, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.39
2023-10-17 20:37:52,277 [podnet.py] => Task 9, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 1.39, Flat_loss 0.07, Train_acc 98.47, Test_acc 53.13
2023-10-17 20:37:54,033 [podnet.py] => Task 9, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.37, Test_acc 52.96
2023-10-17 20:37:55,795 [podnet.py] => Task 9, Epoch 11/20 (LR 0.00211) => LSC_loss 0.12, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.27
2023-10-17 20:37:57,517 [podnet.py] => Task 9, Epoch 12/20 (LR 0.00173) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.68, Test_acc 52.95
2023-10-17 20:37:59,283 [podnet.py] => Task 9, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.09
2023-10-17 20:38:01,019 [podnet.py] => Task 9, Epoch 14/20 (LR 0.00103) => LSC_loss 0.13, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 98.37, Test_acc 53.05
2023-10-17 20:38:02,783 [podnet.py] => Task 9, Epoch 15/20 (LR 0.00073) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.58, Test_acc 53.02
2023-10-17 20:38:04,567 [podnet.py] => Task 9, Epoch 16/20 (LR 0.00048) => LSC_loss 0.13, Spatial_loss 1.37, Flat_loss 0.06, Train_acc 98.32, Test_acc 53.05
2023-10-17 20:38:06,292 [podnet.py] => Task 9, Epoch 17/20 (LR 0.00027) => LSC_loss 0.12, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.58, Test_acc 53.06
2023-10-17 20:38:08,033 [podnet.py] => Task 9, Epoch 18/20 (LR 0.00012) => LSC_loss 0.12, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.05
2023-10-17 20:38:09,788 [podnet.py] => Task 9, Epoch 19/20 (LR 0.00003) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.63, Test_acc 53.35
2023-10-17 20:38:11,518 [podnet.py] => Task 9, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.79, Test_acc 53.21
2023-10-17 20:38:11,520 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:38:51,415 [podnet.py] => Exemplar size: 1900
2023-10-17 20:38:51,416 [trainer.py] => CNN: {'total': 53.21, '00-09': 60.6, '10-19': 44.5, '20-29': 62.0, '30-39': 52.1, '40-49': 60.5, '50-59': 39.9, '60-69': 48.4, '70-79': 48.9, '80-89': 58.9, '90-99': 59.4, 'old': 52.87, 'new': 59.4}
2023-10-17 20:38:51,416 [trainer.py] => NME: {'total': 53.65, '00-09': 67.0, '10-19': 49.2, '20-29': 65.5, '30-39': 55.1, '40-49': 61.9, '50-59': 36.7, '60-69': 45.4, '70-79': 45.9, '80-89': 55.2, '90-99': 55.6, 'old': 53.54, 'new': 55.6}
2023-10-17 20:38:51,416 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7, 53.21]
2023-10-17 20:38:51,416 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5, 80.15]
2023-10-17 20:38:51,416 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1, 53.65]
2023-10-17 20:38:51,416 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73, 81.57]

2023-10-17 20:38:51,416 [trainer.py] => Average Accuracy (CNN): 63.358000000000004
2023-10-17 20:38:51,416 [trainer.py] => Average Accuracy (NME): 63.672000000000004
2023-10-17 20:38:51,416 [trainer.py] => All params: 527057
2023-10-17 20:38:51,417 [trainer.py] => Trainable params: 527057
2023-10-17 20:38:51,418 [podnet.py] => Learning on 95-100
2023-10-17 20:38:51,452 [podnet.py] => Adaptive factor: 4.47213595499958
2023-10-17 20:38:53,823 [podnet.py] => Task 10, Epoch 1/160 (LR 0.09999) => LSC_loss 1.74, Spatial_loss 4.19, Flat_loss 0.84, Train_acc 68.27, Test_acc 34.42
2023-10-17 20:38:56,177 [podnet.py] => Task 10, Epoch 2/160 (LR 0.09996) => LSC_loss 0.89, Spatial_loss 4.67, Flat_loss 0.74, Train_acc 76.41, Test_acc 35.37
2023-10-17 20:38:58,562 [podnet.py] => Task 10, Epoch 3/160 (LR 0.09991) => LSC_loss 0.78, Spatial_loss 4.48, Flat_loss 0.68, Train_acc 78.27, Test_acc 35.87
2023-10-17 20:39:00,932 [podnet.py] => Task 10, Epoch 4/160 (LR 0.09985) => LSC_loss 0.64, Spatial_loss 4.16, Flat_loss 0.56, Train_acc 83.23, Test_acc 41.88
2023-10-17 20:39:03,332 [podnet.py] => Task 10, Epoch 5/160 (LR 0.09976) => LSC_loss 0.56, Spatial_loss 3.82, Flat_loss 0.49, Train_acc 84.98, Test_acc 36.33
2023-10-17 20:39:05,697 [podnet.py] => Task 10, Epoch 6/160 (LR 0.09965) => LSC_loss 0.53, Spatial_loss 3.71, Flat_loss 0.45, Train_acc 86.14, Test_acc 41.55
2023-10-17 20:39:08,092 [podnet.py] => Task 10, Epoch 7/160 (LR 0.09953) => LSC_loss 0.51, Spatial_loss 3.69, Flat_loss 0.44, Train_acc 87.23, Test_acc 41.85
2023-10-17 20:39:10,469 [podnet.py] => Task 10, Epoch 8/160 (LR 0.09938) => LSC_loss 0.50, Spatial_loss 3.66, Flat_loss 0.43, Train_acc 86.98, Test_acc 39.85
2023-10-17 20:39:12,809 [podnet.py] => Task 10, Epoch 9/160 (LR 0.09922) => LSC_loss 0.50, Spatial_loss 3.58, Flat_loss 0.41, Train_acc 87.36, Test_acc 44.21
2023-10-17 20:39:15,088 [podnet.py] => Task 10, Epoch 10/160 (LR 0.09904) => LSC_loss 0.45, Spatial_loss 3.46, Flat_loss 0.39, Train_acc 89.02, Test_acc 44.26
2023-10-17 20:39:17,469 [podnet.py] => Task 10, Epoch 11/160 (LR 0.09884) => LSC_loss 0.45, Spatial_loss 3.42, Flat_loss 0.38, Train_acc 88.84, Test_acc 46.50
2023-10-17 20:39:19,900 [podnet.py] => Task 10, Epoch 12/160 (LR 0.09862) => LSC_loss 0.45, Spatial_loss 3.51, Flat_loss 0.38, Train_acc 88.84, Test_acc 40.46
2023-10-17 20:39:22,255 [podnet.py] => Task 10, Epoch 13/160 (LR 0.09838) => LSC_loss 0.41, Spatial_loss 3.29, Flat_loss 0.36, Train_acc 90.25, Test_acc 44.98
2023-10-17 20:39:24,653 [podnet.py] => Task 10, Epoch 14/160 (LR 0.09812) => LSC_loss 0.44, Spatial_loss 3.35, Flat_loss 0.37, Train_acc 89.11, Test_acc 43.38
2023-10-17 20:39:27,030 [podnet.py] => Task 10, Epoch 15/160 (LR 0.09785) => LSC_loss 0.45, Spatial_loss 3.41, Flat_loss 0.37, Train_acc 89.16, Test_acc 42.15
2023-10-17 20:39:29,420 [podnet.py] => Task 10, Epoch 16/160 (LR 0.09755) => LSC_loss 0.42, Spatial_loss 3.35, Flat_loss 0.36, Train_acc 90.36, Test_acc 37.40
2023-10-17 20:39:31,814 [podnet.py] => Task 10, Epoch 17/160 (LR 0.09724) => LSC_loss 0.42, Spatial_loss 3.37, Flat_loss 0.37, Train_acc 89.66, Test_acc 37.82
2023-10-17 20:39:34,199 [podnet.py] => Task 10, Epoch 18/160 (LR 0.09691) => LSC_loss 0.42, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 90.02, Test_acc 44.82
2023-10-17 20:39:36,563 [podnet.py] => Task 10, Epoch 19/160 (LR 0.09656) => LSC_loss 0.41, Spatial_loss 3.34, Flat_loss 0.36, Train_acc 89.95, Test_acc 41.55
2023-10-17 20:39:38,970 [podnet.py] => Task 10, Epoch 20/160 (LR 0.09619) => LSC_loss 0.42, Spatial_loss 3.34, Flat_loss 0.36, Train_acc 89.91, Test_acc 40.54
2023-10-17 20:39:41,370 [podnet.py] => Task 10, Epoch 21/160 (LR 0.09581) => LSC_loss 0.40, Spatial_loss 3.26, Flat_loss 0.37, Train_acc 90.52, Test_acc 45.27
2023-10-17 20:39:43,755 [podnet.py] => Task 10, Epoch 22/160 (LR 0.09541) => LSC_loss 0.40, Spatial_loss 3.23, Flat_loss 0.35, Train_acc 90.11, Test_acc 42.16
2023-10-17 20:39:46,135 [podnet.py] => Task 10, Epoch 23/160 (LR 0.09499) => LSC_loss 0.39, Spatial_loss 3.26, Flat_loss 0.35, Train_acc 90.18, Test_acc 41.69
2023-10-17 20:39:48,420 [podnet.py] => Task 10, Epoch 24/160 (LR 0.09455) => LSC_loss 0.40, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 90.34, Test_acc 45.09
2023-10-17 20:39:50,800 [podnet.py] => Task 10, Epoch 25/160 (LR 0.09410) => LSC_loss 0.40, Spatial_loss 3.21, Flat_loss 0.35, Train_acc 90.25, Test_acc 40.26
2023-10-17 20:39:53,173 [podnet.py] => Task 10, Epoch 26/160 (LR 0.09362) => LSC_loss 0.42, Spatial_loss 3.40, Flat_loss 0.37, Train_acc 89.36, Test_acc 41.67
2023-10-17 20:39:55,503 [podnet.py] => Task 10, Epoch 27/160 (LR 0.09314) => LSC_loss 0.40, Spatial_loss 3.27, Flat_loss 0.35, Train_acc 90.34, Test_acc 41.84
2023-10-17 20:39:57,903 [podnet.py] => Task 10, Epoch 28/160 (LR 0.09263) => LSC_loss 0.39, Spatial_loss 3.24, Flat_loss 0.35, Train_acc 90.57, Test_acc 43.09
2023-10-17 20:40:00,291 [podnet.py] => Task 10, Epoch 29/160 (LR 0.09211) => LSC_loss 0.38, Spatial_loss 3.18, Flat_loss 0.34, Train_acc 90.86, Test_acc 38.88
2023-10-17 20:40:02,713 [podnet.py] => Task 10, Epoch 30/160 (LR 0.09157) => LSC_loss 0.36, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 91.36, Test_acc 42.06
2023-10-17 20:40:05,080 [podnet.py] => Task 10, Epoch 31/160 (LR 0.09102) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.32, Train_acc 91.50, Test_acc 42.24
2023-10-17 20:40:07,492 [podnet.py] => Task 10, Epoch 32/160 (LR 0.09045) => LSC_loss 0.36, Spatial_loss 3.07, Flat_loss 0.32, Train_acc 91.43, Test_acc 43.99
2023-10-17 20:40:09,896 [podnet.py] => Task 10, Epoch 33/160 (LR 0.08987) => LSC_loss 0.38, Spatial_loss 3.16, Flat_loss 0.33, Train_acc 90.84, Test_acc 41.99
2023-10-17 20:40:12,268 [podnet.py] => Task 10, Epoch 34/160 (LR 0.08927) => LSC_loss 0.38, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 90.84, Test_acc 45.45
2023-10-17 20:40:14,572 [podnet.py] => Task 10, Epoch 35/160 (LR 0.08865) => LSC_loss 0.36, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 91.59, Test_acc 47.53
2023-10-17 20:40:17,003 [podnet.py] => Task 10, Epoch 36/160 (LR 0.08802) => LSC_loss 0.37, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 91.32, Test_acc 44.47
2023-10-17 20:40:19,405 [podnet.py] => Task 10, Epoch 37/160 (LR 0.08738) => LSC_loss 0.35, Spatial_loss 3.01, Flat_loss 0.33, Train_acc 91.73, Test_acc 45.10
2023-10-17 20:40:21,772 [podnet.py] => Task 10, Epoch 38/160 (LR 0.08672) => LSC_loss 0.36, Spatial_loss 3.04, Flat_loss 0.32, Train_acc 91.82, Test_acc 42.99
2023-10-17 20:40:24,199 [podnet.py] => Task 10, Epoch 39/160 (LR 0.08604) => LSC_loss 0.37, Spatial_loss 3.09, Flat_loss 0.32, Train_acc 91.30, Test_acc 40.81
2023-10-17 20:40:26,610 [podnet.py] => Task 10, Epoch 40/160 (LR 0.08536) => LSC_loss 0.36, Spatial_loss 3.10, Flat_loss 0.33, Train_acc 91.07, Test_acc 44.93
2023-10-17 20:40:29,002 [podnet.py] => Task 10, Epoch 41/160 (LR 0.08465) => LSC_loss 0.35, Spatial_loss 3.07, Flat_loss 0.32, Train_acc 91.86, Test_acc 42.71
2023-10-17 20:40:31,381 [podnet.py] => Task 10, Epoch 42/160 (LR 0.08394) => LSC_loss 0.34, Spatial_loss 3.00, Flat_loss 0.31, Train_acc 92.39, Test_acc 43.84
2023-10-17 20:40:33,798 [podnet.py] => Task 10, Epoch 43/160 (LR 0.08321) => LSC_loss 0.33, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 93.16, Test_acc 44.37
2023-10-17 20:40:36,178 [podnet.py] => Task 10, Epoch 44/160 (LR 0.08247) => LSC_loss 0.35, Spatial_loss 2.96, Flat_loss 0.30, Train_acc 92.00, Test_acc 40.63
2023-10-17 20:40:38,616 [podnet.py] => Task 10, Epoch 45/160 (LR 0.08172) => LSC_loss 0.34, Spatial_loss 2.98, Flat_loss 0.31, Train_acc 92.36, Test_acc 43.13
2023-10-17 20:40:40,985 [podnet.py] => Task 10, Epoch 46/160 (LR 0.08095) => LSC_loss 0.36, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 91.59, Test_acc 44.74
2023-10-17 20:40:43,406 [podnet.py] => Task 10, Epoch 47/160 (LR 0.08018) => LSC_loss 0.36, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 91.50, Test_acc 45.77
2023-10-17 20:40:45,729 [podnet.py] => Task 10, Epoch 48/160 (LR 0.07939) => LSC_loss 0.36, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 91.34, Test_acc 44.00
2023-10-17 20:40:48,148 [podnet.py] => Task 10, Epoch 49/160 (LR 0.07859) => LSC_loss 0.35, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 92.02, Test_acc 43.79
2023-10-17 20:40:50,586 [podnet.py] => Task 10, Epoch 50/160 (LR 0.07778) => LSC_loss 0.36, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 91.27, Test_acc 40.46
2023-10-17 20:40:52,932 [podnet.py] => Task 10, Epoch 51/160 (LR 0.07696) => LSC_loss 0.36, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 91.18, Test_acc 45.47
2023-10-17 20:40:55,338 [podnet.py] => Task 10, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.03, Flat_loss 0.30, Train_acc 92.64, Test_acc 44.24
2023-10-17 20:40:57,774 [podnet.py] => Task 10, Epoch 53/160 (LR 0.07528) => LSC_loss 0.34, Spatial_loss 2.98, Flat_loss 0.30, Train_acc 91.64, Test_acc 46.82
2023-10-17 20:41:00,199 [podnet.py] => Task 10, Epoch 54/160 (LR 0.07443) => LSC_loss 0.35, Spatial_loss 2.97, Flat_loss 0.31, Train_acc 91.52, Test_acc 42.57
2023-10-17 20:41:02,617 [podnet.py] => Task 10, Epoch 55/160 (LR 0.07357) => LSC_loss 0.35, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 92.09, Test_acc 44.74
2023-10-17 20:41:04,986 [podnet.py] => Task 10, Epoch 56/160 (LR 0.07270) => LSC_loss 0.34, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 92.05, Test_acc 43.15
2023-10-17 20:41:07,354 [podnet.py] => Task 10, Epoch 57/160 (LR 0.07182) => LSC_loss 0.33, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 92.52, Test_acc 46.80
2023-10-17 20:41:09,740 [podnet.py] => Task 10, Epoch 58/160 (LR 0.07093) => LSC_loss 0.34, Spatial_loss 2.95, Flat_loss 0.29, Train_acc 92.16, Test_acc 43.42
2023-10-17 20:41:12,124 [podnet.py] => Task 10, Epoch 59/160 (LR 0.07004) => LSC_loss 0.31, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.84, Test_acc 40.49
2023-10-17 20:41:14,545 [podnet.py] => Task 10, Epoch 60/160 (LR 0.06913) => LSC_loss 0.33, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 92.23, Test_acc 43.95
2023-10-17 20:41:16,979 [podnet.py] => Task 10, Epoch 61/160 (LR 0.06822) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 93.14, Test_acc 47.12
2023-10-17 20:41:19,395 [podnet.py] => Task 10, Epoch 62/160 (LR 0.06731) => LSC_loss 0.33, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 92.68, Test_acc 45.90
2023-10-17 20:41:21,847 [podnet.py] => Task 10, Epoch 63/160 (LR 0.06638) => LSC_loss 0.32, Spatial_loss 2.82, Flat_loss 0.29, Train_acc 92.93, Test_acc 44.37
2023-10-17 20:41:24,274 [podnet.py] => Task 10, Epoch 64/160 (LR 0.06545) => LSC_loss 0.34, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 92.00, Test_acc 43.75
2023-10-17 20:41:26,675 [podnet.py] => Task 10, Epoch 65/160 (LR 0.06451) => LSC_loss 0.31, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 93.20, Test_acc 44.11
2023-10-17 20:41:29,097 [podnet.py] => Task 10, Epoch 66/160 (LR 0.06357) => LSC_loss 0.33, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.39, Test_acc 46.29
2023-10-17 20:41:31,490 [podnet.py] => Task 10, Epoch 67/160 (LR 0.06262) => LSC_loss 0.31, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 92.86, Test_acc 46.93
2023-10-17 20:41:33,873 [podnet.py] => Task 10, Epoch 68/160 (LR 0.06167) => LSC_loss 0.32, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 92.73, Test_acc 44.73
2023-10-17 20:41:36,192 [podnet.py] => Task 10, Epoch 69/160 (LR 0.06072) => LSC_loss 0.33, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.98, Test_acc 46.16
2023-10-17 20:41:38,576 [podnet.py] => Task 10, Epoch 70/160 (LR 0.05975) => LSC_loss 0.31, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.30, Test_acc 44.79
2023-10-17 20:41:40,951 [podnet.py] => Task 10, Epoch 71/160 (LR 0.05879) => LSC_loss 0.33, Spatial_loss 2.82, Flat_loss 0.28, Train_acc 92.14, Test_acc 46.22
2023-10-17 20:41:43,347 [podnet.py] => Task 10, Epoch 72/160 (LR 0.05782) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 92.84, Test_acc 46.32
2023-10-17 20:41:45,741 [podnet.py] => Task 10, Epoch 73/160 (LR 0.05685) => LSC_loss 0.31, Spatial_loss 2.72, Flat_loss 0.27, Train_acc 92.98, Test_acc 46.52
2023-10-17 20:41:48,076 [podnet.py] => Task 10, Epoch 74/160 (LR 0.05588) => LSC_loss 0.30, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 93.16, Test_acc 46.48
2023-10-17 20:41:50,420 [podnet.py] => Task 10, Epoch 75/160 (LR 0.05490) => LSC_loss 0.31, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.05, Test_acc 45.60
2023-10-17 20:41:52,804 [podnet.py] => Task 10, Epoch 76/160 (LR 0.05392) => LSC_loss 0.32, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 92.86, Test_acc 47.60
2023-10-17 20:41:55,169 [podnet.py] => Task 10, Epoch 77/160 (LR 0.05294) => LSC_loss 0.32, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 93.02, Test_acc 46.29
2023-10-17 20:41:57,597 [podnet.py] => Task 10, Epoch 78/160 (LR 0.05196) => LSC_loss 0.32, Spatial_loss 2.60, Flat_loss 0.26, Train_acc 92.57, Test_acc 42.00
2023-10-17 20:41:59,948 [podnet.py] => Task 10, Epoch 79/160 (LR 0.05098) => LSC_loss 0.32, Spatial_loss 2.58, Flat_loss 0.26, Train_acc 92.57, Test_acc 46.32
2023-10-17 20:42:02,339 [podnet.py] => Task 10, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 2.54, Flat_loss 0.25, Train_acc 93.91, Test_acc 45.37
2023-10-17 20:42:04,785 [podnet.py] => Task 10, Epoch 81/160 (LR 0.04902) => LSC_loss 0.29, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 93.55, Test_acc 46.22
2023-10-17 20:42:07,239 [podnet.py] => Task 10, Epoch 82/160 (LR 0.04804) => LSC_loss 0.30, Spatial_loss 2.39, Flat_loss 0.23, Train_acc 93.61, Test_acc 46.89
2023-10-17 20:42:09,648 [podnet.py] => Task 10, Epoch 83/160 (LR 0.04706) => LSC_loss 0.30, Spatial_loss 2.49, Flat_loss 0.24, Train_acc 93.16, Test_acc 44.32
2023-10-17 20:42:12,035 [podnet.py] => Task 10, Epoch 84/160 (LR 0.04608) => LSC_loss 0.29, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 93.68, Test_acc 47.55
2023-10-17 20:42:14,399 [podnet.py] => Task 10, Epoch 85/160 (LR 0.04510) => LSC_loss 0.30, Spatial_loss 2.42, Flat_loss 0.23, Train_acc 93.91, Test_acc 46.12
2023-10-17 20:42:16,819 [podnet.py] => Task 10, Epoch 86/160 (LR 0.04412) => LSC_loss 0.31, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 93.41, Test_acc 44.38
2023-10-17 20:42:19,221 [podnet.py] => Task 10, Epoch 87/160 (LR 0.04315) => LSC_loss 0.30, Spatial_loss 2.43, Flat_loss 0.25, Train_acc 93.43, Test_acc 48.46
2023-10-17 20:42:21,661 [podnet.py] => Task 10, Epoch 88/160 (LR 0.04218) => LSC_loss 0.30, Spatial_loss 2.47, Flat_loss 0.24, Train_acc 93.59, Test_acc 45.85
2023-10-17 20:42:24,029 [podnet.py] => Task 10, Epoch 89/160 (LR 0.04121) => LSC_loss 0.28, Spatial_loss 2.41, Flat_loss 0.23, Train_acc 93.86, Test_acc 46.75
2023-10-17 20:42:26,515 [podnet.py] => Task 10, Epoch 90/160 (LR 0.04025) => LSC_loss 0.30, Spatial_loss 2.42, Flat_loss 0.23, Train_acc 93.59, Test_acc 44.74
2023-10-17 20:42:28,932 [podnet.py] => Task 10, Epoch 91/160 (LR 0.03928) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 93.66, Test_acc 46.72
2023-10-17 20:42:31,320 [podnet.py] => Task 10, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 93.89, Test_acc 47.58
2023-10-17 20:42:33,726 [podnet.py] => Task 10, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.28, Flat_loss 0.22, Train_acc 93.86, Test_acc 47.94
2023-10-17 20:42:36,112 [podnet.py] => Task 10, Epoch 94/160 (LR 0.03643) => LSC_loss 0.30, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 93.36, Test_acc 44.17
2023-10-17 20:42:38,515 [podnet.py] => Task 10, Epoch 95/160 (LR 0.03549) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.23, Train_acc 94.02, Test_acc 47.81
2023-10-17 20:42:40,864 [podnet.py] => Task 10, Epoch 96/160 (LR 0.03455) => LSC_loss 0.28, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 94.09, Test_acc 44.94
2023-10-17 20:42:43,232 [podnet.py] => Task 10, Epoch 97/160 (LR 0.03362) => LSC_loss 0.29, Spatial_loss 2.27, Flat_loss 0.22, Train_acc 93.80, Test_acc 46.94
2023-10-17 20:42:45,567 [podnet.py] => Task 10, Epoch 98/160 (LR 0.03269) => LSC_loss 0.28, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.16, Test_acc 48.73
2023-10-17 20:42:47,939 [podnet.py] => Task 10, Epoch 99/160 (LR 0.03178) => LSC_loss 0.27, Spatial_loss 2.14, Flat_loss 0.21, Train_acc 94.48, Test_acc 46.41
2023-10-17 20:42:50,262 [podnet.py] => Task 10, Epoch 100/160 (LR 0.03087) => LSC_loss 0.29, Spatial_loss 2.22, Flat_loss 0.21, Train_acc 93.52, Test_acc 49.43
2023-10-17 20:42:52,579 [podnet.py] => Task 10, Epoch 101/160 (LR 0.02996) => LSC_loss 0.28, Spatial_loss 2.18, Flat_loss 0.21, Train_acc 94.36, Test_acc 47.95
2023-10-17 20:42:54,943 [podnet.py] => Task 10, Epoch 102/160 (LR 0.02907) => LSC_loss 0.29, Spatial_loss 2.13, Flat_loss 0.21, Train_acc 93.77, Test_acc 48.91
2023-10-17 20:42:57,324 [podnet.py] => Task 10, Epoch 103/160 (LR 0.02818) => LSC_loss 0.28, Spatial_loss 2.15, Flat_loss 0.20, Train_acc 94.41, Test_acc 47.19
2023-10-17 20:42:59,779 [podnet.py] => Task 10, Epoch 104/160 (LR 0.02730) => LSC_loss 0.28, Spatial_loss 2.17, Flat_loss 0.21, Train_acc 93.86, Test_acc 48.20
2023-10-17 20:43:02,211 [podnet.py] => Task 10, Epoch 105/160 (LR 0.02643) => LSC_loss 0.28, Spatial_loss 2.03, Flat_loss 0.20, Train_acc 94.14, Test_acc 48.09
2023-10-17 20:43:04,695 [podnet.py] => Task 10, Epoch 106/160 (LR 0.02557) => LSC_loss 0.28, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 93.64, Test_acc 48.42
2023-10-17 20:43:07,140 [podnet.py] => Task 10, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 2.07, Flat_loss 0.20, Train_acc 94.11, Test_acc 48.40
2023-10-17 20:43:09,586 [podnet.py] => Task 10, Epoch 108/160 (LR 0.02388) => LSC_loss 0.27, Spatial_loss 2.04, Flat_loss 0.20, Train_acc 94.84, Test_acc 47.09
2023-10-17 20:43:12,043 [podnet.py] => Task 10, Epoch 109/160 (LR 0.02304) => LSC_loss 0.28, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 93.77, Test_acc 46.98
2023-10-17 20:43:14,453 [podnet.py] => Task 10, Epoch 110/160 (LR 0.02222) => LSC_loss 0.28, Spatial_loss 2.11, Flat_loss 0.20, Train_acc 94.45, Test_acc 48.65
2023-10-17 20:43:16,930 [podnet.py] => Task 10, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 2.01, Flat_loss 0.19, Train_acc 94.09, Test_acc 47.55
2023-10-17 20:43:19,377 [podnet.py] => Task 10, Epoch 112/160 (LR 0.02061) => LSC_loss 0.28, Spatial_loss 1.99, Flat_loss 0.19, Train_acc 94.14, Test_acc 48.74
2023-10-17 20:43:21,777 [podnet.py] => Task 10, Epoch 113/160 (LR 0.01982) => LSC_loss 0.28, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 93.95, Test_acc 47.91
2023-10-17 20:43:24,154 [podnet.py] => Task 10, Epoch 114/160 (LR 0.01905) => LSC_loss 0.30, Spatial_loss 1.93, Flat_loss 0.19, Train_acc 93.59, Test_acc 47.18
2023-10-17 20:43:26,608 [podnet.py] => Task 10, Epoch 115/160 (LR 0.01828) => LSC_loss 0.27, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 94.41, Test_acc 49.42
2023-10-17 20:43:29,051 [podnet.py] => Task 10, Epoch 116/160 (LR 0.01753) => LSC_loss 0.28, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 94.39, Test_acc 50.54
2023-10-17 20:43:31,527 [podnet.py] => Task 10, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 1.91, Flat_loss 0.18, Train_acc 94.11, Test_acc 49.62
2023-10-17 20:43:33,954 [podnet.py] => Task 10, Epoch 118/160 (LR 0.01606) => LSC_loss 0.26, Spatial_loss 1.89, Flat_loss 0.18, Train_acc 94.36, Test_acc 46.88
2023-10-17 20:43:36,423 [podnet.py] => Task 10, Epoch 119/160 (LR 0.01535) => LSC_loss 0.27, Spatial_loss 1.87, Flat_loss 0.18, Train_acc 94.23, Test_acc 48.28
2023-10-17 20:43:38,852 [podnet.py] => Task 10, Epoch 120/160 (LR 0.01464) => LSC_loss 0.28, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 94.18, Test_acc 50.21
2023-10-17 20:43:41,220 [podnet.py] => Task 10, Epoch 121/160 (LR 0.01396) => LSC_loss 0.27, Spatial_loss 1.88, Flat_loss 0.18, Train_acc 94.34, Test_acc 49.33
2023-10-17 20:43:43,620 [podnet.py] => Task 10, Epoch 122/160 (LR 0.01328) => LSC_loss 0.26, Spatial_loss 1.85, Flat_loss 0.18, Train_acc 94.75, Test_acc 49.43
2023-10-17 20:43:45,970 [podnet.py] => Task 10, Epoch 123/160 (LR 0.01262) => LSC_loss 0.27, Spatial_loss 1.77, Flat_loss 0.17, Train_acc 94.70, Test_acc 47.79
2023-10-17 20:43:48,316 [podnet.py] => Task 10, Epoch 124/160 (LR 0.01198) => LSC_loss 0.27, Spatial_loss 1.72, Flat_loss 0.17, Train_acc 94.32, Test_acc 49.10
2023-10-17 20:43:50,694 [podnet.py] => Task 10, Epoch 125/160 (LR 0.01135) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.17, Train_acc 94.36, Test_acc 48.60
2023-10-17 20:43:53,061 [podnet.py] => Task 10, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 94.66, Test_acc 48.89
2023-10-17 20:43:55,381 [podnet.py] => Task 10, Epoch 127/160 (LR 0.01013) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.17, Train_acc 93.93, Test_acc 48.52
2023-10-17 20:43:57,781 [podnet.py] => Task 10, Epoch 128/160 (LR 0.00955) => LSC_loss 0.27, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 94.55, Test_acc 47.81
2023-10-17 20:44:00,152 [podnet.py] => Task 10, Epoch 129/160 (LR 0.00898) => LSC_loss 0.27, Spatial_loss 1.73, Flat_loss 0.17, Train_acc 93.93, Test_acc 48.51
2023-10-17 20:44:02,614 [podnet.py] => Task 10, Epoch 130/160 (LR 0.00843) => LSC_loss 0.27, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 94.25, Test_acc 48.96
2023-10-17 20:44:05,019 [podnet.py] => Task 10, Epoch 131/160 (LR 0.00789) => LSC_loss 0.27, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 94.84, Test_acc 49.71
2023-10-17 20:44:07,403 [podnet.py] => Task 10, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 94.59, Test_acc 50.17
2023-10-17 20:44:09,782 [podnet.py] => Task 10, Epoch 133/160 (LR 0.00686) => LSC_loss 0.28, Spatial_loss 1.71, Flat_loss 0.16, Train_acc 93.93, Test_acc 50.17
2023-10-17 20:44:12,166 [podnet.py] => Task 10, Epoch 134/160 (LR 0.00638) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.00, Test_acc 49.28
2023-10-17 20:44:14,585 [podnet.py] => Task 10, Epoch 135/160 (LR 0.00590) => LSC_loss 0.27, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 94.98, Test_acc 50.80
2023-10-17 20:44:16,997 [podnet.py] => Task 10, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 95.18, Test_acc 49.07
2023-10-17 20:44:19,414 [podnet.py] => Task 10, Epoch 137/160 (LR 0.00501) => LSC_loss 0.27, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 94.59, Test_acc 49.80
2023-10-17 20:44:21,780 [podnet.py] => Task 10, Epoch 138/160 (LR 0.00459) => LSC_loss 0.26, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 94.64, Test_acc 49.78
2023-10-17 20:44:24,171 [podnet.py] => Task 10, Epoch 139/160 (LR 0.00419) => LSC_loss 0.27, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 94.84, Test_acc 49.87
2023-10-17 20:44:26,550 [podnet.py] => Task 10, Epoch 140/160 (LR 0.00381) => LSC_loss 0.27, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 94.18, Test_acc 49.90
2023-10-17 20:44:28,943 [podnet.py] => Task 10, Epoch 141/160 (LR 0.00344) => LSC_loss 0.27, Spatial_loss 1.57, Flat_loss 0.15, Train_acc 94.36, Test_acc 50.02
2023-10-17 20:44:31,336 [podnet.py] => Task 10, Epoch 142/160 (LR 0.00309) => LSC_loss 0.27, Spatial_loss 1.60, Flat_loss 0.16, Train_acc 94.41, Test_acc 50.24
2023-10-17 20:44:33,690 [podnet.py] => Task 10, Epoch 143/160 (LR 0.00276) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.45, Test_acc 50.36
2023-10-17 20:44:36,075 [podnet.py] => Task 10, Epoch 144/160 (LR 0.00245) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.11, Test_acc 50.11
2023-10-17 20:44:38,492 [podnet.py] => Task 10, Epoch 145/160 (LR 0.00215) => LSC_loss 0.27, Spatial_loss 1.54, Flat_loss 0.16, Train_acc 94.61, Test_acc 50.58
2023-10-17 20:44:40,849 [podnet.py] => Task 10, Epoch 146/160 (LR 0.00188) => LSC_loss 0.27, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.50, Test_acc 50.25
2023-10-17 20:44:43,239 [podnet.py] => Task 10, Epoch 147/160 (LR 0.00162) => LSC_loss 0.26, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.93, Test_acc 49.84
2023-10-17 20:44:45,626 [podnet.py] => Task 10, Epoch 148/160 (LR 0.00138) => LSC_loss 0.26, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.66, Test_acc 49.85
2023-10-17 20:44:48,016 [podnet.py] => Task 10, Epoch 149/160 (LR 0.00116) => LSC_loss 0.27, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 94.75, Test_acc 50.07
2023-10-17 20:44:50,443 [podnet.py] => Task 10, Epoch 150/160 (LR 0.00096) => LSC_loss 0.27, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 94.30, Test_acc 49.83
2023-10-17 20:44:52,808 [podnet.py] => Task 10, Epoch 151/160 (LR 0.00078) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.11, Test_acc 49.91
2023-10-17 20:44:55,201 [podnet.py] => Task 10, Epoch 152/160 (LR 0.00062) => LSC_loss 0.26, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 94.91, Test_acc 50.23
2023-10-17 20:44:57,576 [podnet.py] => Task 10, Epoch 153/160 (LR 0.00047) => LSC_loss 0.27, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.52, Test_acc 49.70
2023-10-17 20:44:59,950 [podnet.py] => Task 10, Epoch 154/160 (LR 0.00035) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 94.80, Test_acc 50.09
2023-10-17 20:45:02,370 [podnet.py] => Task 10, Epoch 155/160 (LR 0.00024) => LSC_loss 0.27, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.52, Test_acc 49.71
2023-10-17 20:45:04,795 [podnet.py] => Task 10, Epoch 156/160 (LR 0.00015) => LSC_loss 0.26, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 95.07, Test_acc 49.68
2023-10-17 20:45:07,208 [podnet.py] => Task 10, Epoch 157/160 (LR 0.00009) => LSC_loss 0.26, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 94.95, Test_acc 50.23
2023-10-17 20:45:09,672 [podnet.py] => Task 10, Epoch 158/160 (LR 0.00004) => LSC_loss 0.26, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 94.89, Test_acc 50.23
2023-10-17 20:45:11,999 [podnet.py] => Task 10, Epoch 159/160 (LR 0.00001) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.25, Test_acc 49.95
2023-10-17 20:45:14,431 [podnet.py] => Task 10, Epoch 160/160 (LR 0.00000) => LSC_loss 0.26, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.80, Test_acc 49.80
2023-10-17 20:45:14,433 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2023-10-17 20:45:14,433 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:45:54,320 [podnet.py] => The size of finetune dataset: 2000
2023-10-17 20:45:56,186 [podnet.py] => Task 10, Epoch 1/20 (LR 0.00497) => LSC_loss 0.21, Spatial_loss 1.71, Flat_loss 0.14, Train_acc 96.45, Test_acc 50.14
2023-10-17 20:45:57,998 [podnet.py] => Task 10, Epoch 2/20 (LR 0.00488) => LSC_loss 0.14, Spatial_loss 1.57, Flat_loss 0.08, Train_acc 98.45, Test_acc 51.96
2023-10-17 20:45:59,792 [podnet.py] => Task 10, Epoch 3/20 (LR 0.00473) => LSC_loss 0.13, Spatial_loss 1.44, Flat_loss 0.07, Train_acc 98.80, Test_acc 51.92
2023-10-17 20:46:01,647 [podnet.py] => Task 10, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.35, Test_acc 51.30
2023-10-17 20:46:03,462 [podnet.py] => Task 10, Epoch 5/20 (LR 0.00427) => LSC_loss 0.13, Spatial_loss 1.46, Flat_loss 0.06, Train_acc 98.05, Test_acc 52.10
2023-10-17 20:46:05,257 [podnet.py] => Task 10, Epoch 6/20 (LR 0.00397) => LSC_loss 0.13, Spatial_loss 1.48, Flat_loss 0.06, Train_acc 98.70, Test_acc 51.94
2023-10-17 20:46:07,060 [podnet.py] => Task 10, Epoch 7/20 (LR 0.00363) => LSC_loss 0.13, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.30, Test_acc 51.72
2023-10-17 20:46:08,880 [podnet.py] => Task 10, Epoch 8/20 (LR 0.00327) => LSC_loss 0.13, Spatial_loss 1.42, Flat_loss 0.07, Train_acc 98.50, Test_acc 52.00
2023-10-17 20:46:10,639 [podnet.py] => Task 10, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.06, Train_acc 98.60, Test_acc 51.62
2023-10-17 20:46:12,457 [podnet.py] => Task 10, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 1.37, Flat_loss 0.06, Train_acc 98.55, Test_acc 52.08
2023-10-17 20:46:14,290 [podnet.py] => Task 10, Epoch 11/20 (LR 0.00211) => LSC_loss 0.13, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.85, Test_acc 51.48
2023-10-17 20:46:16,099 [podnet.py] => Task 10, Epoch 12/20 (LR 0.00173) => LSC_loss 0.13, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.30, Test_acc 52.22
2023-10-17 20:46:17,952 [podnet.py] => Task 10, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.90, Test_acc 51.77
2023-10-17 20:46:19,695 [podnet.py] => Task 10, Epoch 14/20 (LR 0.00103) => LSC_loss 0.11, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 98.95, Test_acc 52.17
2023-10-17 20:46:21,527 [podnet.py] => Task 10, Epoch 15/20 (LR 0.00073) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.85, Test_acc 52.07
2023-10-17 20:46:23,351 [podnet.py] => Task 10, Epoch 16/20 (LR 0.00048) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.60, Test_acc 52.16
2023-10-17 20:46:25,186 [podnet.py] => Task 10, Epoch 17/20 (LR 0.00027) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.80, Test_acc 52.16
2023-10-17 20:46:27,025 [podnet.py] => Task 10, Epoch 18/20 (LR 0.00012) => LSC_loss 0.13, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.20, Test_acc 52.06
2023-10-17 20:46:28,816 [podnet.py] => Task 10, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 1.31, Flat_loss 0.05, Train_acc 98.45, Test_acc 52.15
2023-10-17 20:46:30,589 [podnet.py] => Task 10, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.55, Test_acc 52.11
2023-10-17 20:46:30,592 [base.py] => Constructing exemplars for new classes...(20 per classes)
2023-10-17 20:47:13,010 [podnet.py] => Exemplar size: 2000
2023-10-17 20:47:13,010 [trainer.py] => CNN: {'total': 52.11, '00-09': 59.3, '10-19': 43.9, '20-29': 60.6, '30-39': 49.7, '40-49': 59.5, '50-59': 38.6, '60-69': 47.6, '70-79': 46.8, '80-89': 56.7, '90-99': 58.4, 'old': 51.58, 'new': 62.2}
2023-10-17 20:47:13,011 [trainer.py] => NME: {'total': 52.55, '00-09': 66.0, '10-19': 48.8, '20-29': 63.6, '30-39': 53.6, '40-49': 61.0, '50-59': 35.6, '60-69': 45.3, '70-79': 44.6, '80-89': 53.5, '90-99': 53.5, 'old': 52.35, 'new': 56.4}
2023-10-17 20:47:13,011 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7, 53.21, 52.11]
2023-10-17 20:47:13,011 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5, 80.15, 79.14]
2023-10-17 20:47:13,011 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1, 53.65, 52.55]
2023-10-17 20:47:13,011 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73, 81.57, 80.19]

2023-10-17 20:47:13,011 [trainer.py] => Average Accuracy (CNN): 62.33545454545455
2023-10-17 20:47:13,011 [trainer.py] => Average Accuracy (NME): 62.66090909090909
2025-02-14 20:56:11,990 [trainer.py] => config: ./exps/podnet.json
2025-02-14 20:56:11,991 [trainer.py] => prefix: reproduce
2025-02-14 20:56:11,992 [trainer.py] => dataset: cifar100
2025-02-14 20:56:11,992 [trainer.py] => memory_size: 2000
2025-02-14 20:56:11,993 [trainer.py] => memory_per_class: 20
2025-02-14 20:56:11,993 [trainer.py] => fixed_memory: True
2025-02-14 20:56:11,993 [trainer.py] => shuffle: True
2025-02-14 20:56:11,994 [trainer.py] => init_cls: 50
2025-02-14 20:56:11,994 [trainer.py] => increment: 5
2025-02-14 20:56:11,994 [trainer.py] => model_name: podnet
2025-02-14 20:56:11,995 [trainer.py] => convnet_type: cosine_resnet32
2025-02-14 20:56:11,995 [trainer.py] => device: [device(type='cuda', index=0)]
2025-02-14 20:56:11,996 [trainer.py] => seed: 1993
2025-02-14 20:56:14,023 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-02-14 20:56:19,224 [trainer.py] => All params: 466256
2025-02-14 20:56:19,225 [trainer.py] => Trainable params: 466256
2025-02-14 20:56:19,225 [podnet.py] => Learning on 0-50
2025-02-14 20:56:19,286 [podnet.py] => Adaptive factor: 0
2025-02-14 20:56:42,331 [podnet.py] => Task 0, Epoch 1/160 (LR 0.09999) => LSC_loss 3.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 5.80, Test_acc 5.94
2025-02-14 20:56:47,553 [podnet.py] => Task 0, Epoch 2/160 (LR 0.09996) => LSC_loss 3.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 14.38, Test_acc 16.40
2025-02-14 20:56:52,734 [podnet.py] => Task 0, Epoch 3/160 (LR 0.09991) => LSC_loss 2.95, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 21.41, Test_acc 24.70
2025-02-14 20:56:57,834 [podnet.py] => Task 0, Epoch 4/160 (LR 0.09985) => LSC_loss 2.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 27.63, Test_acc 24.78
2025-02-14 20:57:03,010 [podnet.py] => Task 0, Epoch 5/160 (LR 0.09976) => LSC_loss 2.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 33.52, Test_acc 26.42
2025-02-14 20:57:08,111 [podnet.py] => Task 0, Epoch 6/160 (LR 0.09965) => LSC_loss 2.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 39.37, Test_acc 40.18
2025-02-14 20:57:13,208 [podnet.py] => Task 0, Epoch 7/160 (LR 0.09953) => LSC_loss 2.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 44.68, Test_acc 40.06
2025-02-14 20:57:18,365 [podnet.py] => Task 0, Epoch 8/160 (LR 0.09938) => LSC_loss 1.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 47.98, Test_acc 45.92
2025-02-14 20:57:23,517 [podnet.py] => Task 0, Epoch 9/160 (LR 0.09922) => LSC_loss 1.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 51.09, Test_acc 43.40
2025-02-14 20:57:28,610 [podnet.py] => Task 0, Epoch 10/160 (LR 0.09904) => LSC_loss 1.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 53.87, Test_acc 50.50
2025-02-14 20:57:33,769 [podnet.py] => Task 0, Epoch 11/160 (LR 0.09884) => LSC_loss 1.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 55.47, Test_acc 50.70
2025-02-14 20:57:38,912 [podnet.py] => Task 0, Epoch 12/160 (LR 0.09862) => LSC_loss 1.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 57.16, Test_acc 48.34
2025-02-14 20:57:44,076 [podnet.py] => Task 0, Epoch 13/160 (LR 0.09838) => LSC_loss 1.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 58.43, Test_acc 52.92
2025-02-14 20:57:49,238 [podnet.py] => Task 0, Epoch 14/160 (LR 0.09812) => LSC_loss 1.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 59.72, Test_acc 54.48
2025-02-14 20:57:54,426 [podnet.py] => Task 0, Epoch 15/160 (LR 0.09785) => LSC_loss 1.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.96, Test_acc 51.06
2025-02-14 20:57:59,617 [podnet.py] => Task 0, Epoch 16/160 (LR 0.09755) => LSC_loss 1.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 61.79, Test_acc 53.44
2025-02-14 20:58:04,846 [podnet.py] => Task 0, Epoch 17/160 (LR 0.09724) => LSC_loss 1.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.04, Test_acc 54.56
2025-02-14 20:58:10,003 [podnet.py] => Task 0, Epoch 18/160 (LR 0.09691) => LSC_loss 1.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.90, Test_acc 55.50
2025-02-14 20:58:15,182 [podnet.py] => Task 0, Epoch 19/160 (LR 0.09656) => LSC_loss 1.29, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 64.37, Test_acc 55.64
2025-02-14 20:58:20,326 [podnet.py] => Task 0, Epoch 20/160 (LR 0.09619) => LSC_loss 1.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.22, Test_acc 55.50
2025-02-14 20:58:25,499 [podnet.py] => Task 0, Epoch 21/160 (LR 0.09581) => LSC_loss 1.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.21, Test_acc 58.10
2025-02-14 20:58:30,670 [podnet.py] => Task 0, Epoch 22/160 (LR 0.09541) => LSC_loss 1.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.86, Test_acc 56.44
2025-02-14 20:58:35,886 [podnet.py] => Task 0, Epoch 23/160 (LR 0.09499) => LSC_loss 1.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 66.24, Test_acc 52.04
2025-02-14 20:58:41,074 [podnet.py] => Task 0, Epoch 24/160 (LR 0.09455) => LSC_loss 1.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.23, Test_acc 53.84
2025-02-14 20:58:46,239 [podnet.py] => Task 0, Epoch 25/160 (LR 0.09410) => LSC_loss 1.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.48, Test_acc 57.28
2025-02-14 20:58:51,424 [podnet.py] => Task 0, Epoch 26/160 (LR 0.09362) => LSC_loss 1.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.84, Test_acc 56.94
2025-02-14 20:58:56,655 [podnet.py] => Task 0, Epoch 27/160 (LR 0.09314) => LSC_loss 1.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.98, Test_acc 50.48
2025-02-14 20:59:01,804 [podnet.py] => Task 0, Epoch 28/160 (LR 0.09263) => LSC_loss 1.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 68.64, Test_acc 59.20
2025-02-14 20:59:06,941 [podnet.py] => Task 0, Epoch 29/160 (LR 0.09211) => LSC_loss 1.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.24, Test_acc 61.20
2025-02-14 20:59:12,160 [podnet.py] => Task 0, Epoch 30/160 (LR 0.09157) => LSC_loss 1.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.00, Test_acc 57.22
2025-02-14 20:59:17,251 [podnet.py] => Task 0, Epoch 31/160 (LR 0.09102) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.35, Test_acc 60.94
2025-02-14 20:59:22,460 [podnet.py] => Task 0, Epoch 32/160 (LR 0.09045) => LSC_loss 1.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.85, Test_acc 59.24
2025-02-14 20:59:27,674 [podnet.py] => Task 0, Epoch 33/160 (LR 0.08987) => LSC_loss 1.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.77, Test_acc 57.96
2025-02-14 20:59:32,801 [podnet.py] => Task 0, Epoch 34/160 (LR 0.08927) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.48, Test_acc 60.34
2025-02-14 20:59:37,931 [podnet.py] => Task 0, Epoch 35/160 (LR 0.08865) => LSC_loss 1.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.84, Test_acc 54.54
2025-02-14 20:59:43,062 [podnet.py] => Task 0, Epoch 36/160 (LR 0.08802) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.92, Test_acc 57.32
2025-02-14 20:59:48,233 [podnet.py] => Task 0, Epoch 37/160 (LR 0.08738) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.68, Test_acc 60.76
2025-02-14 20:59:53,421 [podnet.py] => Task 0, Epoch 38/160 (LR 0.08672) => LSC_loss 1.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.56, Test_acc 59.00
2025-02-14 20:59:58,619 [podnet.py] => Task 0, Epoch 39/160 (LR 0.08604) => LSC_loss 1.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.10, Test_acc 63.64
2025-02-14 21:00:03,777 [podnet.py] => Task 0, Epoch 40/160 (LR 0.08536) => LSC_loss 1.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.57, Test_acc 61.18
2025-02-14 21:00:08,988 [podnet.py] => Task 0, Epoch 41/160 (LR 0.08465) => LSC_loss 1.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.65, Test_acc 63.02
2025-02-14 21:00:14,155 [podnet.py] => Task 0, Epoch 42/160 (LR 0.08394) => LSC_loss 1.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.81, Test_acc 63.68
2025-02-14 21:00:19,373 [podnet.py] => Task 0, Epoch 43/160 (LR 0.08321) => LSC_loss 1.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.06, Test_acc 60.52
2025-02-14 21:00:24,531 [podnet.py] => Task 0, Epoch 44/160 (LR 0.08247) => LSC_loss 0.98, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.61, Test_acc 64.00
2025-02-14 21:00:29,729 [podnet.py] => Task 0, Epoch 45/160 (LR 0.08172) => LSC_loss 0.98, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.84, Test_acc 57.18
2025-02-14 21:00:34,870 [podnet.py] => Task 0, Epoch 46/160 (LR 0.08095) => LSC_loss 0.96, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.14, Test_acc 61.06
2025-02-14 21:00:39,996 [podnet.py] => Task 0, Epoch 47/160 (LR 0.08018) => LSC_loss 0.97, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.99, Test_acc 62.66
2025-02-14 21:00:45,191 [podnet.py] => Task 0, Epoch 48/160 (LR 0.07939) => LSC_loss 0.96, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.11, Test_acc 60.18
2025-02-14 21:00:50,338 [podnet.py] => Task 0, Epoch 49/160 (LR 0.07859) => LSC_loss 0.95, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.14, Test_acc 64.28
2025-02-14 21:00:55,565 [podnet.py] => Task 0, Epoch 50/160 (LR 0.07778) => LSC_loss 0.94, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.23, Test_acc 65.30
2025-02-14 21:01:00,709 [podnet.py] => Task 0, Epoch 51/160 (LR 0.07696) => LSC_loss 0.94, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.60, Test_acc 61.48
2025-02-14 21:01:05,866 [podnet.py] => Task 0, Epoch 52/160 (LR 0.07612) => LSC_loss 0.92, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.31, Test_acc 62.56
2025-02-14 21:01:11,078 [podnet.py] => Task 0, Epoch 53/160 (LR 0.07528) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.40, Test_acc 61.44
2025-02-14 21:01:16,266 [podnet.py] => Task 0, Epoch 54/160 (LR 0.07443) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.24, Test_acc 63.26
2025-02-14 21:01:21,482 [podnet.py] => Task 0, Epoch 55/160 (LR 0.07357) => LSC_loss 0.91, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.45, Test_acc 65.12
2025-02-14 21:01:26,667 [podnet.py] => Task 0, Epoch 56/160 (LR 0.07270) => LSC_loss 0.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.14, Test_acc 60.20
2025-02-14 21:01:31,821 [podnet.py] => Task 0, Epoch 57/160 (LR 0.07182) => LSC_loss 0.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.02, Test_acc 62.46
2025-02-14 21:01:36,975 [podnet.py] => Task 0, Epoch 58/160 (LR 0.07093) => LSC_loss 0.88, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.20, Test_acc 59.80
2025-02-14 21:01:42,167 [podnet.py] => Task 0, Epoch 59/160 (LR 0.07004) => LSC_loss 0.86, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.93, Test_acc 64.40
2025-02-14 21:01:47,379 [podnet.py] => Task 0, Epoch 60/160 (LR 0.06913) => LSC_loss 0.87, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.52, Test_acc 65.06
2025-02-14 21:01:52,641 [podnet.py] => Task 0, Epoch 61/160 (LR 0.06822) => LSC_loss 0.86, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.01, Test_acc 62.32
2025-02-14 21:01:57,870 [podnet.py] => Task 0, Epoch 62/160 (LR 0.06731) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.80, Test_acc 63.02
2025-02-14 21:02:03,054 [podnet.py] => Task 0, Epoch 63/160 (LR 0.06638) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.33, Test_acc 58.36
2025-02-14 21:02:08,245 [podnet.py] => Task 0, Epoch 64/160 (LR 0.06545) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.60, Test_acc 60.96
2025-02-14 21:02:13,468 [podnet.py] => Task 0, Epoch 65/160 (LR 0.06451) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.85, Test_acc 63.92
2025-02-14 21:02:18,686 [podnet.py] => Task 0, Epoch 66/160 (LR 0.06357) => LSC_loss 0.83, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.75, Test_acc 67.14
2025-02-14 21:02:23,825 [podnet.py] => Task 0, Epoch 67/160 (LR 0.06262) => LSC_loss 0.81, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.34, Test_acc 63.74
2025-02-14 21:02:29,010 [podnet.py] => Task 0, Epoch 68/160 (LR 0.06167) => LSC_loss 0.81, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.00, Test_acc 62.90
2025-02-14 21:02:34,154 [podnet.py] => Task 0, Epoch 69/160 (LR 0.06072) => LSC_loss 0.80, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.66, Test_acc 65.02
2025-02-14 21:02:39,327 [podnet.py] => Task 0, Epoch 70/160 (LR 0.05975) => LSC_loss 0.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.88, Test_acc 61.90
2025-02-14 21:02:44,478 [podnet.py] => Task 0, Epoch 71/160 (LR 0.05879) => LSC_loss 0.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.01, Test_acc 60.48
2025-02-14 21:02:49,638 [podnet.py] => Task 0, Epoch 72/160 (LR 0.05782) => LSC_loss 0.77, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.58, Test_acc 63.62
2025-02-14 21:02:54,774 [podnet.py] => Task 0, Epoch 73/160 (LR 0.05685) => LSC_loss 0.76, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.43, Test_acc 64.56
2025-02-14 21:02:59,956 [podnet.py] => Task 0, Epoch 74/160 (LR 0.05588) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.78, Test_acc 64.86
2025-02-14 21:03:05,130 [podnet.py] => Task 0, Epoch 75/160 (LR 0.05490) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.96, Test_acc 63.08
2025-02-14 21:03:10,388 [podnet.py] => Task 0, Epoch 76/160 (LR 0.05392) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.56, Test_acc 66.34
2025-02-14 21:03:15,577 [podnet.py] => Task 0, Epoch 77/160 (LR 0.05294) => LSC_loss 0.72, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.67, Test_acc 63.52
2025-02-14 21:03:20,808 [podnet.py] => Task 0, Epoch 78/160 (LR 0.05196) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.22, Test_acc 69.38
2025-02-14 21:03:25,976 [podnet.py] => Task 0, Epoch 79/160 (LR 0.05098) => LSC_loss 0.71, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.85, Test_acc 66.28
2025-02-14 21:03:31,209 [podnet.py] => Task 0, Epoch 80/160 (LR 0.05000) => LSC_loss 0.71, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.90, Test_acc 63.62
2025-02-14 21:03:36,355 [podnet.py] => Task 0, Epoch 81/160 (LR 0.04902) => LSC_loss 0.68, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.05, Test_acc 65.04
2025-02-14 21:03:41,530 [podnet.py] => Task 0, Epoch 82/160 (LR 0.04804) => LSC_loss 0.68, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.95, Test_acc 63.56
2025-02-14 21:03:46,755 [podnet.py] => Task 0, Epoch 83/160 (LR 0.04706) => LSC_loss 0.67, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.12, Test_acc 69.22
2025-02-14 21:03:51,899 [podnet.py] => Task 0, Epoch 84/160 (LR 0.04608) => LSC_loss 0.67, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.92, Test_acc 63.06
2025-02-14 21:03:57,048 [podnet.py] => Task 0, Epoch 85/160 (LR 0.04510) => LSC_loss 0.66, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.31, Test_acc 68.78
2025-02-14 21:04:02,252 [podnet.py] => Task 0, Epoch 86/160 (LR 0.04412) => LSC_loss 0.65, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.51, Test_acc 67.62
2025-02-14 21:04:07,400 [podnet.py] => Task 0, Epoch 87/160 (LR 0.04315) => LSC_loss 0.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.12, Test_acc 68.04
2025-02-14 21:04:12,518 [podnet.py] => Task 0, Epoch 88/160 (LR 0.04218) => LSC_loss 0.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.14, Test_acc 66.42
2025-02-14 21:04:17,699 [podnet.py] => Task 0, Epoch 89/160 (LR 0.04121) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.83, Test_acc 64.18
2025-02-14 21:04:22,852 [podnet.py] => Task 0, Epoch 90/160 (LR 0.04025) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.68, Test_acc 66.62
2025-02-14 21:04:28,057 [podnet.py] => Task 0, Epoch 91/160 (LR 0.03928) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.06, Test_acc 67.34
2025-02-14 21:04:33,237 [podnet.py] => Task 0, Epoch 92/160 (LR 0.03833) => LSC_loss 0.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.31, Test_acc 67.00
2025-02-14 21:04:38,396 [podnet.py] => Task 0, Epoch 93/160 (LR 0.03738) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.66, Test_acc 67.96
2025-02-14 21:04:43,540 [podnet.py] => Task 0, Epoch 94/160 (LR 0.03643) => LSC_loss 0.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.20, Test_acc 61.16
2025-02-14 21:04:48,669 [podnet.py] => Task 0, Epoch 95/160 (LR 0.03549) => LSC_loss 0.57, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.76, Test_acc 66.52
2025-02-14 21:04:53,886 [podnet.py] => Task 0, Epoch 96/160 (LR 0.03455) => LSC_loss 0.54, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.78, Test_acc 71.20
2025-02-14 21:04:59,059 [podnet.py] => Task 0, Epoch 97/160 (LR 0.03362) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.04, Test_acc 67.96
2025-02-14 21:05:04,174 [podnet.py] => Task 0, Epoch 98/160 (LR 0.03269) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.84, Test_acc 67.58
2025-02-14 21:05:09,346 [podnet.py] => Task 0, Epoch 99/160 (LR 0.03178) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.19, Test_acc 66.90
2025-02-14 21:05:14,553 [podnet.py] => Task 0, Epoch 100/160 (LR 0.03087) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.75, Test_acc 70.46
2025-02-14 21:05:19,707 [podnet.py] => Task 0, Epoch 101/160 (LR 0.02996) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.04, Test_acc 68.36
2025-02-14 21:05:24,957 [podnet.py] => Task 0, Epoch 102/160 (LR 0.02907) => LSC_loss 0.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.57, Test_acc 66.80
2025-02-14 21:05:30,097 [podnet.py] => Task 0, Epoch 103/160 (LR 0.02818) => LSC_loss 0.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.50, Test_acc 70.78
2025-02-14 21:05:35,274 [podnet.py] => Task 0, Epoch 104/160 (LR 0.02730) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.70, Test_acc 68.20
2025-02-14 21:05:40,446 [podnet.py] => Task 0, Epoch 105/160 (LR 0.02643) => LSC_loss 0.45, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.21, Test_acc 70.92
2025-02-14 21:05:45,594 [podnet.py] => Task 0, Epoch 106/160 (LR 0.02557) => LSC_loss 0.43, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.83, Test_acc 68.58
2025-02-14 21:05:50,737 [podnet.py] => Task 0, Epoch 107/160 (LR 0.02472) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.31, Test_acc 71.20
2025-02-14 21:05:55,912 [podnet.py] => Task 0, Epoch 108/160 (LR 0.02388) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.45, Test_acc 71.44
2025-02-14 21:06:01,178 [podnet.py] => Task 0, Epoch 109/160 (LR 0.02304) => LSC_loss 0.39, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.88, Test_acc 68.30
2025-02-14 21:06:06,389 [podnet.py] => Task 0, Epoch 110/160 (LR 0.02222) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.38, Test_acc 71.32
2025-02-14 21:06:11,556 [podnet.py] => Task 0, Epoch 111/160 (LR 0.02141) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.64, Test_acc 72.72
2025-02-14 21:06:16,714 [podnet.py] => Task 0, Epoch 112/160 (LR 0.02061) => LSC_loss 0.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.97, Test_acc 72.24
2025-02-14 21:06:21,898 [podnet.py] => Task 0, Epoch 113/160 (LR 0.01982) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.01, Test_acc 70.94
2025-02-14 21:06:27,038 [podnet.py] => Task 0, Epoch 114/160 (LR 0.01905) => LSC_loss 0.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.27, Test_acc 72.68
2025-02-14 21:06:32,250 [podnet.py] => Task 0, Epoch 115/160 (LR 0.01828) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.02, Test_acc 72.28
2025-02-14 21:06:37,392 [podnet.py] => Task 0, Epoch 116/160 (LR 0.01753) => LSC_loss 0.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.63, Test_acc 70.98
2025-02-14 21:06:42,559 [podnet.py] => Task 0, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.96, Test_acc 71.24
2025-02-14 21:06:47,774 [podnet.py] => Task 0, Epoch 118/160 (LR 0.01606) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.69, Test_acc 71.94
2025-02-14 21:06:52,956 [podnet.py] => Task 0, Epoch 119/160 (LR 0.01535) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.83, Test_acc 72.42
2025-02-14 21:06:58,068 [podnet.py] => Task 0, Epoch 120/160 (LR 0.01464) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.40, Test_acc 74.48
2025-02-14 21:07:03,277 [podnet.py] => Task 0, Epoch 121/160 (LR 0.01396) => LSC_loss 0.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.76, Test_acc 74.18
2025-02-14 21:07:08,409 [podnet.py] => Task 0, Epoch 122/160 (LR 0.01328) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.42, Test_acc 73.64
2025-02-14 21:07:13,575 [podnet.py] => Task 0, Epoch 123/160 (LR 0.01262) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.74, Test_acc 72.58
2025-02-14 21:07:18,768 [podnet.py] => Task 0, Epoch 124/160 (LR 0.01198) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.69, Test_acc 75.54
2025-02-14 21:07:23,945 [podnet.py] => Task 0, Epoch 125/160 (LR 0.01135) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.36, Test_acc 75.46
2025-02-14 21:07:29,093 [podnet.py] => Task 0, Epoch 126/160 (LR 0.01073) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.63, Test_acc 74.86
2025-02-14 21:07:34,229 [podnet.py] => Task 0, Epoch 127/160 (LR 0.01013) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.15, Test_acc 74.60
2025-02-14 21:07:39,382 [podnet.py] => Task 0, Epoch 128/160 (LR 0.00955) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.58, Test_acc 74.94
2025-02-14 21:07:44,536 [podnet.py] => Task 0, Epoch 129/160 (LR 0.00898) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.92, Test_acc 75.24
2025-02-14 21:07:49,714 [podnet.py] => Task 0, Epoch 130/160 (LR 0.00843) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.27, Test_acc 76.26
2025-02-14 21:07:54,884 [podnet.py] => Task 0, Epoch 131/160 (LR 0.00789) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.26, Test_acc 75.82
2025-02-14 21:08:00,028 [podnet.py] => Task 0, Epoch 132/160 (LR 0.00737) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.66, Test_acc 75.96
2025-02-14 21:08:05,281 [podnet.py] => Task 0, Epoch 133/160 (LR 0.00686) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.89, Test_acc 76.62
2025-02-14 21:08:10,424 [podnet.py] => Task 0, Epoch 134/160 (LR 0.00638) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 76.36
2025-02-14 21:08:15,613 [podnet.py] => Task 0, Epoch 135/160 (LR 0.00590) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.21, Test_acc 77.00
2025-02-14 21:08:20,796 [podnet.py] => Task 0, Epoch 136/160 (LR 0.00545) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.40, Test_acc 76.66
2025-02-14 21:08:25,901 [podnet.py] => Task 0, Epoch 137/160 (LR 0.00501) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.69, Test_acc 77.08
2025-02-14 21:08:31,056 [podnet.py] => Task 0, Epoch 138/160 (LR 0.00459) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.68, Test_acc 77.14
2025-02-14 21:08:36,188 [podnet.py] => Task 0, Epoch 139/160 (LR 0.00419) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 77.02
2025-02-14 21:08:41,371 [podnet.py] => Task 0, Epoch 140/160 (LR 0.00381) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.07, Test_acc 77.10
2025-02-14 21:08:46,476 [podnet.py] => Task 0, Epoch 141/160 (LR 0.00344) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 77.54
2025-02-14 21:08:51,619 [podnet.py] => Task 0, Epoch 142/160 (LR 0.00309) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 77.72
2025-02-14 21:08:56,823 [podnet.py] => Task 0, Epoch 143/160 (LR 0.00276) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 77.38
2025-02-14 21:09:01,989 [podnet.py] => Task 0, Epoch 144/160 (LR 0.00245) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.29, Test_acc 77.46
2025-02-14 21:09:07,201 [podnet.py] => Task 0, Epoch 145/160 (LR 0.00215) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.41, Test_acc 77.58
2025-02-14 21:09:12,367 [podnet.py] => Task 0, Epoch 146/160 (LR 0.00188) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.50, Test_acc 77.48
2025-02-14 21:09:17,502 [podnet.py] => Task 0, Epoch 147/160 (LR 0.00162) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.45, Test_acc 77.84
2025-02-14 21:09:22,673 [podnet.py] => Task 0, Epoch 148/160 (LR 0.00138) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.49, Test_acc 78.02
2025-02-14 21:09:27,799 [podnet.py] => Task 0, Epoch 149/160 (LR 0.00116) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.61, Test_acc 78.12
2025-02-14 21:09:33,021 [podnet.py] => Task 0, Epoch 150/160 (LR 0.00096) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.88
2025-02-14 21:09:38,203 [podnet.py] => Task 0, Epoch 151/160 (LR 0.00078) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 77.54
2025-02-14 21:09:43,355 [podnet.py] => Task 0, Epoch 152/160 (LR 0.00062) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 77.64
2025-02-14 21:09:48,579 [podnet.py] => Task 0, Epoch 153/160 (LR 0.00047) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 77.88
2025-02-14 21:09:53,774 [podnet.py] => Task 0, Epoch 154/160 (LR 0.00035) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.88
2025-02-14 21:09:58,949 [podnet.py] => Task 0, Epoch 155/160 (LR 0.00024) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 77.92
2025-02-14 21:10:04,113 [podnet.py] => Task 0, Epoch 156/160 (LR 0.00015) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 77.86
2025-02-14 21:10:09,250 [podnet.py] => Task 0, Epoch 157/160 (LR 0.00009) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.69, Test_acc 77.78
2025-02-14 21:10:14,430 [podnet.py] => Task 0, Epoch 158/160 (LR 0.00004) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 77.90
2025-02-14 21:10:19,687 [podnet.py] => Task 0, Epoch 159/160 (LR 0.00001) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.63, Test_acc 77.98
2025-02-14 21:10:24,842 [podnet.py] => Task 0, Epoch 160/160 (LR 0.00000) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.70, Test_acc 77.70
2025-02-14 21:10:24,843 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:11:00,822 [podnet.py] => Exemplar size: 1000
2025-02-14 21:11:00,823 [trainer.py] => CNN: {'total': 77.7, '00-09': 80.7, '10-19': 71.9, '20-29': 81.0, '30-39': 75.4, '40-49': 79.5, 'old': 0, 'new': 77.7}
2025-02-14 21:11:00,823 [trainer.py] => NME: {'total': 77.44, '00-09': 81.0, '10-19': 71.5, '20-29': 80.8, '30-39': 74.4, '40-49': 79.5, 'old': 0, 'new': 77.44}
2025-02-14 21:11:00,826 [trainer.py] => CNN top1 curve: [77.7]
2025-02-14 21:11:00,826 [trainer.py] => CNN top5 curve: [94.08]
2025-02-14 21:11:00,826 [trainer.py] => NME top1 curve: [77.44]
2025-02-14 21:11:00,826 [trainer.py] => NME top5 curve: [93.98]

2025-02-14 21:11:00,826 [trainer.py] => Average Accuracy (CNN): 77.7
2025-02-14 21:11:00,826 [trainer.py] => Average Accuracy (NME): 77.44
2025-02-14 21:11:00,827 [trainer.py] => All params: 498257
2025-02-14 21:11:00,827 [trainer.py] => Trainable params: 498257
2025-02-14 21:11:00,828 [podnet.py] => Learning on 50-55
2025-02-14 21:11:00,859 [podnet.py] => Adaptive factor: 3.3166247903554
2025-02-14 21:11:02,857 [podnet.py] => Task 1, Epoch 1/160 (LR 0.09999) => LSC_loss 1.86, Spatial_loss 6.80, Flat_loss 1.55, Train_acc 61.51, Test_acc 17.47
2025-02-14 21:11:04,633 [podnet.py] => Task 1, Epoch 2/160 (LR 0.09996) => LSC_loss 0.84, Spatial_loss 5.58, Flat_loss 1.22, Train_acc 78.40, Test_acc 42.35
2025-02-14 21:11:06,392 [podnet.py] => Task 1, Epoch 3/160 (LR 0.09991) => LSC_loss 0.60, Spatial_loss 4.92, Flat_loss 0.99, Train_acc 83.69, Test_acc 46.05
2025-02-14 21:11:08,159 [podnet.py] => Task 1, Epoch 4/160 (LR 0.09985) => LSC_loss 0.54, Spatial_loss 4.73, Flat_loss 0.90, Train_acc 85.46, Test_acc 47.93
2025-02-14 21:11:09,929 [podnet.py] => Task 1, Epoch 5/160 (LR 0.09976) => LSC_loss 0.46, Spatial_loss 4.48, Flat_loss 0.80, Train_acc 87.86, Test_acc 49.49
2025-02-14 21:11:11,679 [podnet.py] => Task 1, Epoch 6/160 (LR 0.09965) => LSC_loss 0.40, Spatial_loss 4.43, Flat_loss 0.76, Train_acc 88.86, Test_acc 52.69
2025-02-14 21:11:13,513 [podnet.py] => Task 1, Epoch 7/160 (LR 0.09953) => LSC_loss 0.39, Spatial_loss 4.23, Flat_loss 0.71, Train_acc 90.14, Test_acc 52.16
2025-02-14 21:11:15,289 [podnet.py] => Task 1, Epoch 8/160 (LR 0.09938) => LSC_loss 0.37, Spatial_loss 4.16, Flat_loss 0.71, Train_acc 89.77, Test_acc 49.11
2025-02-14 21:11:17,134 [podnet.py] => Task 1, Epoch 9/160 (LR 0.09922) => LSC_loss 0.34, Spatial_loss 4.12, Flat_loss 0.68, Train_acc 90.37, Test_acc 53.62
2025-02-14 21:11:18,917 [podnet.py] => Task 1, Epoch 10/160 (LR 0.09904) => LSC_loss 0.31, Spatial_loss 3.94, Flat_loss 0.62, Train_acc 92.03, Test_acc 58.73
2025-02-14 21:11:20,658 [podnet.py] => Task 1, Epoch 11/160 (LR 0.09884) => LSC_loss 0.29, Spatial_loss 3.83, Flat_loss 0.60, Train_acc 92.06, Test_acc 53.45
2025-02-14 21:11:22,458 [podnet.py] => Task 1, Epoch 12/160 (LR 0.09862) => LSC_loss 0.29, Spatial_loss 3.77, Flat_loss 0.59, Train_acc 92.69, Test_acc 54.09
2025-02-14 21:11:24,238 [podnet.py] => Task 1, Epoch 13/160 (LR 0.09838) => LSC_loss 0.27, Spatial_loss 3.73, Flat_loss 0.58, Train_acc 93.11, Test_acc 58.49
2025-02-14 21:11:26,021 [podnet.py] => Task 1, Epoch 14/160 (LR 0.09812) => LSC_loss 0.27, Spatial_loss 3.68, Flat_loss 0.57, Train_acc 92.94, Test_acc 61.35
2025-02-14 21:11:27,781 [podnet.py] => Task 1, Epoch 15/160 (LR 0.09785) => LSC_loss 0.25, Spatial_loss 3.59, Flat_loss 0.54, Train_acc 93.86, Test_acc 65.05
2025-02-14 21:11:29,594 [podnet.py] => Task 1, Epoch 16/160 (LR 0.09755) => LSC_loss 0.27, Spatial_loss 3.56, Flat_loss 0.54, Train_acc 93.26, Test_acc 61.27
2025-02-14 21:11:31,392 [podnet.py] => Task 1, Epoch 17/160 (LR 0.09724) => LSC_loss 0.26, Spatial_loss 3.82, Flat_loss 0.57, Train_acc 93.57, Test_acc 55.71
2025-02-14 21:11:33,120 [podnet.py] => Task 1, Epoch 18/160 (LR 0.09691) => LSC_loss 0.25, Spatial_loss 3.59, Flat_loss 0.54, Train_acc 93.34, Test_acc 59.35
2025-02-14 21:11:34,943 [podnet.py] => Task 1, Epoch 19/160 (LR 0.09656) => LSC_loss 0.24, Spatial_loss 3.47, Flat_loss 0.52, Train_acc 93.57, Test_acc 60.15
2025-02-14 21:11:36,685 [podnet.py] => Task 1, Epoch 20/160 (LR 0.09619) => LSC_loss 0.24, Spatial_loss 3.56, Flat_loss 0.54, Train_acc 94.06, Test_acc 60.20
2025-02-14 21:11:38,480 [podnet.py] => Task 1, Epoch 21/160 (LR 0.09581) => LSC_loss 0.23, Spatial_loss 3.50, Flat_loss 0.52, Train_acc 93.94, Test_acc 60.38
2025-02-14 21:11:40,234 [podnet.py] => Task 1, Epoch 22/160 (LR 0.09541) => LSC_loss 0.23, Spatial_loss 3.40, Flat_loss 0.51, Train_acc 93.77, Test_acc 61.33
2025-02-14 21:11:42,025 [podnet.py] => Task 1, Epoch 23/160 (LR 0.09499) => LSC_loss 0.22, Spatial_loss 3.50, Flat_loss 0.52, Train_acc 94.09, Test_acc 63.15
2025-02-14 21:11:43,761 [podnet.py] => Task 1, Epoch 24/160 (LR 0.09455) => LSC_loss 0.23, Spatial_loss 3.38, Flat_loss 0.50, Train_acc 94.43, Test_acc 62.29
2025-02-14 21:11:45,576 [podnet.py] => Task 1, Epoch 25/160 (LR 0.09410) => LSC_loss 0.22, Spatial_loss 3.33, Flat_loss 0.49, Train_acc 94.49, Test_acc 56.09
2025-02-14 21:11:47,430 [podnet.py] => Task 1, Epoch 26/160 (LR 0.09362) => LSC_loss 0.22, Spatial_loss 3.35, Flat_loss 0.51, Train_acc 94.49, Test_acc 63.18
2025-02-14 21:11:49,197 [podnet.py] => Task 1, Epoch 27/160 (LR 0.09314) => LSC_loss 0.21, Spatial_loss 3.24, Flat_loss 0.49, Train_acc 94.60, Test_acc 65.11
2025-02-14 21:11:50,973 [podnet.py] => Task 1, Epoch 28/160 (LR 0.09263) => LSC_loss 0.20, Spatial_loss 3.26, Flat_loss 0.46, Train_acc 95.49, Test_acc 63.05
2025-02-14 21:11:52,793 [podnet.py] => Task 1, Epoch 29/160 (LR 0.09211) => LSC_loss 0.22, Spatial_loss 3.27, Flat_loss 0.48, Train_acc 94.34, Test_acc 57.33
2025-02-14 21:11:54,555 [podnet.py] => Task 1, Epoch 30/160 (LR 0.09157) => LSC_loss 0.22, Spatial_loss 3.23, Flat_loss 0.47, Train_acc 94.51, Test_acc 63.05
2025-02-14 21:11:56,327 [podnet.py] => Task 1, Epoch 31/160 (LR 0.09102) => LSC_loss 0.22, Spatial_loss 3.26, Flat_loss 0.47, Train_acc 94.46, Test_acc 61.58
2025-02-14 21:11:58,080 [podnet.py] => Task 1, Epoch 32/160 (LR 0.09045) => LSC_loss 0.20, Spatial_loss 3.12, Flat_loss 0.46, Train_acc 95.46, Test_acc 62.11
2025-02-14 21:11:59,891 [podnet.py] => Task 1, Epoch 33/160 (LR 0.08987) => LSC_loss 0.21, Spatial_loss 3.17, Flat_loss 0.46, Train_acc 95.23, Test_acc 59.69
2025-02-14 21:12:01,677 [podnet.py] => Task 1, Epoch 34/160 (LR 0.08927) => LSC_loss 0.21, Spatial_loss 3.19, Flat_loss 0.46, Train_acc 94.89, Test_acc 65.04
2025-02-14 21:12:03,427 [podnet.py] => Task 1, Epoch 35/160 (LR 0.08865) => LSC_loss 0.20, Spatial_loss 3.13, Flat_loss 0.45, Train_acc 94.86, Test_acc 64.15
2025-02-14 21:12:05,229 [podnet.py] => Task 1, Epoch 36/160 (LR 0.08802) => LSC_loss 0.20, Spatial_loss 3.11, Flat_loss 0.44, Train_acc 95.23, Test_acc 61.02
2025-02-14 21:12:06,992 [podnet.py] => Task 1, Epoch 37/160 (LR 0.08738) => LSC_loss 0.21, Spatial_loss 3.16, Flat_loss 0.46, Train_acc 94.63, Test_acc 62.40
2025-02-14 21:12:08,767 [podnet.py] => Task 1, Epoch 38/160 (LR 0.08672) => LSC_loss 0.21, Spatial_loss 3.21, Flat_loss 0.46, Train_acc 95.00, Test_acc 62.76
2025-02-14 21:12:10,551 [podnet.py] => Task 1, Epoch 39/160 (LR 0.08604) => LSC_loss 0.21, Spatial_loss 3.09, Flat_loss 0.45, Train_acc 95.06, Test_acc 56.80
2025-02-14 21:12:12,349 [podnet.py] => Task 1, Epoch 40/160 (LR 0.08536) => LSC_loss 0.21, Spatial_loss 3.14, Flat_loss 0.44, Train_acc 95.26, Test_acc 60.18
2025-02-14 21:12:14,112 [podnet.py] => Task 1, Epoch 41/160 (LR 0.08465) => LSC_loss 0.20, Spatial_loss 3.08, Flat_loss 0.44, Train_acc 95.20, Test_acc 63.53
2025-02-14 21:12:15,948 [podnet.py] => Task 1, Epoch 42/160 (LR 0.08394) => LSC_loss 0.20, Spatial_loss 3.09, Flat_loss 0.43, Train_acc 95.09, Test_acc 66.18
2025-02-14 21:12:17,683 [podnet.py] => Task 1, Epoch 43/160 (LR 0.08321) => LSC_loss 0.20, Spatial_loss 3.03, Flat_loss 0.43, Train_acc 95.14, Test_acc 59.73
2025-02-14 21:12:19,512 [podnet.py] => Task 1, Epoch 44/160 (LR 0.08247) => LSC_loss 0.20, Spatial_loss 3.17, Flat_loss 0.45, Train_acc 95.06, Test_acc 57.98
2025-02-14 21:12:21,288 [podnet.py] => Task 1, Epoch 45/160 (LR 0.08172) => LSC_loss 0.18, Spatial_loss 3.03, Flat_loss 0.42, Train_acc 95.29, Test_acc 63.95
2025-02-14 21:12:23,080 [podnet.py] => Task 1, Epoch 46/160 (LR 0.08095) => LSC_loss 0.20, Spatial_loss 2.98, Flat_loss 0.41, Train_acc 95.31, Test_acc 59.64
2025-02-14 21:12:24,848 [podnet.py] => Task 1, Epoch 47/160 (LR 0.08018) => LSC_loss 0.18, Spatial_loss 2.95, Flat_loss 0.42, Train_acc 95.74, Test_acc 65.22
2025-02-14 21:12:26,635 [podnet.py] => Task 1, Epoch 48/160 (LR 0.07939) => LSC_loss 0.21, Spatial_loss 3.00, Flat_loss 0.42, Train_acc 94.97, Test_acc 62.13
2025-02-14 21:12:28,471 [podnet.py] => Task 1, Epoch 49/160 (LR 0.07859) => LSC_loss 0.22, Spatial_loss 3.17, Flat_loss 0.46, Train_acc 94.17, Test_acc 64.16
2025-02-14 21:12:30,231 [podnet.py] => Task 1, Epoch 50/160 (LR 0.07778) => LSC_loss 0.19, Spatial_loss 3.01, Flat_loss 0.43, Train_acc 95.40, Test_acc 58.31
2025-02-14 21:12:32,013 [podnet.py] => Task 1, Epoch 51/160 (LR 0.07696) => LSC_loss 0.19, Spatial_loss 2.97, Flat_loss 0.42, Train_acc 96.00, Test_acc 63.47
2025-02-14 21:12:33,831 [podnet.py] => Task 1, Epoch 52/160 (LR 0.07612) => LSC_loss 0.19, Spatial_loss 3.02, Flat_loss 0.42, Train_acc 95.54, Test_acc 61.11
2025-02-14 21:12:35,552 [podnet.py] => Task 1, Epoch 53/160 (LR 0.07528) => LSC_loss 0.19, Spatial_loss 2.94, Flat_loss 0.40, Train_acc 95.60, Test_acc 65.69
2025-02-14 21:12:37,341 [podnet.py] => Task 1, Epoch 54/160 (LR 0.07443) => LSC_loss 0.19, Spatial_loss 2.89, Flat_loss 0.40, Train_acc 95.60, Test_acc 64.47
2025-02-14 21:12:39,108 [podnet.py] => Task 1, Epoch 55/160 (LR 0.07357) => LSC_loss 0.20, Spatial_loss 2.98, Flat_loss 0.42, Train_acc 94.91, Test_acc 61.67
2025-02-14 21:12:40,886 [podnet.py] => Task 1, Epoch 56/160 (LR 0.07270) => LSC_loss 0.18, Spatial_loss 2.94, Flat_loss 0.40, Train_acc 95.49, Test_acc 66.91
2025-02-14 21:12:42,650 [podnet.py] => Task 1, Epoch 57/160 (LR 0.07182) => LSC_loss 0.18, Spatial_loss 2.77, Flat_loss 0.38, Train_acc 95.86, Test_acc 65.91
2025-02-14 21:12:44,446 [podnet.py] => Task 1, Epoch 58/160 (LR 0.07093) => LSC_loss 0.18, Spatial_loss 2.81, Flat_loss 0.39, Train_acc 95.80, Test_acc 58.53
2025-02-14 21:12:46,229 [podnet.py] => Task 1, Epoch 59/160 (LR 0.07004) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.38, Train_acc 95.91, Test_acc 64.53
2025-02-14 21:12:48,017 [podnet.py] => Task 1, Epoch 60/160 (LR 0.06913) => LSC_loss 0.19, Spatial_loss 2.87, Flat_loss 0.40, Train_acc 95.71, Test_acc 62.47
2025-02-14 21:12:49,806 [podnet.py] => Task 1, Epoch 61/160 (LR 0.06822) => LSC_loss 0.19, Spatial_loss 2.90, Flat_loss 0.40, Train_acc 95.46, Test_acc 66.58
2025-02-14 21:12:51,541 [podnet.py] => Task 1, Epoch 62/160 (LR 0.06731) => LSC_loss 0.19, Spatial_loss 2.83, Flat_loss 0.38, Train_acc 95.49, Test_acc 66.78
2025-02-14 21:12:53,302 [podnet.py] => Task 1, Epoch 63/160 (LR 0.06638) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.39, Train_acc 95.66, Test_acc 66.75
2025-02-14 21:12:55,148 [podnet.py] => Task 1, Epoch 64/160 (LR 0.06545) => LSC_loss 0.20, Spatial_loss 2.89, Flat_loss 0.39, Train_acc 95.60, Test_acc 65.05
2025-02-14 21:12:56,873 [podnet.py] => Task 1, Epoch 65/160 (LR 0.06451) => LSC_loss 0.18, Spatial_loss 2.85, Flat_loss 0.39, Train_acc 95.89, Test_acc 65.62
2025-02-14 21:12:58,699 [podnet.py] => Task 1, Epoch 66/160 (LR 0.06357) => LSC_loss 0.18, Spatial_loss 2.78, Flat_loss 0.38, Train_acc 95.46, Test_acc 66.93
2025-02-14 21:13:00,559 [podnet.py] => Task 1, Epoch 67/160 (LR 0.06262) => LSC_loss 0.17, Spatial_loss 2.73, Flat_loss 0.37, Train_acc 96.20, Test_acc 65.56
2025-02-14 21:13:02,348 [podnet.py] => Task 1, Epoch 68/160 (LR 0.06167) => LSC_loss 0.19, Spatial_loss 2.79, Flat_loss 0.37, Train_acc 96.17, Test_acc 65.00
2025-02-14 21:13:04,147 [podnet.py] => Task 1, Epoch 69/160 (LR 0.06072) => LSC_loss 0.18, Spatial_loss 2.74, Flat_loss 0.37, Train_acc 96.23, Test_acc 65.24
2025-02-14 21:13:05,909 [podnet.py] => Task 1, Epoch 70/160 (LR 0.05975) => LSC_loss 0.18, Spatial_loss 2.72, Flat_loss 0.37, Train_acc 96.17, Test_acc 68.40
2025-02-14 21:13:07,737 [podnet.py] => Task 1, Epoch 71/160 (LR 0.05879) => LSC_loss 0.19, Spatial_loss 2.65, Flat_loss 0.35, Train_acc 96.46, Test_acc 68.78
2025-02-14 21:13:09,546 [podnet.py] => Task 1, Epoch 72/160 (LR 0.05782) => LSC_loss 0.17, Spatial_loss 2.69, Flat_loss 0.37, Train_acc 96.00, Test_acc 65.47
2025-02-14 21:13:11,267 [podnet.py] => Task 1, Epoch 73/160 (LR 0.05685) => LSC_loss 0.18, Spatial_loss 2.73, Flat_loss 0.37, Train_acc 95.60, Test_acc 64.96
2025-02-14 21:13:13,066 [podnet.py] => Task 1, Epoch 74/160 (LR 0.05588) => LSC_loss 0.18, Spatial_loss 2.65, Flat_loss 0.37, Train_acc 95.57, Test_acc 66.89
2025-02-14 21:13:14,826 [podnet.py] => Task 1, Epoch 75/160 (LR 0.05490) => LSC_loss 0.19, Spatial_loss 2.76, Flat_loss 0.37, Train_acc 95.46, Test_acc 60.00
2025-02-14 21:13:16,637 [podnet.py] => Task 1, Epoch 76/160 (LR 0.05392) => LSC_loss 0.17, Spatial_loss 2.60, Flat_loss 0.36, Train_acc 96.20, Test_acc 67.13
2025-02-14 21:13:18,432 [podnet.py] => Task 1, Epoch 77/160 (LR 0.05294) => LSC_loss 0.17, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.49, Test_acc 65.84
2025-02-14 21:13:20,259 [podnet.py] => Task 1, Epoch 78/160 (LR 0.05196) => LSC_loss 0.18, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.17, Test_acc 66.58
2025-02-14 21:13:22,016 [podnet.py] => Task 1, Epoch 79/160 (LR 0.05098) => LSC_loss 0.18, Spatial_loss 2.71, Flat_loss 0.36, Train_acc 95.86, Test_acc 66.82
2025-02-14 21:13:23,766 [podnet.py] => Task 1, Epoch 80/160 (LR 0.05000) => LSC_loss 0.16, Spatial_loss 2.63, Flat_loss 0.35, Train_acc 96.09, Test_acc 66.18
2025-02-14 21:13:25,557 [podnet.py] => Task 1, Epoch 81/160 (LR 0.04902) => LSC_loss 0.18, Spatial_loss 2.60, Flat_loss 0.35, Train_acc 95.74, Test_acc 66.96
2025-02-14 21:13:27,354 [podnet.py] => Task 1, Epoch 82/160 (LR 0.04804) => LSC_loss 0.19, Spatial_loss 2.65, Flat_loss 0.35, Train_acc 95.74, Test_acc 65.27
2025-02-14 21:13:29,143 [podnet.py] => Task 1, Epoch 83/160 (LR 0.04706) => LSC_loss 0.18, Spatial_loss 2.53, Flat_loss 0.35, Train_acc 95.69, Test_acc 67.58
2025-02-14 21:13:30,892 [podnet.py] => Task 1, Epoch 84/160 (LR 0.04608) => LSC_loss 0.18, Spatial_loss 2.56, Flat_loss 0.34, Train_acc 95.83, Test_acc 68.95
2025-02-14 21:13:32,665 [podnet.py] => Task 1, Epoch 85/160 (LR 0.04510) => LSC_loss 0.18, Spatial_loss 2.55, Flat_loss 0.34, Train_acc 96.03, Test_acc 66.64
2025-02-14 21:13:34,445 [podnet.py] => Task 1, Epoch 86/160 (LR 0.04412) => LSC_loss 0.16, Spatial_loss 2.52, Flat_loss 0.33, Train_acc 96.23, Test_acc 68.58
2025-02-14 21:13:36,206 [podnet.py] => Task 1, Epoch 87/160 (LR 0.04315) => LSC_loss 0.17, Spatial_loss 2.48, Flat_loss 0.33, Train_acc 96.46, Test_acc 67.31
2025-02-14 21:13:37,986 [podnet.py] => Task 1, Epoch 88/160 (LR 0.04218) => LSC_loss 0.17, Spatial_loss 2.47, Flat_loss 0.34, Train_acc 96.17, Test_acc 68.25
2025-02-14 21:13:39,723 [podnet.py] => Task 1, Epoch 89/160 (LR 0.04121) => LSC_loss 0.18, Spatial_loss 2.43, Flat_loss 0.32, Train_acc 96.20, Test_acc 68.25
2025-02-14 21:13:41,475 [podnet.py] => Task 1, Epoch 90/160 (LR 0.04025) => LSC_loss 0.18, Spatial_loss 2.39, Flat_loss 0.33, Train_acc 95.63, Test_acc 66.42
2025-02-14 21:13:43,211 [podnet.py] => Task 1, Epoch 91/160 (LR 0.03928) => LSC_loss 0.17, Spatial_loss 2.41, Flat_loss 0.32, Train_acc 95.89, Test_acc 68.20
2025-02-14 21:13:44,935 [podnet.py] => Task 1, Epoch 92/160 (LR 0.03833) => LSC_loss 0.16, Spatial_loss 2.38, Flat_loss 0.31, Train_acc 96.63, Test_acc 64.82
2025-02-14 21:13:46,724 [podnet.py] => Task 1, Epoch 93/160 (LR 0.03738) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 96.57, Test_acc 68.95
2025-02-14 21:13:48,513 [podnet.py] => Task 1, Epoch 94/160 (LR 0.03643) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 96.31, Test_acc 67.56
2025-02-14 21:13:50,292 [podnet.py] => Task 1, Epoch 95/160 (LR 0.03549) => LSC_loss 0.17, Spatial_loss 2.36, Flat_loss 0.31, Train_acc 96.37, Test_acc 68.45
2025-02-14 21:13:52,055 [podnet.py] => Task 1, Epoch 96/160 (LR 0.03455) => LSC_loss 0.18, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 95.77, Test_acc 68.33
2025-02-14 21:13:53,906 [podnet.py] => Task 1, Epoch 97/160 (LR 0.03362) => LSC_loss 0.17, Spatial_loss 2.33, Flat_loss 0.31, Train_acc 95.94, Test_acc 68.38
2025-02-14 21:13:55,639 [podnet.py] => Task 1, Epoch 98/160 (LR 0.03269) => LSC_loss 0.17, Spatial_loss 2.37, Flat_loss 0.31, Train_acc 95.77, Test_acc 68.78
2025-02-14 21:13:57,450 [podnet.py] => Task 1, Epoch 99/160 (LR 0.03178) => LSC_loss 0.16, Spatial_loss 2.29, Flat_loss 0.30, Train_acc 96.60, Test_acc 69.60
2025-02-14 21:13:59,297 [podnet.py] => Task 1, Epoch 100/160 (LR 0.03087) => LSC_loss 0.16, Spatial_loss 2.29, Flat_loss 0.30, Train_acc 96.83, Test_acc 68.33
2025-02-14 21:14:01,031 [podnet.py] => Task 1, Epoch 101/160 (LR 0.02996) => LSC_loss 0.19, Spatial_loss 2.31, Flat_loss 0.30, Train_acc 95.83, Test_acc 67.80
2025-02-14 21:14:02,818 [podnet.py] => Task 1, Epoch 102/160 (LR 0.02907) => LSC_loss 0.17, Spatial_loss 2.28, Flat_loss 0.30, Train_acc 96.11, Test_acc 69.09
2025-02-14 21:14:04,618 [podnet.py] => Task 1, Epoch 103/160 (LR 0.02818) => LSC_loss 0.16, Spatial_loss 2.21, Flat_loss 0.29, Train_acc 96.57, Test_acc 68.84
2025-02-14 21:14:06,456 [podnet.py] => Task 1, Epoch 104/160 (LR 0.02730) => LSC_loss 0.18, Spatial_loss 2.24, Flat_loss 0.29, Train_acc 95.94, Test_acc 68.44
2025-02-14 21:14:08,243 [podnet.py] => Task 1, Epoch 105/160 (LR 0.02643) => LSC_loss 0.17, Spatial_loss 2.28, Flat_loss 0.29, Train_acc 96.23, Test_acc 68.24
2025-02-14 21:14:10,026 [podnet.py] => Task 1, Epoch 106/160 (LR 0.02557) => LSC_loss 0.17, Spatial_loss 2.22, Flat_loss 0.29, Train_acc 96.00, Test_acc 67.98
2025-02-14 21:14:11,785 [podnet.py] => Task 1, Epoch 107/160 (LR 0.02472) => LSC_loss 0.16, Spatial_loss 2.16, Flat_loss 0.28, Train_acc 96.63, Test_acc 70.18
2025-02-14 21:14:13,525 [podnet.py] => Task 1, Epoch 108/160 (LR 0.02388) => LSC_loss 0.18, Spatial_loss 2.14, Flat_loss 0.28, Train_acc 95.86, Test_acc 69.78
2025-02-14 21:14:15,342 [podnet.py] => Task 1, Epoch 109/160 (LR 0.02304) => LSC_loss 0.17, Spatial_loss 2.15, Flat_loss 0.28, Train_acc 96.77, Test_acc 68.73
2025-02-14 21:14:17,170 [podnet.py] => Task 1, Epoch 110/160 (LR 0.02222) => LSC_loss 0.17, Spatial_loss 2.20, Flat_loss 0.29, Train_acc 96.20, Test_acc 68.91
2025-02-14 21:14:18,919 [podnet.py] => Task 1, Epoch 111/160 (LR 0.02141) => LSC_loss 0.16, Spatial_loss 2.07, Flat_loss 0.27, Train_acc 96.49, Test_acc 69.44
2025-02-14 21:14:20,691 [podnet.py] => Task 1, Epoch 112/160 (LR 0.02061) => LSC_loss 0.17, Spatial_loss 2.11, Flat_loss 0.27, Train_acc 96.34, Test_acc 69.25
2025-02-14 21:14:22,408 [podnet.py] => Task 1, Epoch 113/160 (LR 0.01982) => LSC_loss 0.17, Spatial_loss 2.15, Flat_loss 0.28, Train_acc 96.40, Test_acc 69.95
2025-02-14 21:14:24,262 [podnet.py] => Task 1, Epoch 114/160 (LR 0.01905) => LSC_loss 0.16, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 97.06, Test_acc 71.00
2025-02-14 21:14:26,067 [podnet.py] => Task 1, Epoch 115/160 (LR 0.01828) => LSC_loss 0.17, Spatial_loss 2.08, Flat_loss 0.27, Train_acc 96.29, Test_acc 69.53
2025-02-14 21:14:27,821 [podnet.py] => Task 1, Epoch 116/160 (LR 0.01753) => LSC_loss 0.16, Spatial_loss 2.03, Flat_loss 0.27, Train_acc 96.51, Test_acc 70.00
2025-02-14 21:14:29,533 [podnet.py] => Task 1, Epoch 117/160 (LR 0.01679) => LSC_loss 0.16, Spatial_loss 2.04, Flat_loss 0.26, Train_acc 96.49, Test_acc 70.07
2025-02-14 21:14:31,281 [podnet.py] => Task 1, Epoch 118/160 (LR 0.01606) => LSC_loss 0.17, Spatial_loss 2.03, Flat_loss 0.26, Train_acc 96.03, Test_acc 70.05
2025-02-14 21:14:33,018 [podnet.py] => Task 1, Epoch 119/160 (LR 0.01535) => LSC_loss 0.17, Spatial_loss 2.03, Flat_loss 0.26, Train_acc 96.29, Test_acc 70.78
2025-02-14 21:14:34,869 [podnet.py] => Task 1, Epoch 120/160 (LR 0.01464) => LSC_loss 0.17, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 96.40, Test_acc 69.60
2025-02-14 21:14:36,690 [podnet.py] => Task 1, Epoch 121/160 (LR 0.01396) => LSC_loss 0.16, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 96.60, Test_acc 69.85
2025-02-14 21:14:38,480 [podnet.py] => Task 1, Epoch 122/160 (LR 0.01328) => LSC_loss 0.17, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 96.37, Test_acc 72.49
2025-02-14 21:14:40,277 [podnet.py] => Task 1, Epoch 123/160 (LR 0.01262) => LSC_loss 0.17, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 96.66, Test_acc 71.13
2025-02-14 21:14:42,098 [podnet.py] => Task 1, Epoch 124/160 (LR 0.01198) => LSC_loss 0.17, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 96.66, Test_acc 70.95
2025-02-14 21:14:43,872 [podnet.py] => Task 1, Epoch 125/160 (LR 0.01135) => LSC_loss 0.16, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 96.71, Test_acc 70.78
2025-02-14 21:14:45,680 [podnet.py] => Task 1, Epoch 126/160 (LR 0.01073) => LSC_loss 0.16, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 97.09, Test_acc 70.91
2025-02-14 21:14:47,443 [podnet.py] => Task 1, Epoch 127/160 (LR 0.01013) => LSC_loss 0.16, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 96.89, Test_acc 70.75
2025-02-14 21:14:49,233 [podnet.py] => Task 1, Epoch 128/160 (LR 0.00955) => LSC_loss 0.17, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 96.23, Test_acc 70.42
2025-02-14 21:14:50,967 [podnet.py] => Task 1, Epoch 129/160 (LR 0.00898) => LSC_loss 0.16, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 97.06, Test_acc 71.40
2025-02-14 21:14:52,714 [podnet.py] => Task 1, Epoch 130/160 (LR 0.00843) => LSC_loss 0.16, Spatial_loss 1.82, Flat_loss 0.24, Train_acc 96.40, Test_acc 70.45
2025-02-14 21:14:54,484 [podnet.py] => Task 1, Epoch 131/160 (LR 0.00789) => LSC_loss 0.16, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 97.03, Test_acc 71.05
2025-02-14 21:14:56,282 [podnet.py] => Task 1, Epoch 132/160 (LR 0.00737) => LSC_loss 0.17, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 96.60, Test_acc 70.98
2025-02-14 21:14:58,072 [podnet.py] => Task 1, Epoch 133/160 (LR 0.00686) => LSC_loss 0.16, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 97.06, Test_acc 70.89
2025-02-14 21:14:59,878 [podnet.py] => Task 1, Epoch 134/160 (LR 0.00638) => LSC_loss 0.16, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 97.17, Test_acc 71.22
2025-02-14 21:15:01,700 [podnet.py] => Task 1, Epoch 135/160 (LR 0.00590) => LSC_loss 0.18, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 96.49, Test_acc 70.87
2025-02-14 21:15:03,507 [podnet.py] => Task 1, Epoch 136/160 (LR 0.00545) => LSC_loss 0.17, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 96.63, Test_acc 70.45
2025-02-14 21:15:05,279 [podnet.py] => Task 1, Epoch 137/160 (LR 0.00501) => LSC_loss 0.16, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 96.66, Test_acc 71.27
2025-02-14 21:15:07,002 [podnet.py] => Task 1, Epoch 138/160 (LR 0.00459) => LSC_loss 0.17, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 96.66, Test_acc 72.07
2025-02-14 21:15:08,735 [podnet.py] => Task 1, Epoch 139/160 (LR 0.00419) => LSC_loss 0.16, Spatial_loss 1.72, Flat_loss 0.23, Train_acc 96.91, Test_acc 71.49
2025-02-14 21:15:10,567 [podnet.py] => Task 1, Epoch 140/160 (LR 0.00381) => LSC_loss 0.16, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 96.69, Test_acc 71.42
2025-02-14 21:15:12,357 [podnet.py] => Task 1, Epoch 141/160 (LR 0.00344) => LSC_loss 0.16, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 97.09, Test_acc 71.85
2025-02-14 21:15:14,109 [podnet.py] => Task 1, Epoch 142/160 (LR 0.00309) => LSC_loss 0.17, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 96.43, Test_acc 71.69
2025-02-14 21:15:15,884 [podnet.py] => Task 1, Epoch 143/160 (LR 0.00276) => LSC_loss 0.16, Spatial_loss 1.70, Flat_loss 0.22, Train_acc 96.91, Test_acc 71.36
2025-02-14 21:15:17,725 [podnet.py] => Task 1, Epoch 144/160 (LR 0.00245) => LSC_loss 0.15, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 97.23, Test_acc 71.62
2025-02-14 21:15:19,568 [podnet.py] => Task 1, Epoch 145/160 (LR 0.00215) => LSC_loss 0.17, Spatial_loss 1.71, Flat_loss 0.22, Train_acc 97.23, Test_acc 71.69
2025-02-14 21:15:21,342 [podnet.py] => Task 1, Epoch 146/160 (LR 0.00188) => LSC_loss 0.16, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 96.74, Test_acc 71.25
2025-02-14 21:15:23,152 [podnet.py] => Task 1, Epoch 147/160 (LR 0.00162) => LSC_loss 0.16, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 96.57, Test_acc 71.84
2025-02-14 21:15:24,887 [podnet.py] => Task 1, Epoch 148/160 (LR 0.00138) => LSC_loss 0.17, Spatial_loss 1.71, Flat_loss 0.23, Train_acc 96.60, Test_acc 71.73
2025-02-14 21:15:26,669 [podnet.py] => Task 1, Epoch 149/160 (LR 0.00116) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 96.71, Test_acc 71.69
2025-02-14 21:15:28,432 [podnet.py] => Task 1, Epoch 150/160 (LR 0.00096) => LSC_loss 0.15, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 97.54, Test_acc 71.75
2025-02-14 21:15:30,178 [podnet.py] => Task 1, Epoch 151/160 (LR 0.00078) => LSC_loss 0.16, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 97.14, Test_acc 71.80
2025-02-14 21:15:31,942 [podnet.py] => Task 1, Epoch 152/160 (LR 0.00062) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.22, Train_acc 97.37, Test_acc 71.73
2025-02-14 21:15:33,761 [podnet.py] => Task 1, Epoch 153/160 (LR 0.00047) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 96.91, Test_acc 71.76
2025-02-14 21:15:35,555 [podnet.py] => Task 1, Epoch 154/160 (LR 0.00035) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.22, Train_acc 96.43, Test_acc 71.75
2025-02-14 21:15:37,306 [podnet.py] => Task 1, Epoch 155/160 (LR 0.00024) => LSC_loss 0.15, Spatial_loss 1.58, Flat_loss 0.21, Train_acc 97.74, Test_acc 71.89
2025-02-14 21:15:39,042 [podnet.py] => Task 1, Epoch 156/160 (LR 0.00015) => LSC_loss 0.16, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 97.31, Test_acc 71.78
2025-02-14 21:15:40,813 [podnet.py] => Task 1, Epoch 157/160 (LR 0.00009) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 96.97, Test_acc 71.80
2025-02-14 21:15:42,601 [podnet.py] => Task 1, Epoch 158/160 (LR 0.00004) => LSC_loss 0.17, Spatial_loss 1.64, Flat_loss 0.22, Train_acc 96.49, Test_acc 71.69
2025-02-14 21:15:44,369 [podnet.py] => Task 1, Epoch 159/160 (LR 0.00001) => LSC_loss 0.15, Spatial_loss 1.56, Flat_loss 0.21, Train_acc 97.23, Test_acc 71.69
2025-02-14 21:15:46,154 [podnet.py] => Task 1, Epoch 160/160 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.59, Flat_loss 0.21, Train_acc 97.09, Test_acc 71.69
2025-02-14 21:15:46,155 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:15:46,155 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:16:05,075 [podnet.py] => The size of finetune dataset: 1100
2025-02-14 21:16:06,320 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.12, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 96.73, Test_acc 70.53
2025-02-14 21:16:07,579 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.06, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 99.00, Test_acc 71.00
2025-02-14 21:16:08,823 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.05, Spatial_loss 1.69, Flat_loss 0.13, Train_acc 99.27, Test_acc 72.22
2025-02-14 21:16:10,038 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.05, Spatial_loss 1.67, Flat_loss 0.12, Train_acc 99.09, Test_acc 72.53
2025-02-14 21:16:11,219 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.05, Spatial_loss 1.66, Flat_loss 0.12, Train_acc 98.73, Test_acc 73.04
2025-02-14 21:16:12,405 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.06, Spatial_loss 1.59, Flat_loss 0.10, Train_acc 98.82, Test_acc 73.16
2025-02-14 21:16:13,625 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.05, Spatial_loss 1.61, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.15
2025-02-14 21:16:14,876 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.04, Spatial_loss 1.61, Flat_loss 0.10, Train_acc 99.55, Test_acc 73.67
2025-02-14 21:16:16,082 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.04, Spatial_loss 1.59, Flat_loss 0.10, Train_acc 99.45, Test_acc 73.60
2025-02-14 21:16:17,288 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.05, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.27, Test_acc 73.44
2025-02-14 21:16:18,526 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.04, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 99.18, Test_acc 73.69
2025-02-14 21:16:19,749 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.05, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.73
2025-02-14 21:16:21,009 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.04, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 99.55, Test_acc 73.55
2025-02-14 21:16:22,293 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.05, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.09, Test_acc 73.65
2025-02-14 21:16:23,526 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.05, Spatial_loss 1.52, Flat_loss 0.10, Train_acc 99.00, Test_acc 73.75
2025-02-14 21:16:24,766 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.04, Spatial_loss 1.55, Flat_loss 0.10, Train_acc 99.27, Test_acc 73.95
2025-02-14 21:16:26,033 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.04, Spatial_loss 1.51, Flat_loss 0.09, Train_acc 99.45, Test_acc 73.65
2025-02-14 21:16:27,308 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.04, Spatial_loss 1.52, Flat_loss 0.09, Train_acc 99.45, Test_acc 73.51
2025-02-14 21:16:28,624 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.05, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 99.09, Test_acc 73.65
2025-02-14 21:16:29,864 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.04, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 99.18, Test_acc 73.69
2025-02-14 21:16:29,866 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:16:50,545 [podnet.py] => Exemplar size: 1100
2025-02-14 21:16:50,545 [trainer.py] => CNN: {'total': 73.69, '00-09': 77.2, '10-19': 69.3, '20-29': 78.2, '30-39': 72.5, '40-49': 73.6, '50-59': 69.0, 'old': 74.16, 'new': 69.0}
2025-02-14 21:16:50,545 [trainer.py] => NME: {'total': 73.36, '00-09': 77.9, '10-19': 69.1, '20-29': 78.3, '30-39': 72.4, '40-49': 75.6, '50-59': 60.4, 'old': 74.66, 'new': 60.4}
2025-02-14 21:16:50,545 [trainer.py] => CNN top1 curve: [77.7, 73.69]
2025-02-14 21:16:50,545 [trainer.py] => CNN top5 curve: [94.08, 93.33]
2025-02-14 21:16:50,545 [trainer.py] => NME top1 curve: [77.44, 73.36]
2025-02-14 21:16:50,545 [trainer.py] => NME top5 curve: [93.98, 93.18]

2025-02-14 21:16:50,545 [trainer.py] => Average Accuracy (CNN): 75.695
2025-02-14 21:16:50,545 [trainer.py] => Average Accuracy (NME): 75.4
2025-02-14 21:16:50,546 [trainer.py] => All params: 501457
2025-02-14 21:16:50,546 [trainer.py] => Trainable params: 501457
2025-02-14 21:16:50,547 [podnet.py] => Learning on 55-60
2025-02-14 21:16:50,576 [podnet.py] => Adaptive factor: 3.4641016151377544
2025-02-14 21:16:52,718 [podnet.py] => Task 2, Epoch 1/160 (LR 0.09999) => LSC_loss 2.11, Spatial_loss 4.21, Flat_loss 1.10, Train_acc 53.36, Test_acc 42.88
2025-02-14 21:16:54,536 [podnet.py] => Task 2, Epoch 2/160 (LR 0.09996) => LSC_loss 0.97, Spatial_loss 4.34, Flat_loss 0.92, Train_acc 69.81, Test_acc 52.40
2025-02-14 21:16:56,427 [podnet.py] => Task 2, Epoch 3/160 (LR 0.09991) => LSC_loss 0.81, Spatial_loss 4.03, Flat_loss 0.77, Train_acc 75.89, Test_acc 53.58
2025-02-14 21:16:58,201 [podnet.py] => Task 2, Epoch 4/160 (LR 0.09985) => LSC_loss 0.71, Spatial_loss 3.81, Flat_loss 0.72, Train_acc 78.58, Test_acc 49.85
2025-02-14 21:17:00,041 [podnet.py] => Task 2, Epoch 5/160 (LR 0.09976) => LSC_loss 0.65, Spatial_loss 3.77, Flat_loss 0.66, Train_acc 80.36, Test_acc 57.58
2025-02-14 21:17:01,845 [podnet.py] => Task 2, Epoch 6/160 (LR 0.09965) => LSC_loss 0.59, Spatial_loss 3.56, Flat_loss 0.61, Train_acc 82.53, Test_acc 57.38
2025-02-14 21:17:03,624 [podnet.py] => Task 2, Epoch 7/160 (LR 0.09953) => LSC_loss 0.58, Spatial_loss 3.56, Flat_loss 0.59, Train_acc 82.92, Test_acc 56.23
2025-02-14 21:17:05,407 [podnet.py] => Task 2, Epoch 8/160 (LR 0.09938) => LSC_loss 0.55, Spatial_loss 3.62, Flat_loss 0.60, Train_acc 83.94, Test_acc 56.52
2025-02-14 21:17:07,274 [podnet.py] => Task 2, Epoch 9/160 (LR 0.09922) => LSC_loss 0.51, Spatial_loss 3.49, Flat_loss 0.57, Train_acc 85.33, Test_acc 57.02
2025-02-14 21:17:09,124 [podnet.py] => Task 2, Epoch 10/160 (LR 0.09904) => LSC_loss 0.47, Spatial_loss 3.39, Flat_loss 0.55, Train_acc 85.89, Test_acc 55.67
2025-02-14 21:17:10,976 [podnet.py] => Task 2, Epoch 11/160 (LR 0.09884) => LSC_loss 0.48, Spatial_loss 3.29, Flat_loss 0.53, Train_acc 85.25, Test_acc 56.32
2025-02-14 21:17:12,850 [podnet.py] => Task 2, Epoch 12/160 (LR 0.09862) => LSC_loss 0.45, Spatial_loss 3.42, Flat_loss 0.54, Train_acc 86.89, Test_acc 58.58
2025-02-14 21:17:14,693 [podnet.py] => Task 2, Epoch 13/160 (LR 0.09838) => LSC_loss 0.47, Spatial_loss 3.50, Flat_loss 0.57, Train_acc 86.75, Test_acc 58.83
2025-02-14 21:17:16,536 [podnet.py] => Task 2, Epoch 14/160 (LR 0.09812) => LSC_loss 0.45, Spatial_loss 3.33, Flat_loss 0.52, Train_acc 86.61, Test_acc 58.82
2025-02-14 21:17:18,385 [podnet.py] => Task 2, Epoch 15/160 (LR 0.09785) => LSC_loss 0.43, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 87.69, Test_acc 59.30
2025-02-14 21:17:20,249 [podnet.py] => Task 2, Epoch 16/160 (LR 0.09755) => LSC_loss 0.46, Spatial_loss 3.26, Flat_loss 0.51, Train_acc 87.31, Test_acc 53.95
2025-02-14 21:17:22,077 [podnet.py] => Task 2, Epoch 17/160 (LR 0.09724) => LSC_loss 0.44, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 87.78, Test_acc 57.20
2025-02-14 21:17:23,913 [podnet.py] => Task 2, Epoch 18/160 (LR 0.09691) => LSC_loss 0.42, Spatial_loss 3.37, Flat_loss 0.54, Train_acc 88.33, Test_acc 60.33
2025-02-14 21:17:25,676 [podnet.py] => Task 2, Epoch 19/160 (LR 0.09656) => LSC_loss 0.39, Spatial_loss 3.30, Flat_loss 0.52, Train_acc 89.89, Test_acc 59.32
2025-02-14 21:17:27,483 [podnet.py] => Task 2, Epoch 20/160 (LR 0.09619) => LSC_loss 0.40, Spatial_loss 3.35, Flat_loss 0.52, Train_acc 89.03, Test_acc 61.22
2025-02-14 21:17:29,389 [podnet.py] => Task 2, Epoch 21/160 (LR 0.09581) => LSC_loss 0.43, Spatial_loss 3.29, Flat_loss 0.53, Train_acc 88.28, Test_acc 58.52
2025-02-14 21:17:31,266 [podnet.py] => Task 2, Epoch 22/160 (LR 0.09541) => LSC_loss 0.42, Spatial_loss 3.53, Flat_loss 0.59, Train_acc 87.75, Test_acc 62.92
2025-02-14 21:17:33,128 [podnet.py] => Task 2, Epoch 23/160 (LR 0.09499) => LSC_loss 0.37, Spatial_loss 3.25, Flat_loss 0.52, Train_acc 90.19, Test_acc 59.13
2025-02-14 21:17:34,956 [podnet.py] => Task 2, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.44, Flat_loss 0.55, Train_acc 89.81, Test_acc 54.97
2025-02-14 21:17:36,810 [podnet.py] => Task 2, Epoch 25/160 (LR 0.09410) => LSC_loss 0.36, Spatial_loss 3.22, Flat_loss 0.52, Train_acc 90.06, Test_acc 60.18
2025-02-14 21:17:38,621 [podnet.py] => Task 2, Epoch 26/160 (LR 0.09362) => LSC_loss 0.36, Spatial_loss 3.22, Flat_loss 0.50, Train_acc 90.44, Test_acc 62.38
2025-02-14 21:17:40,454 [podnet.py] => Task 2, Epoch 27/160 (LR 0.09314) => LSC_loss 0.36, Spatial_loss 3.25, Flat_loss 0.50, Train_acc 90.33, Test_acc 57.65
2025-02-14 21:17:42,322 [podnet.py] => Task 2, Epoch 28/160 (LR 0.09263) => LSC_loss 0.37, Spatial_loss 3.23, Flat_loss 0.49, Train_acc 90.31, Test_acc 60.55
2025-02-14 21:17:44,198 [podnet.py] => Task 2, Epoch 29/160 (LR 0.09211) => LSC_loss 0.35, Spatial_loss 3.24, Flat_loss 0.50, Train_acc 90.72, Test_acc 57.88
2025-02-14 21:17:46,026 [podnet.py] => Task 2, Epoch 30/160 (LR 0.09157) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.52, Train_acc 89.03, Test_acc 59.17
2025-02-14 21:17:47,867 [podnet.py] => Task 2, Epoch 31/160 (LR 0.09102) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.49, Train_acc 91.31, Test_acc 58.35
2025-02-14 21:17:49,720 [podnet.py] => Task 2, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 3.22, Flat_loss 0.51, Train_acc 89.86, Test_acc 58.68
2025-02-14 21:17:51,608 [podnet.py] => Task 2, Epoch 33/160 (LR 0.08987) => LSC_loss 0.35, Spatial_loss 3.20, Flat_loss 0.51, Train_acc 90.31, Test_acc 60.30
2025-02-14 21:17:53,449 [podnet.py] => Task 2, Epoch 34/160 (LR 0.08927) => LSC_loss 0.32, Spatial_loss 3.15, Flat_loss 0.49, Train_acc 91.31, Test_acc 58.85
2025-02-14 21:17:55,259 [podnet.py] => Task 2, Epoch 35/160 (LR 0.08865) => LSC_loss 0.31, Spatial_loss 3.00, Flat_loss 0.46, Train_acc 92.28, Test_acc 57.15
2025-02-14 21:17:57,058 [podnet.py] => Task 2, Epoch 36/160 (LR 0.08802) => LSC_loss 0.31, Spatial_loss 3.12, Flat_loss 0.47, Train_acc 91.64, Test_acc 60.50
2025-02-14 21:17:58,850 [podnet.py] => Task 2, Epoch 37/160 (LR 0.08738) => LSC_loss 0.33, Spatial_loss 3.10, Flat_loss 0.49, Train_acc 91.44, Test_acc 58.60
2025-02-14 21:18:00,643 [podnet.py] => Task 2, Epoch 38/160 (LR 0.08672) => LSC_loss 0.33, Spatial_loss 3.11, Flat_loss 0.50, Train_acc 90.67, Test_acc 59.35
2025-02-14 21:18:02,462 [podnet.py] => Task 2, Epoch 39/160 (LR 0.08604) => LSC_loss 0.34, Spatial_loss 3.12, Flat_loss 0.49, Train_acc 91.08, Test_acc 61.20
2025-02-14 21:18:04,308 [podnet.py] => Task 2, Epoch 40/160 (LR 0.08536) => LSC_loss 0.31, Spatial_loss 3.07, Flat_loss 0.48, Train_acc 91.58, Test_acc 62.13
2025-02-14 21:18:06,108 [podnet.py] => Task 2, Epoch 41/160 (LR 0.08465) => LSC_loss 0.31, Spatial_loss 3.08, Flat_loss 0.48, Train_acc 91.89, Test_acc 59.70
2025-02-14 21:18:07,932 [podnet.py] => Task 2, Epoch 42/160 (LR 0.08394) => LSC_loss 0.29, Spatial_loss 2.96, Flat_loss 0.45, Train_acc 92.17, Test_acc 62.53
2025-02-14 21:18:09,729 [podnet.py] => Task 2, Epoch 43/160 (LR 0.08321) => LSC_loss 0.30, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 92.31, Test_acc 61.23
2025-02-14 21:18:11,536 [podnet.py] => Task 2, Epoch 44/160 (LR 0.08247) => LSC_loss 0.29, Spatial_loss 3.03, Flat_loss 0.48, Train_acc 91.72, Test_acc 62.83
2025-02-14 21:18:13,365 [podnet.py] => Task 2, Epoch 45/160 (LR 0.08172) => LSC_loss 0.31, Spatial_loss 3.06, Flat_loss 0.48, Train_acc 91.78, Test_acc 58.85
2025-02-14 21:18:15,207 [podnet.py] => Task 2, Epoch 46/160 (LR 0.08095) => LSC_loss 0.33, Spatial_loss 2.98, Flat_loss 0.47, Train_acc 91.47, Test_acc 62.05
2025-02-14 21:18:17,070 [podnet.py] => Task 2, Epoch 47/160 (LR 0.08018) => LSC_loss 0.31, Spatial_loss 3.00, Flat_loss 0.47, Train_acc 91.78, Test_acc 59.82
2025-02-14 21:18:18,873 [podnet.py] => Task 2, Epoch 48/160 (LR 0.07939) => LSC_loss 0.30, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 92.11, Test_acc 61.33
2025-02-14 21:18:20,672 [podnet.py] => Task 2, Epoch 49/160 (LR 0.07859) => LSC_loss 0.30, Spatial_loss 3.00, Flat_loss 0.46, Train_acc 91.78, Test_acc 60.13
2025-02-14 21:18:22,486 [podnet.py] => Task 2, Epoch 50/160 (LR 0.07778) => LSC_loss 0.31, Spatial_loss 3.04, Flat_loss 0.48, Train_acc 91.17, Test_acc 60.95
2025-02-14 21:18:24,376 [podnet.py] => Task 2, Epoch 51/160 (LR 0.07696) => LSC_loss 0.31, Spatial_loss 3.11, Flat_loss 0.47, Train_acc 91.78, Test_acc 56.88
2025-02-14 21:18:26,219 [podnet.py] => Task 2, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.07, Flat_loss 0.48, Train_acc 91.31, Test_acc 61.58
2025-02-14 21:18:28,056 [podnet.py] => Task 2, Epoch 53/160 (LR 0.07528) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.46, Train_acc 91.94, Test_acc 57.78
2025-02-14 21:18:29,853 [podnet.py] => Task 2, Epoch 54/160 (LR 0.07443) => LSC_loss 0.30, Spatial_loss 3.02, Flat_loss 0.48, Train_acc 92.03, Test_acc 61.52
2025-02-14 21:18:31,737 [podnet.py] => Task 2, Epoch 55/160 (LR 0.07357) => LSC_loss 0.31, Spatial_loss 2.98, Flat_loss 0.46, Train_acc 92.44, Test_acc 63.27
2025-02-14 21:18:33,594 [podnet.py] => Task 2, Epoch 56/160 (LR 0.07270) => LSC_loss 0.31, Spatial_loss 3.04, Flat_loss 0.48, Train_acc 91.69, Test_acc 59.82
2025-02-14 21:18:35,383 [podnet.py] => Task 2, Epoch 57/160 (LR 0.07182) => LSC_loss 0.29, Spatial_loss 2.99, Flat_loss 0.47, Train_acc 93.03, Test_acc 59.70
2025-02-14 21:18:37,238 [podnet.py] => Task 2, Epoch 58/160 (LR 0.07093) => LSC_loss 0.26, Spatial_loss 2.91, Flat_loss 0.45, Train_acc 93.58, Test_acc 62.47
2025-02-14 21:18:39,028 [podnet.py] => Task 2, Epoch 59/160 (LR 0.07004) => LSC_loss 0.27, Spatial_loss 2.75, Flat_loss 0.42, Train_acc 93.11, Test_acc 60.88
2025-02-14 21:18:40,881 [podnet.py] => Task 2, Epoch 60/160 (LR 0.06913) => LSC_loss 0.27, Spatial_loss 2.73, Flat_loss 0.42, Train_acc 93.14, Test_acc 63.57
2025-02-14 21:18:42,760 [podnet.py] => Task 2, Epoch 61/160 (LR 0.06822) => LSC_loss 0.26, Spatial_loss 2.75, Flat_loss 0.43, Train_acc 92.64, Test_acc 62.38
2025-02-14 21:18:44,589 [podnet.py] => Task 2, Epoch 62/160 (LR 0.06731) => LSC_loss 0.25, Spatial_loss 2.75, Flat_loss 0.40, Train_acc 94.31, Test_acc 62.93
2025-02-14 21:18:46,419 [podnet.py] => Task 2, Epoch 63/160 (LR 0.06638) => LSC_loss 0.27, Spatial_loss 2.76, Flat_loss 0.42, Train_acc 93.50, Test_acc 60.05
2025-02-14 21:18:48,303 [podnet.py] => Task 2, Epoch 64/160 (LR 0.06545) => LSC_loss 0.30, Spatial_loss 2.77, Flat_loss 0.43, Train_acc 92.81, Test_acc 61.08
2025-02-14 21:18:50,147 [podnet.py] => Task 2, Epoch 65/160 (LR 0.06451) => LSC_loss 0.31, Spatial_loss 2.93, Flat_loss 0.46, Train_acc 92.25, Test_acc 59.90
2025-02-14 21:18:51,984 [podnet.py] => Task 2, Epoch 66/160 (LR 0.06357) => LSC_loss 0.31, Spatial_loss 2.94, Flat_loss 0.46, Train_acc 91.64, Test_acc 63.70
2025-02-14 21:18:53,860 [podnet.py] => Task 2, Epoch 67/160 (LR 0.06262) => LSC_loss 0.25, Spatial_loss 2.68, Flat_loss 0.40, Train_acc 94.22, Test_acc 65.50
2025-02-14 21:18:55,700 [podnet.py] => Task 2, Epoch 68/160 (LR 0.06167) => LSC_loss 0.26, Spatial_loss 2.64, Flat_loss 0.40, Train_acc 93.78, Test_acc 64.17
2025-02-14 21:18:57,529 [podnet.py] => Task 2, Epoch 69/160 (LR 0.06072) => LSC_loss 0.25, Spatial_loss 2.66, Flat_loss 0.39, Train_acc 93.72, Test_acc 64.07
2025-02-14 21:18:59,363 [podnet.py] => Task 2, Epoch 70/160 (LR 0.05975) => LSC_loss 0.25, Spatial_loss 2.65, Flat_loss 0.39, Train_acc 93.86, Test_acc 62.13
2025-02-14 21:19:01,165 [podnet.py] => Task 2, Epoch 71/160 (LR 0.05879) => LSC_loss 0.28, Spatial_loss 2.79, Flat_loss 0.43, Train_acc 93.03, Test_acc 60.75
2025-02-14 21:19:02,960 [podnet.py] => Task 2, Epoch 72/160 (LR 0.05782) => LSC_loss 0.32, Spatial_loss 2.98, Flat_loss 0.48, Train_acc 91.50, Test_acc 61.95
2025-02-14 21:19:04,844 [podnet.py] => Task 2, Epoch 73/160 (LR 0.05685) => LSC_loss 0.34, Spatial_loss 2.95, Flat_loss 0.48, Train_acc 90.86, Test_acc 63.38
2025-02-14 21:19:06,734 [podnet.py] => Task 2, Epoch 74/160 (LR 0.05588) => LSC_loss 0.27, Spatial_loss 2.78, Flat_loss 0.43, Train_acc 93.83, Test_acc 61.37
2025-02-14 21:19:08,558 [podnet.py] => Task 2, Epoch 75/160 (LR 0.05490) => LSC_loss 0.27, Spatial_loss 2.79, Flat_loss 0.43, Train_acc 93.44, Test_acc 63.48
2025-02-14 21:19:10,438 [podnet.py] => Task 2, Epoch 76/160 (LR 0.05392) => LSC_loss 0.28, Spatial_loss 2.76, Flat_loss 0.42, Train_acc 92.75, Test_acc 62.10
2025-02-14 21:19:12,188 [podnet.py] => Task 2, Epoch 77/160 (LR 0.05294) => LSC_loss 0.26, Spatial_loss 2.68, Flat_loss 0.40, Train_acc 93.39, Test_acc 60.90
2025-02-14 21:19:14,006 [podnet.py] => Task 2, Epoch 78/160 (LR 0.05196) => LSC_loss 0.26, Spatial_loss 2.77, Flat_loss 0.41, Train_acc 93.78, Test_acc 65.68
2025-02-14 21:19:15,814 [podnet.py] => Task 2, Epoch 79/160 (LR 0.05098) => LSC_loss 0.24, Spatial_loss 2.66, Flat_loss 0.40, Train_acc 94.58, Test_acc 65.38
2025-02-14 21:19:17,613 [podnet.py] => Task 2, Epoch 80/160 (LR 0.05000) => LSC_loss 0.26, Spatial_loss 2.61, Flat_loss 0.39, Train_acc 93.47, Test_acc 64.40
2025-02-14 21:19:19,470 [podnet.py] => Task 2, Epoch 81/160 (LR 0.04902) => LSC_loss 0.25, Spatial_loss 2.63, Flat_loss 0.38, Train_acc 94.06, Test_acc 62.85
2025-02-14 21:19:21,333 [podnet.py] => Task 2, Epoch 82/160 (LR 0.04804) => LSC_loss 0.24, Spatial_loss 2.51, Flat_loss 0.38, Train_acc 94.53, Test_acc 65.28
2025-02-14 21:19:23,203 [podnet.py] => Task 2, Epoch 83/160 (LR 0.04706) => LSC_loss 0.25, Spatial_loss 2.45, Flat_loss 0.36, Train_acc 93.81, Test_acc 64.63
2025-02-14 21:19:25,030 [podnet.py] => Task 2, Epoch 84/160 (LR 0.04608) => LSC_loss 0.27, Spatial_loss 2.55, Flat_loss 0.38, Train_acc 94.14, Test_acc 63.05
2025-02-14 21:19:26,875 [podnet.py] => Task 2, Epoch 85/160 (LR 0.04510) => LSC_loss 0.26, Spatial_loss 2.52, Flat_loss 0.39, Train_acc 93.67, Test_acc 64.62
2025-02-14 21:19:28,707 [podnet.py] => Task 2, Epoch 86/160 (LR 0.04412) => LSC_loss 0.26, Spatial_loss 2.47, Flat_loss 0.37, Train_acc 94.44, Test_acc 60.10
2025-02-14 21:19:30,539 [podnet.py] => Task 2, Epoch 87/160 (LR 0.04315) => LSC_loss 0.26, Spatial_loss 2.48, Flat_loss 0.37, Train_acc 94.00, Test_acc 64.10
2025-02-14 21:19:32,363 [podnet.py] => Task 2, Epoch 88/160 (LR 0.04218) => LSC_loss 0.24, Spatial_loss 2.42, Flat_loss 0.36, Train_acc 94.17, Test_acc 65.40
2025-02-14 21:19:34,170 [podnet.py] => Task 2, Epoch 89/160 (LR 0.04121) => LSC_loss 0.24, Spatial_loss 2.50, Flat_loss 0.36, Train_acc 94.81, Test_acc 64.65
2025-02-14 21:19:35,980 [podnet.py] => Task 2, Epoch 90/160 (LR 0.04025) => LSC_loss 0.24, Spatial_loss 2.48, Flat_loss 0.37, Train_acc 94.53, Test_acc 63.92
2025-02-14 21:19:37,859 [podnet.py] => Task 2, Epoch 91/160 (LR 0.03928) => LSC_loss 0.25, Spatial_loss 2.43, Flat_loss 0.36, Train_acc 94.69, Test_acc 62.92
2025-02-14 21:19:39,679 [podnet.py] => Task 2, Epoch 92/160 (LR 0.03833) => LSC_loss 0.24, Spatial_loss 2.38, Flat_loss 0.37, Train_acc 95.06, Test_acc 66.25
2025-02-14 21:19:41,492 [podnet.py] => Task 2, Epoch 93/160 (LR 0.03738) => LSC_loss 0.23, Spatial_loss 2.35, Flat_loss 0.35, Train_acc 94.11, Test_acc 64.85
2025-02-14 21:19:43,341 [podnet.py] => Task 2, Epoch 94/160 (LR 0.03643) => LSC_loss 0.24, Spatial_loss 2.31, Flat_loss 0.34, Train_acc 94.64, Test_acc 66.17
2025-02-14 21:19:45,139 [podnet.py] => Task 2, Epoch 95/160 (LR 0.03549) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.34, Train_acc 94.72, Test_acc 65.05
2025-02-14 21:19:47,013 [podnet.py] => Task 2, Epoch 96/160 (LR 0.03455) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.34, Train_acc 94.44, Test_acc 65.78
2025-02-14 21:19:48,878 [podnet.py] => Task 2, Epoch 97/160 (LR 0.03362) => LSC_loss 0.23, Spatial_loss 2.30, Flat_loss 0.34, Train_acc 95.11, Test_acc 65.12
2025-02-14 21:19:50,684 [podnet.py] => Task 2, Epoch 98/160 (LR 0.03269) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.33, Train_acc 94.28, Test_acc 65.20
2025-02-14 21:19:52,565 [podnet.py] => Task 2, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.27, Flat_loss 0.34, Train_acc 94.08, Test_acc 64.63
2025-02-14 21:19:54,403 [podnet.py] => Task 2, Epoch 100/160 (LR 0.03087) => LSC_loss 0.26, Spatial_loss 2.29, Flat_loss 0.34, Train_acc 93.83, Test_acc 64.38
2025-02-14 21:19:56,160 [podnet.py] => Task 2, Epoch 101/160 (LR 0.02996) => LSC_loss 0.26, Spatial_loss 2.34, Flat_loss 0.35, Train_acc 94.36, Test_acc 66.07
2025-02-14 21:19:58,000 [podnet.py] => Task 2, Epoch 102/160 (LR 0.02907) => LSC_loss 0.23, Spatial_loss 2.24, Flat_loss 0.32, Train_acc 94.83, Test_acc 65.43
2025-02-14 21:19:59,844 [podnet.py] => Task 2, Epoch 103/160 (LR 0.02818) => LSC_loss 0.24, Spatial_loss 2.18, Flat_loss 0.31, Train_acc 94.50, Test_acc 64.65
2025-02-14 21:20:01,661 [podnet.py] => Task 2, Epoch 104/160 (LR 0.02730) => LSC_loss 0.25, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 94.94, Test_acc 66.50
2025-02-14 21:20:03,429 [podnet.py] => Task 2, Epoch 105/160 (LR 0.02643) => LSC_loss 0.26, Spatial_loss 2.25, Flat_loss 0.34, Train_acc 94.33, Test_acc 66.23
2025-02-14 21:20:05,293 [podnet.py] => Task 2, Epoch 106/160 (LR 0.02557) => LSC_loss 0.25, Spatial_loss 2.18, Flat_loss 0.32, Train_acc 94.17, Test_acc 67.00
2025-02-14 21:20:07,115 [podnet.py] => Task 2, Epoch 107/160 (LR 0.02472) => LSC_loss 0.24, Spatial_loss 2.22, Flat_loss 0.32, Train_acc 94.39, Test_acc 65.12
2025-02-14 21:20:08,943 [podnet.py] => Task 2, Epoch 108/160 (LR 0.02388) => LSC_loss 0.22, Spatial_loss 2.13, Flat_loss 0.31, Train_acc 95.36, Test_acc 66.50
2025-02-14 21:20:10,809 [podnet.py] => Task 2, Epoch 109/160 (LR 0.02304) => LSC_loss 0.25, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 94.89, Test_acc 65.25
2025-02-14 21:20:12,628 [podnet.py] => Task 2, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 2.20, Flat_loss 0.33, Train_acc 94.08, Test_acc 65.82
2025-02-14 21:20:14,474 [podnet.py] => Task 2, Epoch 111/160 (LR 0.02141) => LSC_loss 0.24, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 94.86, Test_acc 63.43
2025-02-14 21:20:16,263 [podnet.py] => Task 2, Epoch 112/160 (LR 0.02061) => LSC_loss 0.24, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 94.53, Test_acc 64.57
2025-02-14 21:20:18,073 [podnet.py] => Task 2, Epoch 113/160 (LR 0.01982) => LSC_loss 0.23, Spatial_loss 2.13, Flat_loss 0.31, Train_acc 95.61, Test_acc 65.53
2025-02-14 21:20:19,954 [podnet.py] => Task 2, Epoch 114/160 (LR 0.01905) => LSC_loss 0.22, Spatial_loss 2.06, Flat_loss 0.30, Train_acc 95.00, Test_acc 66.82
2025-02-14 21:20:21,766 [podnet.py] => Task 2, Epoch 115/160 (LR 0.01828) => LSC_loss 0.25, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 94.97, Test_acc 66.40
2025-02-14 21:20:23,618 [podnet.py] => Task 2, Epoch 116/160 (LR 0.01753) => LSC_loss 0.25, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 94.31, Test_acc 67.42
2025-02-14 21:20:25,506 [podnet.py] => Task 2, Epoch 117/160 (LR 0.01679) => LSC_loss 0.22, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 95.47, Test_acc 66.42
2025-02-14 21:20:27,333 [podnet.py] => Task 2, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 95.36, Test_acc 65.45
2025-02-14 21:20:29,138 [podnet.py] => Task 2, Epoch 119/160 (LR 0.01535) => LSC_loss 0.22, Spatial_loss 1.95, Flat_loss 0.30, Train_acc 95.19, Test_acc 67.25
2025-02-14 21:20:30,973 [podnet.py] => Task 2, Epoch 120/160 (LR 0.01464) => LSC_loss 0.23, Spatial_loss 1.99, Flat_loss 0.29, Train_acc 94.86, Test_acc 65.00
2025-02-14 21:20:32,778 [podnet.py] => Task 2, Epoch 121/160 (LR 0.01396) => LSC_loss 0.23, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 95.11, Test_acc 67.67
2025-02-14 21:20:34,595 [podnet.py] => Task 2, Epoch 122/160 (LR 0.01328) => LSC_loss 0.24, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 95.11, Test_acc 68.17
2025-02-14 21:20:36,425 [podnet.py] => Task 2, Epoch 123/160 (LR 0.01262) => LSC_loss 0.23, Spatial_loss 1.92, Flat_loss 0.28, Train_acc 95.86, Test_acc 66.40
2025-02-14 21:20:38,261 [podnet.py] => Task 2, Epoch 124/160 (LR 0.01198) => LSC_loss 0.23, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 95.36, Test_acc 67.42
2025-02-14 21:20:40,097 [podnet.py] => Task 2, Epoch 125/160 (LR 0.01135) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.28, Train_acc 94.67, Test_acc 66.25
2025-02-14 21:20:41,874 [podnet.py] => Task 2, Epoch 126/160 (LR 0.01073) => LSC_loss 0.24, Spatial_loss 1.88, Flat_loss 0.28, Train_acc 95.19, Test_acc 67.77
2025-02-14 21:20:43,715 [podnet.py] => Task 2, Epoch 127/160 (LR 0.01013) => LSC_loss 0.22, Spatial_loss 1.84, Flat_loss 0.27, Train_acc 95.36, Test_acc 68.20
2025-02-14 21:20:45,467 [podnet.py] => Task 2, Epoch 128/160 (LR 0.00955) => LSC_loss 0.24, Spatial_loss 1.84, Flat_loss 0.27, Train_acc 95.03, Test_acc 67.45
2025-02-14 21:20:47,305 [podnet.py] => Task 2, Epoch 129/160 (LR 0.00898) => LSC_loss 0.23, Spatial_loss 1.84, Flat_loss 0.28, Train_acc 95.17, Test_acc 67.32
2025-02-14 21:20:49,146 [podnet.py] => Task 2, Epoch 130/160 (LR 0.00843) => LSC_loss 0.22, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 96.14, Test_acc 67.17
2025-02-14 21:20:50,980 [podnet.py] => Task 2, Epoch 131/160 (LR 0.00789) => LSC_loss 0.23, Spatial_loss 1.75, Flat_loss 0.26, Train_acc 95.31, Test_acc 67.60
2025-02-14 21:20:52,804 [podnet.py] => Task 2, Epoch 132/160 (LR 0.00737) => LSC_loss 0.22, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 95.58, Test_acc 67.00
2025-02-14 21:20:54,622 [podnet.py] => Task 2, Epoch 133/160 (LR 0.00686) => LSC_loss 0.23, Spatial_loss 1.79, Flat_loss 0.27, Train_acc 95.28, Test_acc 66.83
2025-02-14 21:20:56,436 [podnet.py] => Task 2, Epoch 134/160 (LR 0.00638) => LSC_loss 0.23, Spatial_loss 1.76, Flat_loss 0.26, Train_acc 95.22, Test_acc 67.62
2025-02-14 21:20:58,264 [podnet.py] => Task 2, Epoch 135/160 (LR 0.00590) => LSC_loss 0.21, Spatial_loss 1.74, Flat_loss 0.26, Train_acc 96.03, Test_acc 67.15
2025-02-14 21:21:00,045 [podnet.py] => Task 2, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 94.83, Test_acc 67.82
2025-02-14 21:21:01,854 [podnet.py] => Task 2, Epoch 137/160 (LR 0.00501) => LSC_loss 0.22, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 96.03, Test_acc 67.72
2025-02-14 21:21:03,634 [podnet.py] => Task 2, Epoch 138/160 (LR 0.00459) => LSC_loss 0.22, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 95.69, Test_acc 66.97
2025-02-14 21:21:05,502 [podnet.py] => Task 2, Epoch 139/160 (LR 0.00419) => LSC_loss 0.22, Spatial_loss 1.72, Flat_loss 0.25, Train_acc 95.36, Test_acc 67.57
2025-02-14 21:21:07,390 [podnet.py] => Task 2, Epoch 140/160 (LR 0.00381) => LSC_loss 0.22, Spatial_loss 1.70, Flat_loss 0.26, Train_acc 95.97, Test_acc 68.05
2025-02-14 21:21:09,224 [podnet.py] => Task 2, Epoch 141/160 (LR 0.00344) => LSC_loss 0.21, Spatial_loss 1.66, Flat_loss 0.25, Train_acc 95.47, Test_acc 67.90
2025-02-14 21:21:11,105 [podnet.py] => Task 2, Epoch 142/160 (LR 0.00309) => LSC_loss 0.22, Spatial_loss 1.67, Flat_loss 0.25, Train_acc 95.81, Test_acc 68.23
2025-02-14 21:21:12,925 [podnet.py] => Task 2, Epoch 143/160 (LR 0.00276) => LSC_loss 0.21, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.67, Test_acc 67.65
2025-02-14 21:21:14,739 [podnet.py] => Task 2, Epoch 144/160 (LR 0.00245) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.25, Train_acc 96.03, Test_acc 68.18
2025-02-14 21:21:16,576 [podnet.py] => Task 2, Epoch 145/160 (LR 0.00215) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.24, Train_acc 95.78, Test_acc 68.05
2025-02-14 21:21:18,424 [podnet.py] => Task 2, Epoch 146/160 (LR 0.00188) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.14, Test_acc 67.82
2025-02-14 21:21:20,196 [podnet.py] => Task 2, Epoch 147/160 (LR 0.00162) => LSC_loss 0.22, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.39, Test_acc 68.17
2025-02-14 21:21:22,036 [podnet.py] => Task 2, Epoch 148/160 (LR 0.00138) => LSC_loss 0.21, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.92, Test_acc 68.02
2025-02-14 21:21:23,813 [podnet.py] => Task 2, Epoch 149/160 (LR 0.00116) => LSC_loss 0.24, Spatial_loss 1.68, Flat_loss 0.26, Train_acc 95.78, Test_acc 67.48
2025-02-14 21:21:25,669 [podnet.py] => Task 2, Epoch 150/160 (LR 0.00096) => LSC_loss 0.22, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 95.81, Test_acc 67.95
2025-02-14 21:21:27,465 [podnet.py] => Task 2, Epoch 151/160 (LR 0.00078) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.24, Train_acc 96.69, Test_acc 68.37
2025-02-14 21:21:29,335 [podnet.py] => Task 2, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 1.60, Flat_loss 0.24, Train_acc 96.06, Test_acc 68.00
2025-02-14 21:21:31,162 [podnet.py] => Task 2, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 95.39, Test_acc 68.07
2025-02-14 21:21:32,962 [podnet.py] => Task 2, Epoch 154/160 (LR 0.00035) => LSC_loss 0.22, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 95.92, Test_acc 67.30
2025-02-14 21:21:34,727 [podnet.py] => Task 2, Epoch 155/160 (LR 0.00024) => LSC_loss 0.22, Spatial_loss 1.57, Flat_loss 0.24, Train_acc 95.97, Test_acc 67.87
2025-02-14 21:21:36,547 [podnet.py] => Task 2, Epoch 156/160 (LR 0.00015) => LSC_loss 0.21, Spatial_loss 1.57, Flat_loss 0.24, Train_acc 96.47, Test_acc 67.72
2025-02-14 21:21:38,396 [podnet.py] => Task 2, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 95.92, Test_acc 68.13
2025-02-14 21:21:40,274 [podnet.py] => Task 2, Epoch 158/160 (LR 0.00004) => LSC_loss 0.21, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 96.03, Test_acc 67.62
2025-02-14 21:21:42,061 [podnet.py] => Task 2, Epoch 159/160 (LR 0.00001) => LSC_loss 0.21, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 96.53, Test_acc 67.73
2025-02-14 21:21:43,884 [podnet.py] => Task 2, Epoch 160/160 (LR 0.00000) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.24, Train_acc 95.64, Test_acc 67.68
2025-02-14 21:21:43,885 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:21:43,885 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:22:04,856 [podnet.py] => The size of finetune dataset: 1200
2025-02-14 21:22:06,157 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 95.33, Test_acc 67.03
2025-02-14 21:22:07,420 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.08, Spatial_loss 1.83, Flat_loss 0.18, Train_acc 98.83, Test_acc 67.88
2025-02-14 21:22:08,690 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.06, Spatial_loss 1.69, Flat_loss 0.13, Train_acc 98.92, Test_acc 68.13
2025-02-14 21:22:09,998 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.06, Spatial_loss 1.57, Flat_loss 0.11, Train_acc 99.33, Test_acc 68.63
2025-02-14 21:22:11,329 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.06, Spatial_loss 1.61, Flat_loss 0.11, Train_acc 99.08, Test_acc 69.08
2025-02-14 21:22:12,605 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.07, Spatial_loss 1.57, Flat_loss 0.10, Train_acc 98.75, Test_acc 69.58
2025-02-14 21:22:13,899 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.06, Spatial_loss 1.53, Flat_loss 0.10, Train_acc 98.92, Test_acc 69.20
2025-02-14 21:22:15,199 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.06, Spatial_loss 1.54, Flat_loss 0.11, Train_acc 99.50, Test_acc 69.43
2025-02-14 21:22:16,515 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.07, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 98.67, Test_acc 69.20
2025-02-14 21:22:17,785 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.05, Spatial_loss 1.54, Flat_loss 0.10, Train_acc 98.75, Test_acc 69.35
2025-02-14 21:22:19,088 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 98.83, Test_acc 69.22
2025-02-14 21:22:20,417 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.05, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 99.25, Test_acc 69.35
2025-02-14 21:22:21,650 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.09, Train_acc 99.17, Test_acc 69.93
2025-02-14 21:22:22,921 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.06, Spatial_loss 1.46, Flat_loss 0.10, Train_acc 98.92, Test_acc 69.55
2025-02-14 21:22:24,147 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.05, Spatial_loss 1.44, Flat_loss 0.10, Train_acc 99.00, Test_acc 69.73
2025-02-14 21:22:25,456 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.06, Spatial_loss 1.48, Flat_loss 0.10, Train_acc 98.67, Test_acc 69.58
2025-02-14 21:22:26,773 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.06, Spatial_loss 1.49, Flat_loss 0.10, Train_acc 99.25, Test_acc 69.58
2025-02-14 21:22:28,121 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.05, Spatial_loss 1.52, Flat_loss 0.10, Train_acc 99.33, Test_acc 69.37
2025-02-14 21:22:29,400 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.07, Spatial_loss 1.44, Flat_loss 0.10, Train_acc 98.50, Test_acc 69.58
2025-02-14 21:22:30,699 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 1.51, Flat_loss 0.10, Train_acc 99.17, Test_acc 69.58
2025-02-14 21:22:30,701 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:22:53,433 [podnet.py] => Exemplar size: 1200
2025-02-14 21:22:53,433 [trainer.py] => CNN: {'total': 69.58, '00-09': 74.9, '10-19': 64.1, '20-29': 75.4, '30-39': 70.3, '40-49': 72.9, '50-59': 59.9, 'old': 71.36, 'new': 50.0}
2025-02-14 21:22:53,433 [trainer.py] => NME: {'total': 69.3, '00-09': 76.2, '10-19': 66.4, '20-29': 75.4, '30-39': 70.3, '40-49': 73.8, '50-59': 53.7, 'old': 71.51, 'new': 45.0}
2025-02-14 21:22:53,433 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58]
2025-02-14 21:22:53,433 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27]
2025-02-14 21:22:53,433 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3]
2025-02-14 21:22:53,433 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18]

2025-02-14 21:22:53,433 [trainer.py] => Average Accuracy (CNN): 73.65666666666665
2025-02-14 21:22:53,433 [trainer.py] => Average Accuracy (NME): 73.36666666666667
2025-02-14 21:22:53,434 [trainer.py] => All params: 504657
2025-02-14 21:22:53,434 [trainer.py] => Trainable params: 504657
2025-02-14 21:22:53,435 [podnet.py] => Learning on 60-65
2025-02-14 21:22:53,463 [podnet.py] => Adaptive factor: 3.605551275463989
2025-02-14 21:22:55,391 [podnet.py] => Task 3, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 3.77, Flat_loss 0.80, Train_acc 66.65, Test_acc 44.95
2025-02-14 21:22:57,303 [podnet.py] => Task 3, Epoch 2/160 (LR 0.09996) => LSC_loss 0.64, Spatial_loss 3.79, Flat_loss 0.59, Train_acc 82.57, Test_acc 57.15
2025-02-14 21:22:59,197 [podnet.py] => Task 3, Epoch 3/160 (LR 0.09991) => LSC_loss 0.48, Spatial_loss 3.42, Flat_loss 0.47, Train_acc 86.43, Test_acc 57.82
2025-02-14 21:23:01,050 [podnet.py] => Task 3, Epoch 4/160 (LR 0.09985) => LSC_loss 0.43, Spatial_loss 3.24, Flat_loss 0.42, Train_acc 88.46, Test_acc 58.57
2025-02-14 21:23:02,975 [podnet.py] => Task 3, Epoch 5/160 (LR 0.09976) => LSC_loss 0.41, Spatial_loss 3.10, Flat_loss 0.39, Train_acc 89.59, Test_acc 54.05
2025-02-14 21:23:04,834 [podnet.py] => Task 3, Epoch 6/160 (LR 0.09965) => LSC_loss 0.40, Spatial_loss 3.07, Flat_loss 0.38, Train_acc 89.22, Test_acc 59.94
2025-02-14 21:23:06,666 [podnet.py] => Task 3, Epoch 7/160 (LR 0.09953) => LSC_loss 0.35, Spatial_loss 2.96, Flat_loss 0.36, Train_acc 90.43, Test_acc 59.28
2025-02-14 21:23:08,535 [podnet.py] => Task 3, Epoch 8/160 (LR 0.09938) => LSC_loss 0.34, Spatial_loss 2.99, Flat_loss 0.36, Train_acc 91.35, Test_acc 56.20
2025-02-14 21:23:10,365 [podnet.py] => Task 3, Epoch 9/160 (LR 0.09922) => LSC_loss 0.33, Spatial_loss 2.97, Flat_loss 0.35, Train_acc 91.78, Test_acc 58.51
2025-02-14 21:23:12,180 [podnet.py] => Task 3, Epoch 10/160 (LR 0.09904) => LSC_loss 0.34, Spatial_loss 2.91, Flat_loss 0.34, Train_acc 91.51, Test_acc 56.75
2025-02-14 21:23:14,067 [podnet.py] => Task 3, Epoch 11/160 (LR 0.09884) => LSC_loss 0.31, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 92.32, Test_acc 54.80
2025-02-14 21:23:15,987 [podnet.py] => Task 3, Epoch 12/160 (LR 0.09862) => LSC_loss 0.32, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 92.08, Test_acc 55.22
2025-02-14 21:23:17,850 [podnet.py] => Task 3, Epoch 13/160 (LR 0.09838) => LSC_loss 0.30, Spatial_loss 2.81, Flat_loss 0.33, Train_acc 92.76, Test_acc 58.60
2025-02-14 21:23:19,655 [podnet.py] => Task 3, Epoch 14/160 (LR 0.09812) => LSC_loss 0.30, Spatial_loss 2.87, Flat_loss 0.32, Train_acc 92.41, Test_acc 57.85
2025-02-14 21:23:21,522 [podnet.py] => Task 3, Epoch 15/160 (LR 0.09785) => LSC_loss 0.29, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 92.81, Test_acc 57.20
2025-02-14 21:23:23,416 [podnet.py] => Task 3, Epoch 16/160 (LR 0.09755) => LSC_loss 0.29, Spatial_loss 2.88, Flat_loss 0.33, Train_acc 93.03, Test_acc 55.26
2025-02-14 21:23:25,235 [podnet.py] => Task 3, Epoch 17/160 (LR 0.09724) => LSC_loss 0.28, Spatial_loss 2.82, Flat_loss 0.32, Train_acc 93.00, Test_acc 54.02
2025-02-14 21:23:27,122 [podnet.py] => Task 3, Epoch 18/160 (LR 0.09691) => LSC_loss 0.27, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.78, Test_acc 55.92
2025-02-14 21:23:28,985 [podnet.py] => Task 3, Epoch 19/160 (LR 0.09656) => LSC_loss 0.27, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.89, Test_acc 58.18
2025-02-14 21:23:30,834 [podnet.py] => Task 3, Epoch 20/160 (LR 0.09619) => LSC_loss 0.26, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 93.78, Test_acc 59.92
2025-02-14 21:23:32,712 [podnet.py] => Task 3, Epoch 21/160 (LR 0.09581) => LSC_loss 0.27, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 93.76, Test_acc 57.49
2025-02-14 21:23:34,570 [podnet.py] => Task 3, Epoch 22/160 (LR 0.09541) => LSC_loss 0.27, Spatial_loss 2.81, Flat_loss 0.31, Train_acc 94.03, Test_acc 56.94
2025-02-14 21:23:36,416 [podnet.py] => Task 3, Epoch 23/160 (LR 0.09499) => LSC_loss 0.26, Spatial_loss 2.77, Flat_loss 0.31, Train_acc 93.78, Test_acc 59.91
2025-02-14 21:23:38,323 [podnet.py] => Task 3, Epoch 24/160 (LR 0.09455) => LSC_loss 0.26, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 93.76, Test_acc 58.52
2025-02-14 21:23:40,169 [podnet.py] => Task 3, Epoch 25/160 (LR 0.09410) => LSC_loss 0.27, Spatial_loss 2.82, Flat_loss 0.31, Train_acc 93.59, Test_acc 56.97
2025-02-14 21:23:42,127 [podnet.py] => Task 3, Epoch 26/160 (LR 0.09362) => LSC_loss 0.25, Spatial_loss 2.79, Flat_loss 0.31, Train_acc 95.00, Test_acc 53.57
2025-02-14 21:23:44,005 [podnet.py] => Task 3, Epoch 27/160 (LR 0.09314) => LSC_loss 0.26, Spatial_loss 2.74, Flat_loss 0.31, Train_acc 93.81, Test_acc 58.32
2025-02-14 21:23:45,906 [podnet.py] => Task 3, Epoch 28/160 (LR 0.09263) => LSC_loss 0.25, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 94.38, Test_acc 59.75
2025-02-14 21:23:47,726 [podnet.py] => Task 3, Epoch 29/160 (LR 0.09211) => LSC_loss 0.25, Spatial_loss 2.72, Flat_loss 0.30, Train_acc 94.84, Test_acc 56.29
2025-02-14 21:23:49,642 [podnet.py] => Task 3, Epoch 30/160 (LR 0.09157) => LSC_loss 0.24, Spatial_loss 2.67, Flat_loss 0.30, Train_acc 94.51, Test_acc 54.03
2025-02-14 21:23:51,476 [podnet.py] => Task 3, Epoch 31/160 (LR 0.09102) => LSC_loss 0.26, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 94.00, Test_acc 57.35
2025-02-14 21:23:53,302 [podnet.py] => Task 3, Epoch 32/160 (LR 0.09045) => LSC_loss 0.27, Spatial_loss 2.82, Flat_loss 0.32, Train_acc 93.14, Test_acc 59.14
2025-02-14 21:23:55,157 [podnet.py] => Task 3, Epoch 33/160 (LR 0.08987) => LSC_loss 0.24, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 94.03, Test_acc 60.17
2025-02-14 21:23:57,000 [podnet.py] => Task 3, Epoch 34/160 (LR 0.08927) => LSC_loss 0.24, Spatial_loss 2.66, Flat_loss 0.29, Train_acc 94.92, Test_acc 59.62
2025-02-14 21:23:58,861 [podnet.py] => Task 3, Epoch 35/160 (LR 0.08865) => LSC_loss 0.24, Spatial_loss 2.65, Flat_loss 0.29, Train_acc 94.84, Test_acc 58.85
2025-02-14 21:24:00,704 [podnet.py] => Task 3, Epoch 36/160 (LR 0.08802) => LSC_loss 0.25, Spatial_loss 2.69, Flat_loss 0.30, Train_acc 94.24, Test_acc 57.52
2025-02-14 21:24:02,608 [podnet.py] => Task 3, Epoch 37/160 (LR 0.08738) => LSC_loss 0.26, Spatial_loss 2.65, Flat_loss 0.30, Train_acc 93.49, Test_acc 59.06
2025-02-14 21:24:04,483 [podnet.py] => Task 3, Epoch 38/160 (LR 0.08672) => LSC_loss 0.23, Spatial_loss 2.72, Flat_loss 0.30, Train_acc 95.00, Test_acc 61.34
2025-02-14 21:24:06,383 [podnet.py] => Task 3, Epoch 39/160 (LR 0.08604) => LSC_loss 0.24, Spatial_loss 2.56, Flat_loss 0.28, Train_acc 94.62, Test_acc 58.23
2025-02-14 21:24:08,206 [podnet.py] => Task 3, Epoch 40/160 (LR 0.08536) => LSC_loss 0.23, Spatial_loss 2.59, Flat_loss 0.28, Train_acc 94.70, Test_acc 61.51
2025-02-14 21:24:10,027 [podnet.py] => Task 3, Epoch 41/160 (LR 0.08465) => LSC_loss 0.24, Spatial_loss 2.64, Flat_loss 0.29, Train_acc 94.38, Test_acc 59.38
2025-02-14 21:24:11,869 [podnet.py] => Task 3, Epoch 42/160 (LR 0.08394) => LSC_loss 0.24, Spatial_loss 2.61, Flat_loss 0.29, Train_acc 94.43, Test_acc 58.62
2025-02-14 21:24:13,689 [podnet.py] => Task 3, Epoch 43/160 (LR 0.08321) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 94.92, Test_acc 56.75
2025-02-14 21:24:15,579 [podnet.py] => Task 3, Epoch 44/160 (LR 0.08247) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 94.59, Test_acc 60.22
2025-02-14 21:24:17,458 [podnet.py] => Task 3, Epoch 45/160 (LR 0.08172) => LSC_loss 0.24, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 94.78, Test_acc 59.31
2025-02-14 21:24:19,309 [podnet.py] => Task 3, Epoch 46/160 (LR 0.08095) => LSC_loss 0.22, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 95.78, Test_acc 57.31
2025-02-14 21:24:21,174 [podnet.py] => Task 3, Epoch 47/160 (LR 0.08018) => LSC_loss 0.23, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.11, Test_acc 55.00
2025-02-14 21:24:23,039 [podnet.py] => Task 3, Epoch 48/160 (LR 0.07939) => LSC_loss 0.23, Spatial_loss 2.66, Flat_loss 0.29, Train_acc 94.95, Test_acc 57.69
2025-02-14 21:24:24,892 [podnet.py] => Task 3, Epoch 49/160 (LR 0.07859) => LSC_loss 0.24, Spatial_loss 2.63, Flat_loss 0.29, Train_acc 94.68, Test_acc 58.23
2025-02-14 21:24:26,741 [podnet.py] => Task 3, Epoch 50/160 (LR 0.07778) => LSC_loss 0.23, Spatial_loss 2.59, Flat_loss 0.28, Train_acc 94.81, Test_acc 61.09
2025-02-14 21:24:28,572 [podnet.py] => Task 3, Epoch 51/160 (LR 0.07696) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.27, Train_acc 95.46, Test_acc 58.62
2025-02-14 21:24:30,419 [podnet.py] => Task 3, Epoch 52/160 (LR 0.07612) => LSC_loss 0.22, Spatial_loss 2.57, Flat_loss 0.28, Train_acc 95.03, Test_acc 58.00
2025-02-14 21:24:32,271 [podnet.py] => Task 3, Epoch 53/160 (LR 0.07528) => LSC_loss 0.22, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 94.97, Test_acc 59.52
2025-02-14 21:24:34,121 [podnet.py] => Task 3, Epoch 54/160 (LR 0.07443) => LSC_loss 0.22, Spatial_loss 2.57, Flat_loss 0.27, Train_acc 95.30, Test_acc 57.54
2025-02-14 21:24:35,930 [podnet.py] => Task 3, Epoch 55/160 (LR 0.07357) => LSC_loss 0.22, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 95.16, Test_acc 60.57
2025-02-14 21:24:37,813 [podnet.py] => Task 3, Epoch 56/160 (LR 0.07270) => LSC_loss 0.23, Spatial_loss 2.56, Flat_loss 0.28, Train_acc 94.89, Test_acc 59.57
2025-02-14 21:24:39,708 [podnet.py] => Task 3, Epoch 57/160 (LR 0.07182) => LSC_loss 0.23, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.03, Test_acc 56.69
2025-02-14 21:24:41,606 [podnet.py] => Task 3, Epoch 58/160 (LR 0.07093) => LSC_loss 0.22, Spatial_loss 2.46, Flat_loss 0.26, Train_acc 95.24, Test_acc 58.66
2025-02-14 21:24:43,487 [podnet.py] => Task 3, Epoch 59/160 (LR 0.07004) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.26, Train_acc 95.92, Test_acc 60.65
2025-02-14 21:24:45,370 [podnet.py] => Task 3, Epoch 60/160 (LR 0.06913) => LSC_loss 0.22, Spatial_loss 2.42, Flat_loss 0.26, Train_acc 95.68, Test_acc 57.78
2025-02-14 21:24:47,220 [podnet.py] => Task 3, Epoch 61/160 (LR 0.06822) => LSC_loss 0.23, Spatial_loss 2.49, Flat_loss 0.26, Train_acc 95.03, Test_acc 58.28
2025-02-14 21:24:49,041 [podnet.py] => Task 3, Epoch 62/160 (LR 0.06731) => LSC_loss 0.23, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 94.51, Test_acc 56.92
2025-02-14 21:24:50,969 [podnet.py] => Task 3, Epoch 63/160 (LR 0.06638) => LSC_loss 0.21, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 95.76, Test_acc 59.71
2025-02-14 21:24:52,913 [podnet.py] => Task 3, Epoch 64/160 (LR 0.06545) => LSC_loss 0.22, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 95.14, Test_acc 59.78
2025-02-14 21:24:54,805 [podnet.py] => Task 3, Epoch 65/160 (LR 0.06451) => LSC_loss 0.21, Spatial_loss 2.37, Flat_loss 0.25, Train_acc 95.81, Test_acc 60.29
2025-02-14 21:24:56,682 [podnet.py] => Task 3, Epoch 66/160 (LR 0.06357) => LSC_loss 0.22, Spatial_loss 2.33, Flat_loss 0.25, Train_acc 95.38, Test_acc 56.74
2025-02-14 21:24:58,474 [podnet.py] => Task 3, Epoch 67/160 (LR 0.06262) => LSC_loss 0.22, Spatial_loss 2.35, Flat_loss 0.25, Train_acc 95.27, Test_acc 60.40
2025-02-14 21:25:00,345 [podnet.py] => Task 3, Epoch 68/160 (LR 0.06167) => LSC_loss 0.21, Spatial_loss 2.32, Flat_loss 0.25, Train_acc 95.32, Test_acc 60.40
2025-02-14 21:25:02,180 [podnet.py] => Task 3, Epoch 69/160 (LR 0.06072) => LSC_loss 0.23, Spatial_loss 2.33, Flat_loss 0.24, Train_acc 94.62, Test_acc 59.55
2025-02-14 21:25:03,976 [podnet.py] => Task 3, Epoch 70/160 (LR 0.05975) => LSC_loss 0.23, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 95.08, Test_acc 59.28
2025-02-14 21:25:05,816 [podnet.py] => Task 3, Epoch 71/160 (LR 0.05879) => LSC_loss 0.21, Spatial_loss 2.35, Flat_loss 0.25, Train_acc 95.49, Test_acc 60.06
2025-02-14 21:25:07,663 [podnet.py] => Task 3, Epoch 72/160 (LR 0.05782) => LSC_loss 0.21, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 95.57, Test_acc 61.74
2025-02-14 21:25:09,570 [podnet.py] => Task 3, Epoch 73/160 (LR 0.05685) => LSC_loss 0.21, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 95.62, Test_acc 58.98
2025-02-14 21:25:11,397 [podnet.py] => Task 3, Epoch 74/160 (LR 0.05588) => LSC_loss 0.21, Spatial_loss 2.33, Flat_loss 0.24, Train_acc 95.54, Test_acc 56.32
2025-02-14 21:25:13,268 [podnet.py] => Task 3, Epoch 75/160 (LR 0.05490) => LSC_loss 0.21, Spatial_loss 2.32, Flat_loss 0.24, Train_acc 95.54, Test_acc 60.91
2025-02-14 21:25:15,195 [podnet.py] => Task 3, Epoch 76/160 (LR 0.05392) => LSC_loss 0.20, Spatial_loss 2.28, Flat_loss 0.24, Train_acc 96.38, Test_acc 61.51
2025-02-14 21:25:17,051 [podnet.py] => Task 3, Epoch 77/160 (LR 0.05294) => LSC_loss 0.21, Spatial_loss 2.28, Flat_loss 0.24, Train_acc 95.62, Test_acc 60.46
2025-02-14 21:25:18,955 [podnet.py] => Task 3, Epoch 78/160 (LR 0.05196) => LSC_loss 0.21, Spatial_loss 2.21, Flat_loss 0.23, Train_acc 95.46, Test_acc 60.49
2025-02-14 21:25:20,840 [podnet.py] => Task 3, Epoch 79/160 (LR 0.05098) => LSC_loss 0.21, Spatial_loss 2.18, Flat_loss 0.23, Train_acc 95.43, Test_acc 59.86
2025-02-14 21:25:22,701 [podnet.py] => Task 3, Epoch 80/160 (LR 0.05000) => LSC_loss 0.21, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 95.84, Test_acc 63.52
2025-02-14 21:25:24,542 [podnet.py] => Task 3, Epoch 81/160 (LR 0.04902) => LSC_loss 0.22, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 95.32, Test_acc 60.08
2025-02-14 21:25:26,397 [podnet.py] => Task 3, Epoch 82/160 (LR 0.04804) => LSC_loss 0.21, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 95.62, Test_acc 61.77
2025-02-14 21:25:28,232 [podnet.py] => Task 3, Epoch 83/160 (LR 0.04706) => LSC_loss 0.20, Spatial_loss 2.18, Flat_loss 0.22, Train_acc 96.24, Test_acc 63.37
2025-02-14 21:25:30,037 [podnet.py] => Task 3, Epoch 84/160 (LR 0.04608) => LSC_loss 0.21, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 95.46, Test_acc 61.68
2025-02-14 21:25:31,937 [podnet.py] => Task 3, Epoch 85/160 (LR 0.04510) => LSC_loss 0.22, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 95.41, Test_acc 62.43
2025-02-14 21:25:33,728 [podnet.py] => Task 3, Epoch 86/160 (LR 0.04412) => LSC_loss 0.21, Spatial_loss 2.17, Flat_loss 0.22, Train_acc 96.24, Test_acc 58.66
2025-02-14 21:25:35,550 [podnet.py] => Task 3, Epoch 87/160 (LR 0.04315) => LSC_loss 0.21, Spatial_loss 2.11, Flat_loss 0.22, Train_acc 96.00, Test_acc 62.02
2025-02-14 21:25:37,428 [podnet.py] => Task 3, Epoch 88/160 (LR 0.04218) => LSC_loss 0.21, Spatial_loss 2.16, Flat_loss 0.22, Train_acc 95.70, Test_acc 60.32
2025-02-14 21:25:39,320 [podnet.py] => Task 3, Epoch 89/160 (LR 0.04121) => LSC_loss 0.21, Spatial_loss 2.06, Flat_loss 0.21, Train_acc 95.84, Test_acc 60.49
2025-02-14 21:25:41,246 [podnet.py] => Task 3, Epoch 90/160 (LR 0.04025) => LSC_loss 0.20, Spatial_loss 2.03, Flat_loss 0.21, Train_acc 96.16, Test_acc 60.26
2025-02-14 21:25:43,061 [podnet.py] => Task 3, Epoch 91/160 (LR 0.03928) => LSC_loss 0.21, Spatial_loss 2.02, Flat_loss 0.21, Train_acc 95.86, Test_acc 62.00
2025-02-14 21:25:44,872 [podnet.py] => Task 3, Epoch 92/160 (LR 0.03833) => LSC_loss 0.20, Spatial_loss 2.00, Flat_loss 0.21, Train_acc 96.22, Test_acc 59.35
2025-02-14 21:25:46,701 [podnet.py] => Task 3, Epoch 93/160 (LR 0.03738) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.97, Test_acc 63.60
2025-02-14 21:25:48,567 [podnet.py] => Task 3, Epoch 94/160 (LR 0.03643) => LSC_loss 0.20, Spatial_loss 2.00, Flat_loss 0.20, Train_acc 96.05, Test_acc 61.68
2025-02-14 21:25:50,493 [podnet.py] => Task 3, Epoch 95/160 (LR 0.03549) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 96.32, Test_acc 63.31
2025-02-14 21:25:52,380 [podnet.py] => Task 3, Epoch 96/160 (LR 0.03455) => LSC_loss 0.20, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.95, Test_acc 61.02
2025-02-14 21:25:54,238 [podnet.py] => Task 3, Epoch 97/160 (LR 0.03362) => LSC_loss 0.20, Spatial_loss 1.91, Flat_loss 0.20, Train_acc 96.14, Test_acc 61.20
2025-02-14 21:25:56,088 [podnet.py] => Task 3, Epoch 98/160 (LR 0.03269) => LSC_loss 0.20, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 96.27, Test_acc 61.83
2025-02-14 21:25:57,944 [podnet.py] => Task 3, Epoch 99/160 (LR 0.03178) => LSC_loss 0.21, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 95.92, Test_acc 61.62
2025-02-14 21:25:59,846 [podnet.py] => Task 3, Epoch 100/160 (LR 0.03087) => LSC_loss 0.20, Spatial_loss 1.91, Flat_loss 0.19, Train_acc 96.41, Test_acc 61.42
2025-02-14 21:26:01,733 [podnet.py] => Task 3, Epoch 101/160 (LR 0.02996) => LSC_loss 0.20, Spatial_loss 1.90, Flat_loss 0.19, Train_acc 96.00, Test_acc 61.09
2025-02-14 21:26:03,577 [podnet.py] => Task 3, Epoch 102/160 (LR 0.02907) => LSC_loss 0.20, Spatial_loss 1.94, Flat_loss 0.19, Train_acc 96.03, Test_acc 62.23
2025-02-14 21:26:05,396 [podnet.py] => Task 3, Epoch 103/160 (LR 0.02818) => LSC_loss 0.20, Spatial_loss 1.85, Flat_loss 0.19, Train_acc 96.14, Test_acc 60.42
2025-02-14 21:26:07,262 [podnet.py] => Task 3, Epoch 104/160 (LR 0.02730) => LSC_loss 0.21, Spatial_loss 1.84, Flat_loss 0.18, Train_acc 96.03, Test_acc 61.55
2025-02-14 21:26:09,181 [podnet.py] => Task 3, Epoch 105/160 (LR 0.02643) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 95.59, Test_acc 62.94
2025-02-14 21:26:11,084 [podnet.py] => Task 3, Epoch 106/160 (LR 0.02557) => LSC_loss 0.20, Spatial_loss 1.81, Flat_loss 0.18, Train_acc 96.38, Test_acc 62.94
2025-02-14 21:26:12,973 [podnet.py] => Task 3, Epoch 107/160 (LR 0.02472) => LSC_loss 0.20, Spatial_loss 1.84, Flat_loss 0.18, Train_acc 96.65, Test_acc 63.71
2025-02-14 21:26:14,898 [podnet.py] => Task 3, Epoch 108/160 (LR 0.02388) => LSC_loss 0.20, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 96.08, Test_acc 63.46
2025-02-14 21:26:16,734 [podnet.py] => Task 3, Epoch 109/160 (LR 0.02304) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 96.00, Test_acc 63.25
2025-02-14 21:26:18,582 [podnet.py] => Task 3, Epoch 110/160 (LR 0.02222) => LSC_loss 0.20, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 96.30, Test_acc 63.40
2025-02-14 21:26:20,441 [podnet.py] => Task 3, Epoch 111/160 (LR 0.02141) => LSC_loss 0.20, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 96.43, Test_acc 62.68
2025-02-14 21:26:22,258 [podnet.py] => Task 3, Epoch 112/160 (LR 0.02061) => LSC_loss 0.20, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 96.24, Test_acc 61.37
2025-02-14 21:26:24,164 [podnet.py] => Task 3, Epoch 113/160 (LR 0.01982) => LSC_loss 0.20, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 96.08, Test_acc 64.71
2025-02-14 21:26:26,049 [podnet.py] => Task 3, Epoch 114/160 (LR 0.01905) => LSC_loss 0.21, Spatial_loss 1.75, Flat_loss 0.17, Train_acc 95.59, Test_acc 62.23
2025-02-14 21:26:27,904 [podnet.py] => Task 3, Epoch 115/160 (LR 0.01828) => LSC_loss 0.21, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 95.89, Test_acc 60.98
2025-02-14 21:26:29,798 [podnet.py] => Task 3, Epoch 116/160 (LR 0.01753) => LSC_loss 0.19, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 96.73, Test_acc 62.00
2025-02-14 21:26:31,657 [podnet.py] => Task 3, Epoch 117/160 (LR 0.01679) => LSC_loss 0.20, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 96.35, Test_acc 62.86
2025-02-14 21:26:33,511 [podnet.py] => Task 3, Epoch 118/160 (LR 0.01606) => LSC_loss 0.20, Spatial_loss 1.67, Flat_loss 0.16, Train_acc 96.51, Test_acc 61.80
2025-02-14 21:26:35,414 [podnet.py] => Task 3, Epoch 119/160 (LR 0.01535) => LSC_loss 0.20, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 96.24, Test_acc 64.22
2025-02-14 21:26:37,231 [podnet.py] => Task 3, Epoch 120/160 (LR 0.01464) => LSC_loss 0.21, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 95.81, Test_acc 62.48
2025-02-14 21:26:39,083 [podnet.py] => Task 3, Epoch 121/160 (LR 0.01396) => LSC_loss 0.19, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 96.68, Test_acc 62.49
2025-02-14 21:26:40,913 [podnet.py] => Task 3, Epoch 122/160 (LR 0.01328) => LSC_loss 0.20, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 96.30, Test_acc 63.60
2025-02-14 21:26:42,741 [podnet.py] => Task 3, Epoch 123/160 (LR 0.01262) => LSC_loss 0.20, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 96.22, Test_acc 64.46
2025-02-14 21:26:44,569 [podnet.py] => Task 3, Epoch 124/160 (LR 0.01198) => LSC_loss 0.20, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 96.70, Test_acc 65.37
2025-02-14 21:26:46,448 [podnet.py] => Task 3, Epoch 125/160 (LR 0.01135) => LSC_loss 0.20, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 96.24, Test_acc 62.51
2025-02-14 21:26:48,330 [podnet.py] => Task 3, Epoch 126/160 (LR 0.01073) => LSC_loss 0.20, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 96.22, Test_acc 63.57
2025-02-14 21:26:50,229 [podnet.py] => Task 3, Epoch 127/160 (LR 0.01013) => LSC_loss 0.20, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 96.76, Test_acc 63.85
2025-02-14 21:26:52,093 [podnet.py] => Task 3, Epoch 128/160 (LR 0.00955) => LSC_loss 0.20, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 96.78, Test_acc 62.54
2025-02-14 21:26:53,987 [podnet.py] => Task 3, Epoch 129/160 (LR 0.00898) => LSC_loss 0.20, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 96.57, Test_acc 63.88
2025-02-14 21:26:55,850 [podnet.py] => Task 3, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.95, Test_acc 62.49
2025-02-14 21:26:57,688 [podnet.py] => Task 3, Epoch 131/160 (LR 0.00789) => LSC_loss 0.19, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 96.30, Test_acc 62.86
2025-02-14 21:26:59,526 [podnet.py] => Task 3, Epoch 132/160 (LR 0.00737) => LSC_loss 0.19, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 96.59, Test_acc 64.58
2025-02-14 21:27:01,359 [podnet.py] => Task 3, Epoch 133/160 (LR 0.00686) => LSC_loss 0.20, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 96.35, Test_acc 64.51
2025-02-14 21:27:03,232 [podnet.py] => Task 3, Epoch 134/160 (LR 0.00638) => LSC_loss 0.20, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 96.43, Test_acc 64.14
2025-02-14 21:27:05,080 [podnet.py] => Task 3, Epoch 135/160 (LR 0.00590) => LSC_loss 0.20, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 96.73, Test_acc 63.62
2025-02-14 21:27:06,917 [podnet.py] => Task 3, Epoch 136/160 (LR 0.00545) => LSC_loss 0.20, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 96.62, Test_acc 64.02
2025-02-14 21:27:08,837 [podnet.py] => Task 3, Epoch 137/160 (LR 0.00501) => LSC_loss 0.20, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 96.35, Test_acc 63.15
2025-02-14 21:27:10,706 [podnet.py] => Task 3, Epoch 138/160 (LR 0.00459) => LSC_loss 0.20, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 96.54, Test_acc 64.20
2025-02-14 21:27:12,596 [podnet.py] => Task 3, Epoch 139/160 (LR 0.00419) => LSC_loss 0.21, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 96.68, Test_acc 63.98
2025-02-14 21:27:14,493 [podnet.py] => Task 3, Epoch 140/160 (LR 0.00381) => LSC_loss 0.19, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 96.97, Test_acc 64.14
2025-02-14 21:27:16,306 [podnet.py] => Task 3, Epoch 141/160 (LR 0.00344) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 97.08, Test_acc 64.32
2025-02-14 21:27:18,149 [podnet.py] => Task 3, Epoch 142/160 (LR 0.00309) => LSC_loss 0.20, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 96.57, Test_acc 64.05
2025-02-14 21:27:20,016 [podnet.py] => Task 3, Epoch 143/160 (LR 0.00276) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 96.38, Test_acc 64.12
2025-02-14 21:27:21,854 [podnet.py] => Task 3, Epoch 144/160 (LR 0.00245) => LSC_loss 0.20, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 96.54, Test_acc 64.46
2025-02-14 21:27:23,708 [podnet.py] => Task 3, Epoch 145/160 (LR 0.00215) => LSC_loss 0.20, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 96.57, Test_acc 64.03
2025-02-14 21:27:25,540 [podnet.py] => Task 3, Epoch 146/160 (LR 0.00188) => LSC_loss 0.21, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 96.16, Test_acc 63.91
2025-02-14 21:27:27,363 [podnet.py] => Task 3, Epoch 147/160 (LR 0.00162) => LSC_loss 0.20, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 95.95, Test_acc 64.38
2025-02-14 21:27:29,194 [podnet.py] => Task 3, Epoch 148/160 (LR 0.00138) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.11, Test_acc 64.20
2025-02-14 21:27:31,013 [podnet.py] => Task 3, Epoch 149/160 (LR 0.00116) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.65, Test_acc 64.26
2025-02-14 21:27:32,839 [podnet.py] => Task 3, Epoch 150/160 (LR 0.00096) => LSC_loss 0.19, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 96.86, Test_acc 64.55
2025-02-14 21:27:34,745 [podnet.py] => Task 3, Epoch 151/160 (LR 0.00078) => LSC_loss 0.20, Spatial_loss 1.29, Flat_loss 0.13, Train_acc 96.84, Test_acc 64.49
2025-02-14 21:27:36,599 [podnet.py] => Task 3, Epoch 152/160 (LR 0.00062) => LSC_loss 0.20, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 96.51, Test_acc 64.46
2025-02-14 21:27:38,454 [podnet.py] => Task 3, Epoch 153/160 (LR 0.00047) => LSC_loss 0.20, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 96.49, Test_acc 64.34
2025-02-14 21:27:40,325 [podnet.py] => Task 3, Epoch 154/160 (LR 0.00035) => LSC_loss 0.20, Spatial_loss 1.30, Flat_loss 0.14, Train_acc 96.70, Test_acc 64.34
2025-02-14 21:27:42,110 [podnet.py] => Task 3, Epoch 155/160 (LR 0.00024) => LSC_loss 0.20, Spatial_loss 1.29, Flat_loss 0.14, Train_acc 96.84, Test_acc 64.46
2025-02-14 21:27:44,008 [podnet.py] => Task 3, Epoch 156/160 (LR 0.00015) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.24, Test_acc 64.46
2025-02-14 21:27:45,833 [podnet.py] => Task 3, Epoch 157/160 (LR 0.00009) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.41, Test_acc 64.32
2025-02-14 21:27:47,694 [podnet.py] => Task 3, Epoch 158/160 (LR 0.00004) => LSC_loss 0.19, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 96.84, Test_acc 64.35
2025-02-14 21:27:49,502 [podnet.py] => Task 3, Epoch 159/160 (LR 0.00001) => LSC_loss 0.20, Spatial_loss 1.26, Flat_loss 0.13, Train_acc 96.54, Test_acc 64.45
2025-02-14 21:27:51,308 [podnet.py] => Task 3, Epoch 160/160 (LR 0.00000) => LSC_loss 0.20, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 96.32, Test_acc 64.37
2025-02-14 21:27:51,308 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:27:51,309 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:28:13,354 [podnet.py] => The size of finetune dataset: 1300
2025-02-14 21:28:14,627 [podnet.py] => Task 3, Epoch 1/20 (LR 0.00497) => LSC_loss 0.17, Spatial_loss 1.97, Flat_loss 0.20, Train_acc 95.31, Test_acc 64.11
2025-02-14 21:28:15,950 [podnet.py] => Task 3, Epoch 2/20 (LR 0.00488) => LSC_loss 0.08, Spatial_loss 1.57, Flat_loss 0.11, Train_acc 99.00, Test_acc 65.43
2025-02-14 21:28:17,202 [podnet.py] => Task 3, Epoch 3/20 (LR 0.00473) => LSC_loss 0.08, Spatial_loss 1.50, Flat_loss 0.09, Train_acc 99.00, Test_acc 65.65
2025-02-14 21:28:18,596 [podnet.py] => Task 3, Epoch 4/20 (LR 0.00452) => LSC_loss 0.08, Spatial_loss 1.51, Flat_loss 0.09, Train_acc 98.46, Test_acc 66.05
2025-02-14 21:28:19,897 [podnet.py] => Task 3, Epoch 5/20 (LR 0.00427) => LSC_loss 0.07, Spatial_loss 1.47, Flat_loss 0.08, Train_acc 99.15, Test_acc 66.20
2025-02-14 21:28:21,267 [podnet.py] => Task 3, Epoch 6/20 (LR 0.00397) => LSC_loss 0.06, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 98.92, Test_acc 65.66
2025-02-14 21:28:22,522 [podnet.py] => Task 3, Epoch 7/20 (LR 0.00363) => LSC_loss 0.09, Spatial_loss 1.43, Flat_loss 0.08, Train_acc 99.00, Test_acc 66.18
2025-02-14 21:28:23,820 [podnet.py] => Task 3, Epoch 8/20 (LR 0.00327) => LSC_loss 0.08, Spatial_loss 1.46, Flat_loss 0.08, Train_acc 99.15, Test_acc 66.05
2025-02-14 21:28:25,071 [podnet.py] => Task 3, Epoch 9/20 (LR 0.00289) => LSC_loss 0.07, Spatial_loss 1.35, Flat_loss 0.07, Train_acc 98.85, Test_acc 65.82
2025-02-14 21:28:26,407 [podnet.py] => Task 3, Epoch 10/20 (LR 0.00250) => LSC_loss 0.06, Spatial_loss 1.38, Flat_loss 0.07, Train_acc 99.15, Test_acc 66.20
2025-02-14 21:28:27,710 [podnet.py] => Task 3, Epoch 11/20 (LR 0.00211) => LSC_loss 0.07, Spatial_loss 1.41, Flat_loss 0.08, Train_acc 99.46, Test_acc 66.00
2025-02-14 21:28:29,040 [podnet.py] => Task 3, Epoch 12/20 (LR 0.00173) => LSC_loss 0.06, Spatial_loss 1.30, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.18
2025-02-14 21:28:30,348 [podnet.py] => Task 3, Epoch 13/20 (LR 0.00137) => LSC_loss 0.07, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 98.77, Test_acc 66.40
2025-02-14 21:28:31,689 [podnet.py] => Task 3, Epoch 14/20 (LR 0.00103) => LSC_loss 0.06, Spatial_loss 1.42, Flat_loss 0.08, Train_acc 99.38, Test_acc 66.20
2025-02-14 21:28:33,075 [podnet.py] => Task 3, Epoch 15/20 (LR 0.00073) => LSC_loss 0.06, Spatial_loss 1.34, Flat_loss 0.07, Train_acc 99.31, Test_acc 65.77
2025-02-14 21:28:34,414 [podnet.py] => Task 3, Epoch 16/20 (LR 0.00048) => LSC_loss 0.07, Spatial_loss 1.30, Flat_loss 0.07, Train_acc 98.77, Test_acc 66.00
2025-02-14 21:28:35,741 [podnet.py] => Task 3, Epoch 17/20 (LR 0.00027) => LSC_loss 0.06, Spatial_loss 1.38, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.05
2025-02-14 21:28:37,049 [podnet.py] => Task 3, Epoch 18/20 (LR 0.00012) => LSC_loss 0.08, Spatial_loss 1.35, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.03
2025-02-14 21:28:38,380 [podnet.py] => Task 3, Epoch 19/20 (LR 0.00003) => LSC_loss 0.07, Spatial_loss 1.34, Flat_loss 0.07, Train_acc 99.23, Test_acc 66.26
2025-02-14 21:28:39,709 [podnet.py] => Task 3, Epoch 20/20 (LR 0.00000) => LSC_loss 0.06, Spatial_loss 1.36, Flat_loss 0.07, Train_acc 99.31, Test_acc 66.06
2025-02-14 21:28:39,710 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:29:03,625 [podnet.py] => Exemplar size: 1300
2025-02-14 21:29:03,625 [trainer.py] => CNN: {'total': 66.06, '00-09': 72.8, '10-19': 60.4, '20-29': 73.9, '30-39': 66.7, '40-49': 68.5, '50-59': 55.6, '60-69': 63.0, 'old': 66.32, 'new': 63.0}
2025-02-14 21:29:03,626 [trainer.py] => NME: {'total': 66.14, '00-09': 75.5, '10-19': 63.9, '20-29': 74.8, '30-39': 68.1, '40-49': 70.7, '50-59': 50.1, '60-69': 53.6, 'old': 67.18, 'new': 53.6}
2025-02-14 21:29:03,626 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06]
2025-02-14 21:29:03,626 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49]
2025-02-14 21:29:03,626 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14]
2025-02-14 21:29:03,626 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22]

2025-02-14 21:29:03,626 [trainer.py] => Average Accuracy (CNN): 71.7575
2025-02-14 21:29:03,626 [trainer.py] => Average Accuracy (NME): 71.56
2025-02-14 21:29:03,626 [trainer.py] => All params: 507857
2025-02-14 21:29:03,626 [trainer.py] => Trainable params: 507857
2025-02-14 21:29:03,627 [podnet.py] => Learning on 65-70
2025-02-14 21:29:03,657 [podnet.py] => Adaptive factor: 3.7416573867739413
2025-02-14 21:29:05,627 [podnet.py] => Task 4, Epoch 1/160 (LR 0.09999) => LSC_loss 1.66, Spatial_loss 4.04, Flat_loss 0.82, Train_acc 69.32, Test_acc 46.57
2025-02-14 21:29:07,511 [podnet.py] => Task 4, Epoch 2/160 (LR 0.09996) => LSC_loss 0.61, Spatial_loss 4.18, Flat_loss 0.64, Train_acc 83.08, Test_acc 44.96
2025-02-14 21:29:09,452 [podnet.py] => Task 4, Epoch 3/160 (LR 0.09991) => LSC_loss 0.47, Spatial_loss 3.81, Flat_loss 0.52, Train_acc 87.76, Test_acc 53.10
2025-02-14 21:29:11,407 [podnet.py] => Task 4, Epoch 4/160 (LR 0.09985) => LSC_loss 0.38, Spatial_loss 3.57, Flat_loss 0.44, Train_acc 90.18, Test_acc 51.56
2025-02-14 21:29:13,324 [podnet.py] => Task 4, Epoch 5/160 (LR 0.09976) => LSC_loss 0.34, Spatial_loss 3.36, Flat_loss 0.40, Train_acc 91.50, Test_acc 53.60
2025-02-14 21:29:15,252 [podnet.py] => Task 4, Epoch 6/160 (LR 0.09965) => LSC_loss 0.31, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 92.32, Test_acc 52.09
2025-02-14 21:29:17,200 [podnet.py] => Task 4, Epoch 7/160 (LR 0.09953) => LSC_loss 0.28, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 93.24, Test_acc 52.17
2025-02-14 21:29:19,138 [podnet.py] => Task 4, Epoch 8/160 (LR 0.09938) => LSC_loss 0.31, Spatial_loss 3.11, Flat_loss 0.34, Train_acc 91.95, Test_acc 53.27
2025-02-14 21:29:21,064 [podnet.py] => Task 4, Epoch 9/160 (LR 0.09922) => LSC_loss 0.28, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 93.11, Test_acc 59.43
2025-02-14 21:29:22,965 [podnet.py] => Task 4, Epoch 10/160 (LR 0.09904) => LSC_loss 0.27, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 94.13, Test_acc 56.69
2025-02-14 21:29:24,882 [podnet.py] => Task 4, Epoch 11/160 (LR 0.09884) => LSC_loss 0.27, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 93.95, Test_acc 54.64
2025-02-14 21:29:26,729 [podnet.py] => Task 4, Epoch 12/160 (LR 0.09862) => LSC_loss 0.27, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 93.61, Test_acc 53.10
2025-02-14 21:29:28,638 [podnet.py] => Task 4, Epoch 13/160 (LR 0.09838) => LSC_loss 0.26, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 94.11, Test_acc 55.57
2025-02-14 21:29:30,544 [podnet.py] => Task 4, Epoch 14/160 (LR 0.09812) => LSC_loss 0.26, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 94.05, Test_acc 55.73
2025-02-14 21:29:32,475 [podnet.py] => Task 4, Epoch 15/160 (LR 0.09785) => LSC_loss 0.27, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 93.42, Test_acc 51.77
2025-02-14 21:29:34,347 [podnet.py] => Task 4, Epoch 16/160 (LR 0.09755) => LSC_loss 0.25, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 94.18, Test_acc 57.51
2025-02-14 21:29:36,249 [podnet.py] => Task 4, Epoch 17/160 (LR 0.09724) => LSC_loss 0.26, Spatial_loss 2.96, Flat_loss 0.30, Train_acc 94.03, Test_acc 55.37
2025-02-14 21:29:38,161 [podnet.py] => Task 4, Epoch 18/160 (LR 0.09691) => LSC_loss 0.24, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 94.29, Test_acc 53.57
2025-02-14 21:29:40,037 [podnet.py] => Task 4, Epoch 19/160 (LR 0.09656) => LSC_loss 0.25, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 94.26, Test_acc 54.81
2025-02-14 21:29:41,925 [podnet.py] => Task 4, Epoch 20/160 (LR 0.09619) => LSC_loss 0.24, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 94.53, Test_acc 56.93
2025-02-14 21:29:43,891 [podnet.py] => Task 4, Epoch 21/160 (LR 0.09581) => LSC_loss 0.23, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 95.29, Test_acc 56.83
2025-02-14 21:29:45,807 [podnet.py] => Task 4, Epoch 22/160 (LR 0.09541) => LSC_loss 0.24, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 94.47, Test_acc 53.77
2025-02-14 21:29:47,788 [podnet.py] => Task 4, Epoch 23/160 (LR 0.09499) => LSC_loss 0.25, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 94.53, Test_acc 52.87
2025-02-14 21:29:49,709 [podnet.py] => Task 4, Epoch 24/160 (LR 0.09455) => LSC_loss 0.23, Spatial_loss 2.86, Flat_loss 0.29, Train_acc 94.97, Test_acc 55.06
2025-02-14 21:29:51,556 [podnet.py] => Task 4, Epoch 25/160 (LR 0.09410) => LSC_loss 0.24, Spatial_loss 2.88, Flat_loss 0.29, Train_acc 94.18, Test_acc 51.10
2025-02-14 21:29:53,495 [podnet.py] => Task 4, Epoch 26/160 (LR 0.09362) => LSC_loss 0.23, Spatial_loss 2.81, Flat_loss 0.28, Train_acc 95.34, Test_acc 53.59
2025-02-14 21:29:55,419 [podnet.py] => Task 4, Epoch 27/160 (LR 0.09314) => LSC_loss 0.22, Spatial_loss 2.76, Flat_loss 0.27, Train_acc 95.42, Test_acc 51.97
2025-02-14 21:29:57,387 [podnet.py] => Task 4, Epoch 28/160 (LR 0.09263) => LSC_loss 0.23, Spatial_loss 2.79, Flat_loss 0.28, Train_acc 95.29, Test_acc 55.24
2025-02-14 21:29:59,323 [podnet.py] => Task 4, Epoch 29/160 (LR 0.09211) => LSC_loss 0.23, Spatial_loss 2.80, Flat_loss 0.28, Train_acc 94.92, Test_acc 50.23
2025-02-14 21:30:01,276 [podnet.py] => Task 4, Epoch 30/160 (LR 0.09157) => LSC_loss 0.23, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 94.84, Test_acc 53.03
2025-02-14 21:30:03,197 [podnet.py] => Task 4, Epoch 31/160 (LR 0.09102) => LSC_loss 0.22, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 95.55, Test_acc 54.40
2025-02-14 21:30:05,119 [podnet.py] => Task 4, Epoch 32/160 (LR 0.09045) => LSC_loss 0.22, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 94.74, Test_acc 54.63
2025-02-14 21:30:07,040 [podnet.py] => Task 4, Epoch 33/160 (LR 0.08987) => LSC_loss 0.22, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 95.34, Test_acc 56.76
2025-02-14 21:30:09,004 [podnet.py] => Task 4, Epoch 34/160 (LR 0.08927) => LSC_loss 0.22, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 95.00, Test_acc 54.10
2025-02-14 21:30:10,929 [podnet.py] => Task 4, Epoch 35/160 (LR 0.08865) => LSC_loss 0.22, Spatial_loss 2.78, Flat_loss 0.27, Train_acc 95.00, Test_acc 57.69
2025-02-14 21:30:12,863 [podnet.py] => Task 4, Epoch 36/160 (LR 0.08802) => LSC_loss 0.22, Spatial_loss 2.71, Flat_loss 0.28, Train_acc 95.45, Test_acc 57.89
2025-02-14 21:30:14,799 [podnet.py] => Task 4, Epoch 37/160 (LR 0.08738) => LSC_loss 0.21, Spatial_loss 2.67, Flat_loss 0.27, Train_acc 95.71, Test_acc 56.56
2025-02-14 21:30:16,658 [podnet.py] => Task 4, Epoch 38/160 (LR 0.08672) => LSC_loss 0.21, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 95.11, Test_acc 54.49
2025-02-14 21:30:18,604 [podnet.py] => Task 4, Epoch 39/160 (LR 0.08604) => LSC_loss 0.22, Spatial_loss 2.72, Flat_loss 0.27, Train_acc 95.21, Test_acc 58.00
2025-02-14 21:30:20,520 [podnet.py] => Task 4, Epoch 40/160 (LR 0.08536) => LSC_loss 0.21, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 95.42, Test_acc 57.30
2025-02-14 21:30:22,435 [podnet.py] => Task 4, Epoch 41/160 (LR 0.08465) => LSC_loss 0.22, Spatial_loss 2.70, Flat_loss 0.27, Train_acc 95.08, Test_acc 58.26
2025-02-14 21:30:24,367 [podnet.py] => Task 4, Epoch 42/160 (LR 0.08394) => LSC_loss 0.21, Spatial_loss 2.69, Flat_loss 0.26, Train_acc 95.87, Test_acc 54.16
2025-02-14 21:30:26,309 [podnet.py] => Task 4, Epoch 43/160 (LR 0.08321) => LSC_loss 0.22, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 95.03, Test_acc 55.19
2025-02-14 21:30:28,238 [podnet.py] => Task 4, Epoch 44/160 (LR 0.08247) => LSC_loss 0.21, Spatial_loss 2.67, Flat_loss 0.27, Train_acc 95.61, Test_acc 56.07
2025-02-14 21:30:30,188 [podnet.py] => Task 4, Epoch 45/160 (LR 0.08172) => LSC_loss 0.21, Spatial_loss 2.68, Flat_loss 0.26, Train_acc 95.68, Test_acc 51.54
2025-02-14 21:30:32,154 [podnet.py] => Task 4, Epoch 46/160 (LR 0.08095) => LSC_loss 0.22, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 95.37, Test_acc 55.13
2025-02-14 21:30:34,118 [podnet.py] => Task 4, Epoch 47/160 (LR 0.08018) => LSC_loss 0.21, Spatial_loss 2.58, Flat_loss 0.26, Train_acc 95.61, Test_acc 56.39
2025-02-14 21:30:36,046 [podnet.py] => Task 4, Epoch 48/160 (LR 0.07939) => LSC_loss 0.21, Spatial_loss 2.66, Flat_loss 0.26, Train_acc 95.76, Test_acc 59.81
2025-02-14 21:30:38,003 [podnet.py] => Task 4, Epoch 49/160 (LR 0.07859) => LSC_loss 0.21, Spatial_loss 2.63, Flat_loss 0.26, Train_acc 95.63, Test_acc 52.24
2025-02-14 21:30:39,914 [podnet.py] => Task 4, Epoch 50/160 (LR 0.07778) => LSC_loss 0.20, Spatial_loss 2.65, Flat_loss 0.26, Train_acc 96.16, Test_acc 57.19
2025-02-14 21:30:41,839 [podnet.py] => Task 4, Epoch 51/160 (LR 0.07696) => LSC_loss 0.21, Spatial_loss 2.63, Flat_loss 0.26, Train_acc 95.66, Test_acc 55.01
2025-02-14 21:30:43,711 [podnet.py] => Task 4, Epoch 52/160 (LR 0.07612) => LSC_loss 0.21, Spatial_loss 2.64, Flat_loss 0.26, Train_acc 95.08, Test_acc 56.87
2025-02-14 21:30:45,694 [podnet.py] => Task 4, Epoch 53/160 (LR 0.07528) => LSC_loss 0.22, Spatial_loss 2.60, Flat_loss 0.26, Train_acc 95.24, Test_acc 54.30
2025-02-14 21:30:47,625 [podnet.py] => Task 4, Epoch 54/160 (LR 0.07443) => LSC_loss 0.20, Spatial_loss 2.58, Flat_loss 0.25, Train_acc 95.74, Test_acc 55.91
2025-02-14 21:30:49,588 [podnet.py] => Task 4, Epoch 55/160 (LR 0.07357) => LSC_loss 0.20, Spatial_loss 2.46, Flat_loss 0.24, Train_acc 95.97, Test_acc 55.51
2025-02-14 21:30:51,510 [podnet.py] => Task 4, Epoch 56/160 (LR 0.07270) => LSC_loss 0.21, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 95.58, Test_acc 54.30
2025-02-14 21:30:53,482 [podnet.py] => Task 4, Epoch 57/160 (LR 0.07182) => LSC_loss 0.20, Spatial_loss 2.50, Flat_loss 0.24, Train_acc 95.71, Test_acc 54.69
2025-02-14 21:30:55,429 [podnet.py] => Task 4, Epoch 58/160 (LR 0.07093) => LSC_loss 0.19, Spatial_loss 2.51, Flat_loss 0.24, Train_acc 95.95, Test_acc 53.59
2025-02-14 21:30:57,342 [podnet.py] => Task 4, Epoch 59/160 (LR 0.07004) => LSC_loss 0.21, Spatial_loss 2.51, Flat_loss 0.24, Train_acc 95.66, Test_acc 57.91
2025-02-14 21:30:59,237 [podnet.py] => Task 4, Epoch 60/160 (LR 0.06913) => LSC_loss 0.20, Spatial_loss 2.52, Flat_loss 0.24, Train_acc 96.18, Test_acc 57.84
2025-02-14 21:31:01,113 [podnet.py] => Task 4, Epoch 61/160 (LR 0.06822) => LSC_loss 0.21, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 95.68, Test_acc 51.06
2025-02-14 21:31:03,029 [podnet.py] => Task 4, Epoch 62/160 (LR 0.06731) => LSC_loss 0.21, Spatial_loss 2.52, Flat_loss 0.24, Train_acc 95.61, Test_acc 56.16
2025-02-14 21:31:04,917 [podnet.py] => Task 4, Epoch 63/160 (LR 0.06638) => LSC_loss 0.21, Spatial_loss 2.47, Flat_loss 0.24, Train_acc 95.50, Test_acc 55.56
2025-02-14 21:31:06,813 [podnet.py] => Task 4, Epoch 64/160 (LR 0.06545) => LSC_loss 0.20, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 96.24, Test_acc 55.53
2025-02-14 21:31:08,749 [podnet.py] => Task 4, Epoch 65/160 (LR 0.06451) => LSC_loss 0.20, Spatial_loss 2.41, Flat_loss 0.22, Train_acc 96.05, Test_acc 55.90
2025-02-14 21:31:10,743 [podnet.py] => Task 4, Epoch 66/160 (LR 0.06357) => LSC_loss 0.20, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 95.84, Test_acc 60.51
2025-02-14 21:31:12,668 [podnet.py] => Task 4, Epoch 67/160 (LR 0.06262) => LSC_loss 0.19, Spatial_loss 2.40, Flat_loss 0.23, Train_acc 96.47, Test_acc 57.66
2025-02-14 21:31:14,568 [podnet.py] => Task 4, Epoch 68/160 (LR 0.06167) => LSC_loss 0.19, Spatial_loss 2.32, Flat_loss 0.22, Train_acc 96.26, Test_acc 57.04
2025-02-14 21:31:16,477 [podnet.py] => Task 4, Epoch 69/160 (LR 0.06072) => LSC_loss 0.20, Spatial_loss 2.37, Flat_loss 0.23, Train_acc 95.71, Test_acc 55.80
2025-02-14 21:31:18,462 [podnet.py] => Task 4, Epoch 70/160 (LR 0.05975) => LSC_loss 0.19, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 96.32, Test_acc 55.67
2025-02-14 21:31:20,376 [podnet.py] => Task 4, Epoch 71/160 (LR 0.05879) => LSC_loss 0.19, Spatial_loss 2.36, Flat_loss 0.22, Train_acc 96.24, Test_acc 58.01
2025-02-14 21:31:22,328 [podnet.py] => Task 4, Epoch 72/160 (LR 0.05782) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.63, Test_acc 55.19
2025-02-14 21:31:24,228 [podnet.py] => Task 4, Epoch 73/160 (LR 0.05685) => LSC_loss 0.20, Spatial_loss 2.40, Flat_loss 0.22, Train_acc 96.03, Test_acc 59.63
2025-02-14 21:31:26,135 [podnet.py] => Task 4, Epoch 74/160 (LR 0.05588) => LSC_loss 0.19, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 96.21, Test_acc 56.01
2025-02-14 21:31:28,051 [podnet.py] => Task 4, Epoch 75/160 (LR 0.05490) => LSC_loss 0.19, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 96.55, Test_acc 57.60
2025-02-14 21:31:29,889 [podnet.py] => Task 4, Epoch 76/160 (LR 0.05392) => LSC_loss 0.20, Spatial_loss 2.25, Flat_loss 0.21, Train_acc 95.55, Test_acc 58.03
2025-02-14 21:31:31,835 [podnet.py] => Task 4, Epoch 77/160 (LR 0.05294) => LSC_loss 0.18, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 96.87, Test_acc 56.07
2025-02-14 21:31:33,709 [podnet.py] => Task 4, Epoch 78/160 (LR 0.05196) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.21, Test_acc 56.09
2025-02-14 21:31:35,628 [podnet.py] => Task 4, Epoch 79/160 (LR 0.05098) => LSC_loss 0.20, Spatial_loss 2.31, Flat_loss 0.22, Train_acc 95.84, Test_acc 57.99
2025-02-14 21:31:37,566 [podnet.py] => Task 4, Epoch 80/160 (LR 0.05000) => LSC_loss 0.19, Spatial_loss 2.28, Flat_loss 0.21, Train_acc 96.00, Test_acc 58.61
2025-02-14 21:31:39,497 [podnet.py] => Task 4, Epoch 81/160 (LR 0.04902) => LSC_loss 0.18, Spatial_loss 2.24, Flat_loss 0.20, Train_acc 96.50, Test_acc 54.50
2025-02-14 21:31:41,432 [podnet.py] => Task 4, Epoch 82/160 (LR 0.04804) => LSC_loss 0.19, Spatial_loss 2.30, Flat_loss 0.21, Train_acc 96.03, Test_acc 57.07
2025-02-14 21:31:43,340 [podnet.py] => Task 4, Epoch 83/160 (LR 0.04706) => LSC_loss 0.19, Spatial_loss 2.23, Flat_loss 0.20, Train_acc 96.61, Test_acc 59.07
2025-02-14 21:31:45,267 [podnet.py] => Task 4, Epoch 84/160 (LR 0.04608) => LSC_loss 0.19, Spatial_loss 2.20, Flat_loss 0.20, Train_acc 96.13, Test_acc 59.87
2025-02-14 21:31:47,221 [podnet.py] => Task 4, Epoch 85/160 (LR 0.04510) => LSC_loss 0.18, Spatial_loss 2.21, Flat_loss 0.20, Train_acc 96.55, Test_acc 55.10
2025-02-14 21:31:49,127 [podnet.py] => Task 4, Epoch 86/160 (LR 0.04412) => LSC_loss 0.20, Spatial_loss 2.16, Flat_loss 0.19, Train_acc 96.18, Test_acc 56.33
2025-02-14 21:31:51,026 [podnet.py] => Task 4, Epoch 87/160 (LR 0.04315) => LSC_loss 0.18, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 97.08, Test_acc 57.86
2025-02-14 21:31:52,927 [podnet.py] => Task 4, Epoch 88/160 (LR 0.04218) => LSC_loss 0.19, Spatial_loss 2.11, Flat_loss 0.19, Train_acc 96.34, Test_acc 56.83
2025-02-14 21:31:54,887 [podnet.py] => Task 4, Epoch 89/160 (LR 0.04121) => LSC_loss 0.19, Spatial_loss 2.15, Flat_loss 0.19, Train_acc 96.24, Test_acc 57.21
2025-02-14 21:31:56,795 [podnet.py] => Task 4, Epoch 90/160 (LR 0.04025) => LSC_loss 0.19, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 96.34, Test_acc 58.20
2025-02-14 21:31:58,781 [podnet.py] => Task 4, Epoch 91/160 (LR 0.03928) => LSC_loss 0.18, Spatial_loss 2.08, Flat_loss 0.19, Train_acc 96.84, Test_acc 60.99
2025-02-14 21:32:00,720 [podnet.py] => Task 4, Epoch 92/160 (LR 0.03833) => LSC_loss 0.19, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 96.16, Test_acc 56.49
2025-02-14 21:32:02,629 [podnet.py] => Task 4, Epoch 93/160 (LR 0.03738) => LSC_loss 0.19, Spatial_loss 2.01, Flat_loss 0.18, Train_acc 96.45, Test_acc 58.97
2025-02-14 21:32:04,617 [podnet.py] => Task 4, Epoch 94/160 (LR 0.03643) => LSC_loss 0.19, Spatial_loss 2.02, Flat_loss 0.19, Train_acc 95.89, Test_acc 59.14
2025-02-14 21:32:06,616 [podnet.py] => Task 4, Epoch 95/160 (LR 0.03549) => LSC_loss 0.18, Spatial_loss 2.04, Flat_loss 0.18, Train_acc 96.66, Test_acc 60.00
2025-02-14 21:32:08,468 [podnet.py] => Task 4, Epoch 96/160 (LR 0.03455) => LSC_loss 0.19, Spatial_loss 2.03, Flat_loss 0.18, Train_acc 96.16, Test_acc 59.81
2025-02-14 21:32:10,444 [podnet.py] => Task 4, Epoch 97/160 (LR 0.03362) => LSC_loss 0.19, Spatial_loss 2.01, Flat_loss 0.18, Train_acc 96.32, Test_acc 58.41
2025-02-14 21:32:12,386 [podnet.py] => Task 4, Epoch 98/160 (LR 0.03269) => LSC_loss 0.18, Spatial_loss 1.99, Flat_loss 0.18, Train_acc 96.71, Test_acc 59.51
2025-02-14 21:32:14,333 [podnet.py] => Task 4, Epoch 99/160 (LR 0.03178) => LSC_loss 0.17, Spatial_loss 1.98, Flat_loss 0.17, Train_acc 96.92, Test_acc 59.50
2025-02-14 21:32:16,261 [podnet.py] => Task 4, Epoch 100/160 (LR 0.03087) => LSC_loss 0.18, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 96.53, Test_acc 59.69
2025-02-14 21:32:18,295 [podnet.py] => Task 4, Epoch 101/160 (LR 0.02996) => LSC_loss 0.19, Spatial_loss 1.93, Flat_loss 0.17, Train_acc 96.37, Test_acc 58.10
2025-02-14 21:32:20,209 [podnet.py] => Task 4, Epoch 102/160 (LR 0.02907) => LSC_loss 0.18, Spatial_loss 1.97, Flat_loss 0.17, Train_acc 96.84, Test_acc 58.40
2025-02-14 21:32:22,166 [podnet.py] => Task 4, Epoch 103/160 (LR 0.02818) => LSC_loss 0.18, Spatial_loss 1.99, Flat_loss 0.17, Train_acc 96.58, Test_acc 57.80
2025-02-14 21:32:24,096 [podnet.py] => Task 4, Epoch 104/160 (LR 0.02730) => LSC_loss 0.18, Spatial_loss 1.90, Flat_loss 0.17, Train_acc 96.61, Test_acc 59.27
2025-02-14 21:32:26,012 [podnet.py] => Task 4, Epoch 105/160 (LR 0.02643) => LSC_loss 0.18, Spatial_loss 1.89, Flat_loss 0.17, Train_acc 96.87, Test_acc 60.60
2025-02-14 21:32:27,921 [podnet.py] => Task 4, Epoch 106/160 (LR 0.02557) => LSC_loss 0.19, Spatial_loss 1.83, Flat_loss 0.16, Train_acc 96.53, Test_acc 59.60
2025-02-14 21:32:29,873 [podnet.py] => Task 4, Epoch 107/160 (LR 0.02472) => LSC_loss 0.18, Spatial_loss 1.80, Flat_loss 0.15, Train_acc 96.89, Test_acc 59.27
2025-02-14 21:32:31,761 [podnet.py] => Task 4, Epoch 108/160 (LR 0.02388) => LSC_loss 0.17, Spatial_loss 1.82, Flat_loss 0.16, Train_acc 97.08, Test_acc 60.11
2025-02-14 21:32:33,664 [podnet.py] => Task 4, Epoch 109/160 (LR 0.02304) => LSC_loss 0.18, Spatial_loss 1.82, Flat_loss 0.16, Train_acc 97.37, Test_acc 58.80
2025-02-14 21:32:35,531 [podnet.py] => Task 4, Epoch 110/160 (LR 0.02222) => LSC_loss 0.18, Spatial_loss 1.73, Flat_loss 0.15, Train_acc 96.87, Test_acc 59.16
2025-02-14 21:32:37,484 [podnet.py] => Task 4, Epoch 111/160 (LR 0.02141) => LSC_loss 0.18, Spatial_loss 1.77, Flat_loss 0.15, Train_acc 97.11, Test_acc 57.74
2025-02-14 21:32:39,398 [podnet.py] => Task 4, Epoch 112/160 (LR 0.02061) => LSC_loss 0.18, Spatial_loss 1.74, Flat_loss 0.15, Train_acc 97.00, Test_acc 59.39
2025-02-14 21:32:41,292 [podnet.py] => Task 4, Epoch 113/160 (LR 0.01982) => LSC_loss 0.18, Spatial_loss 1.75, Flat_loss 0.15, Train_acc 96.61, Test_acc 57.46
2025-02-14 21:32:43,225 [podnet.py] => Task 4, Epoch 114/160 (LR 0.01905) => LSC_loss 0.18, Spatial_loss 1.79, Flat_loss 0.15, Train_acc 96.71, Test_acc 58.90
2025-02-14 21:32:45,122 [podnet.py] => Task 4, Epoch 115/160 (LR 0.01828) => LSC_loss 0.18, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 96.89, Test_acc 61.37
2025-02-14 21:32:47,099 [podnet.py] => Task 4, Epoch 116/160 (LR 0.01753) => LSC_loss 0.18, Spatial_loss 1.70, Flat_loss 0.15, Train_acc 96.74, Test_acc 57.94
2025-02-14 21:32:49,064 [podnet.py] => Task 4, Epoch 117/160 (LR 0.01679) => LSC_loss 0.19, Spatial_loss 1.69, Flat_loss 0.15, Train_acc 96.74, Test_acc 58.34
2025-02-14 21:32:51,002 [podnet.py] => Task 4, Epoch 118/160 (LR 0.01606) => LSC_loss 0.19, Spatial_loss 1.74, Flat_loss 0.15, Train_acc 96.47, Test_acc 59.81
2025-02-14 21:32:52,899 [podnet.py] => Task 4, Epoch 119/160 (LR 0.01535) => LSC_loss 0.18, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 96.87, Test_acc 61.56
2025-02-14 21:32:54,821 [podnet.py] => Task 4, Epoch 120/160 (LR 0.01464) => LSC_loss 0.18, Spatial_loss 1.66, Flat_loss 0.14, Train_acc 97.00, Test_acc 60.51
2025-02-14 21:32:56,750 [podnet.py] => Task 4, Epoch 121/160 (LR 0.01396) => LSC_loss 0.18, Spatial_loss 1.65, Flat_loss 0.14, Train_acc 97.29, Test_acc 59.47
2025-02-14 21:32:58,628 [podnet.py] => Task 4, Epoch 122/160 (LR 0.01328) => LSC_loss 0.17, Spatial_loss 1.58, Flat_loss 0.14, Train_acc 97.34, Test_acc 59.81
2025-02-14 21:33:00,514 [podnet.py] => Task 4, Epoch 123/160 (LR 0.01262) => LSC_loss 0.18, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 97.11, Test_acc 61.09
2025-02-14 21:33:02,448 [podnet.py] => Task 4, Epoch 124/160 (LR 0.01198) => LSC_loss 0.19, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 96.79, Test_acc 59.69
2025-02-14 21:33:04,360 [podnet.py] => Task 4, Epoch 125/160 (LR 0.01135) => LSC_loss 0.18, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 96.95, Test_acc 59.97
2025-02-14 21:33:06,260 [podnet.py] => Task 4, Epoch 126/160 (LR 0.01073) => LSC_loss 0.18, Spatial_loss 1.52, Flat_loss 0.13, Train_acc 96.71, Test_acc 59.81
2025-02-14 21:33:08,148 [podnet.py] => Task 4, Epoch 127/160 (LR 0.01013) => LSC_loss 0.18, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 96.53, Test_acc 61.17
2025-02-14 21:33:10,110 [podnet.py] => Task 4, Epoch 128/160 (LR 0.00955) => LSC_loss 0.18, Spatial_loss 1.52, Flat_loss 0.13, Train_acc 96.84, Test_acc 61.31
2025-02-14 21:33:11,974 [podnet.py] => Task 4, Epoch 129/160 (LR 0.00898) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 97.08, Test_acc 60.71
2025-02-14 21:33:13,890 [podnet.py] => Task 4, Epoch 130/160 (LR 0.00843) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 97.00, Test_acc 60.09
2025-02-14 21:33:15,744 [podnet.py] => Task 4, Epoch 131/160 (LR 0.00789) => LSC_loss 0.18, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 96.84, Test_acc 60.13
2025-02-14 21:33:17,697 [podnet.py] => Task 4, Epoch 132/160 (LR 0.00737) => LSC_loss 0.17, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 97.16, Test_acc 60.66
2025-02-14 21:33:19,654 [podnet.py] => Task 4, Epoch 133/160 (LR 0.00686) => LSC_loss 0.18, Spatial_loss 1.49, Flat_loss 0.13, Train_acc 96.66, Test_acc 60.41
2025-02-14 21:33:21,574 [podnet.py] => Task 4, Epoch 134/160 (LR 0.00638) => LSC_loss 0.18, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 96.95, Test_acc 60.51
2025-02-14 21:33:23,430 [podnet.py] => Task 4, Epoch 135/160 (LR 0.00590) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 96.79, Test_acc 60.41
2025-02-14 21:33:25,340 [podnet.py] => Task 4, Epoch 136/160 (LR 0.00545) => LSC_loss 0.18, Spatial_loss 1.38, Flat_loss 0.12, Train_acc 97.05, Test_acc 60.93
2025-02-14 21:33:27,282 [podnet.py] => Task 4, Epoch 137/160 (LR 0.00501) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.12, Train_acc 96.50, Test_acc 61.01
2025-02-14 21:33:29,185 [podnet.py] => Task 4, Epoch 138/160 (LR 0.00459) => LSC_loss 0.18, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 96.68, Test_acc 60.73
2025-02-14 21:33:31,104 [podnet.py] => Task 4, Epoch 139/160 (LR 0.00419) => LSC_loss 0.19, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 96.87, Test_acc 60.19
2025-02-14 21:33:33,040 [podnet.py] => Task 4, Epoch 140/160 (LR 0.00381) => LSC_loss 0.18, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 97.16, Test_acc 61.14
2025-02-14 21:33:34,913 [podnet.py] => Task 4, Epoch 141/160 (LR 0.00344) => LSC_loss 0.18, Spatial_loss 1.42, Flat_loss 0.12, Train_acc 97.13, Test_acc 59.90
2025-02-14 21:33:36,848 [podnet.py] => Task 4, Epoch 142/160 (LR 0.00309) => LSC_loss 0.18, Spatial_loss 1.36, Flat_loss 0.12, Train_acc 97.16, Test_acc 61.51
2025-02-14 21:33:38,761 [podnet.py] => Task 4, Epoch 143/160 (LR 0.00276) => LSC_loss 0.18, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 97.08, Test_acc 60.99
2025-02-14 21:33:40,692 [podnet.py] => Task 4, Epoch 144/160 (LR 0.00245) => LSC_loss 0.18, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 96.84, Test_acc 60.71
2025-02-14 21:33:42,639 [podnet.py] => Task 4, Epoch 145/160 (LR 0.00215) => LSC_loss 0.18, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 96.84, Test_acc 60.99
2025-02-14 21:33:44,576 [podnet.py] => Task 4, Epoch 146/160 (LR 0.00188) => LSC_loss 0.18, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 96.82, Test_acc 61.20
2025-02-14 21:33:46,510 [podnet.py] => Task 4, Epoch 147/160 (LR 0.00162) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.08, Test_acc 61.17
2025-02-14 21:33:48,452 [podnet.py] => Task 4, Epoch 148/160 (LR 0.00138) => LSC_loss 0.17, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 97.26, Test_acc 61.53
2025-02-14 21:33:50,355 [podnet.py] => Task 4, Epoch 149/160 (LR 0.00116) => LSC_loss 0.17, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 97.50, Test_acc 61.26
2025-02-14 21:33:52,298 [podnet.py] => Task 4, Epoch 150/160 (LR 0.00096) => LSC_loss 0.18, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 97.00, Test_acc 61.04
2025-02-14 21:33:54,209 [podnet.py] => Task 4, Epoch 151/160 (LR 0.00078) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.55, Test_acc 61.43
2025-02-14 21:33:56,114 [podnet.py] => Task 4, Epoch 152/160 (LR 0.00062) => LSC_loss 0.17, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 97.34, Test_acc 61.40
2025-02-14 21:33:58,053 [podnet.py] => Task 4, Epoch 153/160 (LR 0.00047) => LSC_loss 0.17, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 97.05, Test_acc 61.47
2025-02-14 21:33:59,992 [podnet.py] => Task 4, Epoch 154/160 (LR 0.00035) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.97, Test_acc 61.14
2025-02-14 21:34:01,857 [podnet.py] => Task 4, Epoch 155/160 (LR 0.00024) => LSC_loss 0.18, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 96.76, Test_acc 61.13
2025-02-14 21:34:03,743 [podnet.py] => Task 4, Epoch 156/160 (LR 0.00015) => LSC_loss 0.18, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 96.55, Test_acc 61.06
2025-02-14 21:34:05,606 [podnet.py] => Task 4, Epoch 157/160 (LR 0.00009) => LSC_loss 0.18, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 96.42, Test_acc 61.19
2025-02-14 21:34:07,540 [podnet.py] => Task 4, Epoch 158/160 (LR 0.00004) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.03, Test_acc 61.51
2025-02-14 21:34:09,482 [podnet.py] => Task 4, Epoch 159/160 (LR 0.00001) => LSC_loss 0.18, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 97.05, Test_acc 61.20
2025-02-14 21:34:11,402 [podnet.py] => Task 4, Epoch 160/160 (LR 0.00000) => LSC_loss 0.18, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 97.21, Test_acc 61.57
2025-02-14 21:34:11,403 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:34:11,403 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:34:35,063 [podnet.py] => The size of finetune dataset: 1400
2025-02-14 21:34:36,482 [podnet.py] => Task 4, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 1.66, Flat_loss 0.14, Train_acc 95.71, Test_acc 62.41
2025-02-14 21:34:37,875 [podnet.py] => Task 4, Epoch 2/20 (LR 0.00488) => LSC_loss 0.10, Spatial_loss 1.41, Flat_loss 0.08, Train_acc 98.64, Test_acc 62.06
2025-02-14 21:34:39,246 [podnet.py] => Task 4, Epoch 3/20 (LR 0.00473) => LSC_loss 0.08, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.93, Test_acc 62.50
2025-02-14 21:34:40,655 [podnet.py] => Task 4, Epoch 4/20 (LR 0.00452) => LSC_loss 0.08, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 99.00, Test_acc 63.54
2025-02-14 21:34:42,001 [podnet.py] => Task 4, Epoch 5/20 (LR 0.00427) => LSC_loss 0.09, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.93, Test_acc 63.23
2025-02-14 21:34:43,351 [podnet.py] => Task 4, Epoch 6/20 (LR 0.00397) => LSC_loss 0.07, Spatial_loss 1.21, Flat_loss 0.05, Train_acc 99.57, Test_acc 63.23
2025-02-14 21:34:44,696 [podnet.py] => Task 4, Epoch 7/20 (LR 0.00363) => LSC_loss 0.09, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 99.07, Test_acc 63.29
2025-02-14 21:34:46,120 [podnet.py] => Task 4, Epoch 8/20 (LR 0.00327) => LSC_loss 0.08, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 99.29, Test_acc 63.30
2025-02-14 21:34:47,501 [podnet.py] => Task 4, Epoch 9/20 (LR 0.00289) => LSC_loss 0.08, Spatial_loss 1.17, Flat_loss 0.05, Train_acc 98.86, Test_acc 63.37
2025-02-14 21:34:48,925 [podnet.py] => Task 4, Epoch 10/20 (LR 0.00250) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.71, Test_acc 63.24
2025-02-14 21:34:50,265 [podnet.py] => Task 4, Epoch 11/20 (LR 0.00211) => LSC_loss 0.08, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 99.21, Test_acc 63.41
2025-02-14 21:34:51,673 [podnet.py] => Task 4, Epoch 12/20 (LR 0.00173) => LSC_loss 0.07, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 99.43, Test_acc 63.59
2025-02-14 21:34:53,005 [podnet.py] => Task 4, Epoch 13/20 (LR 0.00137) => LSC_loss 0.08, Spatial_loss 1.33, Flat_loss 0.05, Train_acc 99.36, Test_acc 63.03
2025-02-14 21:34:54,340 [podnet.py] => Task 4, Epoch 14/20 (LR 0.00103) => LSC_loss 0.07, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 99.21, Test_acc 63.69
2025-02-14 21:34:55,725 [podnet.py] => Task 4, Epoch 15/20 (LR 0.00073) => LSC_loss 0.08, Spatial_loss 1.15, Flat_loss 0.05, Train_acc 98.93, Test_acc 63.39
2025-02-14 21:34:57,071 [podnet.py] => Task 4, Epoch 16/20 (LR 0.00048) => LSC_loss 0.07, Spatial_loss 1.16, Flat_loss 0.05, Train_acc 99.00, Test_acc 63.44
2025-02-14 21:34:58,458 [podnet.py] => Task 4, Epoch 17/20 (LR 0.00027) => LSC_loss 0.08, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 99.29, Test_acc 63.31
2025-02-14 21:34:59,789 [podnet.py] => Task 4, Epoch 18/20 (LR 0.00012) => LSC_loss 0.08, Spatial_loss 1.17, Flat_loss 0.05, Train_acc 99.36, Test_acc 63.39
2025-02-14 21:35:01,189 [podnet.py] => Task 4, Epoch 19/20 (LR 0.00003) => LSC_loss 0.08, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.86, Test_acc 63.57
2025-02-14 21:35:02,551 [podnet.py] => Task 4, Epoch 20/20 (LR 0.00000) => LSC_loss 0.08, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 99.43, Test_acc 63.51
2025-02-14 21:35:02,552 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:35:28,231 [podnet.py] => Exemplar size: 1400
2025-02-14 21:35:28,231 [trainer.py] => CNN: {'total': 63.51, '00-09': 70.5, '10-19': 54.7, '20-29': 71.6, '30-39': 62.8, '40-49': 67.4, '50-59': 52.5, '60-69': 65.1, 'old': 62.91, 'new': 71.4}
2025-02-14 21:35:28,231 [trainer.py] => NME: {'total': 64.09, '00-09': 74.1, '10-19': 60.8, '20-29': 73.1, '30-39': 64.8, '40-49': 68.8, '50-59': 47.8, '60-69': 59.2, 'old': 64.09, 'new': 64.0}
2025-02-14 21:35:28,231 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51]
2025-02-14 21:35:28,231 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67]
2025-02-14 21:35:28,231 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09]
2025-02-14 21:35:28,231 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2]

2025-02-14 21:35:28,231 [trainer.py] => Average Accuracy (CNN): 70.10799999999999
2025-02-14 21:35:28,231 [trainer.py] => Average Accuracy (NME): 70.066
2025-02-14 21:35:28,232 [trainer.py] => All params: 511057
2025-02-14 21:35:28,232 [trainer.py] => Trainable params: 511057
2025-02-14 21:35:28,233 [podnet.py] => Learning on 70-75
2025-02-14 21:35:28,262 [podnet.py] => Adaptive factor: 3.872983346207417
2025-02-14 21:35:30,307 [podnet.py] => Task 5, Epoch 1/160 (LR 0.09999) => LSC_loss 1.73, Spatial_loss 4.16, Flat_loss 0.84, Train_acc 67.31, Test_acc 42.76
2025-02-14 21:35:32,326 [podnet.py] => Task 5, Epoch 2/160 (LR 0.09996) => LSC_loss 0.70, Spatial_loss 4.28, Flat_loss 0.68, Train_acc 80.79, Test_acc 46.36
2025-02-14 21:35:34,266 [podnet.py] => Task 5, Epoch 3/160 (LR 0.09991) => LSC_loss 0.57, Spatial_loss 3.94, Flat_loss 0.56, Train_acc 84.38, Test_acc 49.39
2025-02-14 21:35:36,208 [podnet.py] => Task 5, Epoch 4/160 (LR 0.09985) => LSC_loss 0.50, Spatial_loss 3.65, Flat_loss 0.49, Train_acc 86.72, Test_acc 48.96
2025-02-14 21:35:38,198 [podnet.py] => Task 5, Epoch 5/160 (LR 0.09976) => LSC_loss 0.45, Spatial_loss 3.51, Flat_loss 0.44, Train_acc 88.74, Test_acc 49.61
2025-02-14 21:35:40,251 [podnet.py] => Task 5, Epoch 6/160 (LR 0.09965) => LSC_loss 0.45, Spatial_loss 3.38, Flat_loss 0.42, Train_acc 88.49, Test_acc 45.81
2025-02-14 21:35:42,275 [podnet.py] => Task 5, Epoch 7/160 (LR 0.09953) => LSC_loss 0.41, Spatial_loss 3.41, Flat_loss 0.40, Train_acc 89.69, Test_acc 47.59
2025-02-14 21:35:44,283 [podnet.py] => Task 5, Epoch 8/160 (LR 0.09938) => LSC_loss 0.40, Spatial_loss 3.38, Flat_loss 0.39, Train_acc 89.97, Test_acc 49.95
2025-02-14 21:35:46,235 [podnet.py] => Task 5, Epoch 9/160 (LR 0.09922) => LSC_loss 0.38, Spatial_loss 3.30, Flat_loss 0.37, Train_acc 91.15, Test_acc 51.43
2025-02-14 21:35:48,239 [podnet.py] => Task 5, Epoch 10/160 (LR 0.09904) => LSC_loss 0.37, Spatial_loss 3.16, Flat_loss 0.35, Train_acc 90.77, Test_acc 53.05
2025-02-14 21:35:50,260 [podnet.py] => Task 5, Epoch 11/160 (LR 0.09884) => LSC_loss 0.36, Spatial_loss 3.25, Flat_loss 0.36, Train_acc 91.46, Test_acc 49.43
2025-02-14 21:35:52,251 [podnet.py] => Task 5, Epoch 12/160 (LR 0.09862) => LSC_loss 0.35, Spatial_loss 3.12, Flat_loss 0.34, Train_acc 92.00, Test_acc 51.21
2025-02-14 21:35:54,224 [podnet.py] => Task 5, Epoch 13/160 (LR 0.09838) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.15, Test_acc 46.69
2025-02-14 21:35:56,230 [podnet.py] => Task 5, Epoch 14/160 (LR 0.09812) => LSC_loss 0.36, Spatial_loss 3.18, Flat_loss 0.35, Train_acc 91.15, Test_acc 48.71
2025-02-14 21:35:58,166 [podnet.py] => Task 5, Epoch 15/160 (LR 0.09785) => LSC_loss 0.34, Spatial_loss 3.13, Flat_loss 0.34, Train_acc 92.15, Test_acc 49.69
2025-02-14 21:36:00,132 [podnet.py] => Task 5, Epoch 16/160 (LR 0.09755) => LSC_loss 0.34, Spatial_loss 3.08, Flat_loss 0.34, Train_acc 91.95, Test_acc 52.60
2025-02-14 21:36:02,151 [podnet.py] => Task 5, Epoch 17/160 (LR 0.09724) => LSC_loss 0.33, Spatial_loss 3.04, Flat_loss 0.32, Train_acc 92.51, Test_acc 51.91
2025-02-14 21:36:04,165 [podnet.py] => Task 5, Epoch 18/160 (LR 0.09691) => LSC_loss 0.32, Spatial_loss 2.92, Flat_loss 0.30, Train_acc 92.67, Test_acc 52.55
2025-02-14 21:36:06,153 [podnet.py] => Task 5, Epoch 19/160 (LR 0.09656) => LSC_loss 0.32, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.74, Test_acc 47.71
2025-02-14 21:36:08,157 [podnet.py] => Task 5, Epoch 20/160 (LR 0.09619) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.34, Train_acc 92.41, Test_acc 43.76
2025-02-14 21:36:10,222 [podnet.py] => Task 5, Epoch 21/160 (LR 0.09581) => LSC_loss 0.32, Spatial_loss 3.09, Flat_loss 0.33, Train_acc 92.74, Test_acc 51.81
2025-02-14 21:36:12,196 [podnet.py] => Task 5, Epoch 22/160 (LR 0.09541) => LSC_loss 0.32, Spatial_loss 3.19, Flat_loss 0.34, Train_acc 92.36, Test_acc 52.75
2025-02-14 21:36:14,212 [podnet.py] => Task 5, Epoch 23/160 (LR 0.09499) => LSC_loss 0.30, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 93.13, Test_acc 46.40
2025-02-14 21:36:16,200 [podnet.py] => Task 5, Epoch 24/160 (LR 0.09455) => LSC_loss 0.31, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 92.67, Test_acc 52.23
2025-02-14 21:36:18,221 [podnet.py] => Task 5, Epoch 25/160 (LR 0.09410) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 93.10, Test_acc 52.13
2025-02-14 21:36:20,229 [podnet.py] => Task 5, Epoch 26/160 (LR 0.09362) => LSC_loss 0.30, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 93.23, Test_acc 47.68
2025-02-14 21:36:22,169 [podnet.py] => Task 5, Epoch 27/160 (LR 0.09314) => LSC_loss 0.29, Spatial_loss 2.91, Flat_loss 0.30, Train_acc 93.74, Test_acc 53.97
2025-02-14 21:36:24,171 [podnet.py] => Task 5, Epoch 28/160 (LR 0.09263) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 93.79, Test_acc 50.80
2025-02-14 21:36:26,062 [podnet.py] => Task 5, Epoch 29/160 (LR 0.09211) => LSC_loss 0.29, Spatial_loss 2.86, Flat_loss 0.29, Train_acc 93.90, Test_acc 52.53
2025-02-14 21:36:28,054 [podnet.py] => Task 5, Epoch 30/160 (LR 0.09157) => LSC_loss 0.30, Spatial_loss 2.83, Flat_loss 0.30, Train_acc 93.15, Test_acc 54.61
2025-02-14 21:36:30,055 [podnet.py] => Task 5, Epoch 31/160 (LR 0.09102) => LSC_loss 0.30, Spatial_loss 2.88, Flat_loss 0.30, Train_acc 92.59, Test_acc 50.75
2025-02-14 21:36:32,052 [podnet.py] => Task 5, Epoch 32/160 (LR 0.09045) => LSC_loss 0.30, Spatial_loss 2.97, Flat_loss 0.31, Train_acc 92.77, Test_acc 55.01
2025-02-14 21:36:34,017 [podnet.py] => Task 5, Epoch 33/160 (LR 0.08987) => LSC_loss 0.30, Spatial_loss 3.00, Flat_loss 0.31, Train_acc 93.00, Test_acc 52.39
2025-02-14 21:36:36,002 [podnet.py] => Task 5, Epoch 34/160 (LR 0.08927) => LSC_loss 0.28, Spatial_loss 2.99, Flat_loss 0.31, Train_acc 94.00, Test_acc 52.57
2025-02-14 21:36:38,022 [podnet.py] => Task 5, Epoch 35/160 (LR 0.08865) => LSC_loss 0.28, Spatial_loss 2.84, Flat_loss 0.29, Train_acc 93.95, Test_acc 54.16
2025-02-14 21:36:39,961 [podnet.py] => Task 5, Epoch 36/160 (LR 0.08802) => LSC_loss 0.30, Spatial_loss 2.93, Flat_loss 0.30, Train_acc 92.85, Test_acc 51.83
2025-02-14 21:36:42,000 [podnet.py] => Task 5, Epoch 37/160 (LR 0.08738) => LSC_loss 0.28, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 93.59, Test_acc 54.00
2025-02-14 21:36:43,947 [podnet.py] => Task 5, Epoch 38/160 (LR 0.08672) => LSC_loss 0.31, Spatial_loss 2.99, Flat_loss 0.32, Train_acc 92.62, Test_acc 50.80
2025-02-14 21:36:45,879 [podnet.py] => Task 5, Epoch 39/160 (LR 0.08604) => LSC_loss 0.30, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 92.44, Test_acc 48.24
2025-02-14 21:36:47,815 [podnet.py] => Task 5, Epoch 40/160 (LR 0.08536) => LSC_loss 0.28, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 93.26, Test_acc 53.41
2025-02-14 21:36:49,770 [podnet.py] => Task 5, Epoch 41/160 (LR 0.08465) => LSC_loss 0.27, Spatial_loss 2.79, Flat_loss 0.29, Train_acc 93.46, Test_acc 53.09
2025-02-14 21:36:51,740 [podnet.py] => Task 5, Epoch 42/160 (LR 0.08394) => LSC_loss 0.28, Spatial_loss 2.78, Flat_loss 0.29, Train_acc 94.46, Test_acc 48.64
2025-02-14 21:36:53,711 [podnet.py] => Task 5, Epoch 43/160 (LR 0.08321) => LSC_loss 0.28, Spatial_loss 2.79, Flat_loss 0.29, Train_acc 93.77, Test_acc 46.85
2025-02-14 21:36:55,649 [podnet.py] => Task 5, Epoch 44/160 (LR 0.08247) => LSC_loss 0.26, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 94.51, Test_acc 52.44
2025-02-14 21:36:57,626 [podnet.py] => Task 5, Epoch 45/160 (LR 0.08172) => LSC_loss 0.27, Spatial_loss 2.76, Flat_loss 0.28, Train_acc 93.97, Test_acc 53.60
2025-02-14 21:36:59,583 [podnet.py] => Task 5, Epoch 46/160 (LR 0.08095) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.27, Train_acc 94.38, Test_acc 53.25
2025-02-14 21:37:01,592 [podnet.py] => Task 5, Epoch 47/160 (LR 0.08018) => LSC_loss 0.26, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 94.59, Test_acc 52.63
2025-02-14 21:37:03,615 [podnet.py] => Task 5, Epoch 48/160 (LR 0.07939) => LSC_loss 0.28, Spatial_loss 2.61, Flat_loss 0.27, Train_acc 94.00, Test_acc 55.39
2025-02-14 21:37:05,570 [podnet.py] => Task 5, Epoch 49/160 (LR 0.07859) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.28, Train_acc 94.08, Test_acc 51.44
2025-02-14 21:37:07,564 [podnet.py] => Task 5, Epoch 50/160 (LR 0.07778) => LSC_loss 0.27, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 94.36, Test_acc 53.19
2025-02-14 21:37:09,518 [podnet.py] => Task 5, Epoch 51/160 (LR 0.07696) => LSC_loss 0.26, Spatial_loss 2.64, Flat_loss 0.26, Train_acc 94.54, Test_acc 52.83
2025-02-14 21:37:11,518 [podnet.py] => Task 5, Epoch 52/160 (LR 0.07612) => LSC_loss 0.26, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.56, Test_acc 48.73
2025-02-14 21:37:13,539 [podnet.py] => Task 5, Epoch 53/160 (LR 0.07528) => LSC_loss 0.25, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.97, Test_acc 55.71
2025-02-14 21:37:15,492 [podnet.py] => Task 5, Epoch 54/160 (LR 0.07443) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.26, Train_acc 94.03, Test_acc 52.03
2025-02-14 21:37:17,467 [podnet.py] => Task 5, Epoch 55/160 (LR 0.07357) => LSC_loss 0.27, Spatial_loss 2.71, Flat_loss 0.26, Train_acc 94.54, Test_acc 51.95
2025-02-14 21:37:19,548 [podnet.py] => Task 5, Epoch 56/160 (LR 0.07270) => LSC_loss 0.27, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.64, Test_acc 50.27
2025-02-14 21:37:21,478 [podnet.py] => Task 5, Epoch 57/160 (LR 0.07182) => LSC_loss 0.26, Spatial_loss 2.65, Flat_loss 0.26, Train_acc 93.90, Test_acc 50.91
2025-02-14 21:37:23,459 [podnet.py] => Task 5, Epoch 58/160 (LR 0.07093) => LSC_loss 0.28, Spatial_loss 2.66, Flat_loss 0.27, Train_acc 93.67, Test_acc 50.87
2025-02-14 21:37:25,424 [podnet.py] => Task 5, Epoch 59/160 (LR 0.07004) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.26, Train_acc 94.05, Test_acc 50.91
2025-02-14 21:37:27,468 [podnet.py] => Task 5, Epoch 60/160 (LR 0.06913) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 94.56, Test_acc 52.68
2025-02-14 21:37:29,452 [podnet.py] => Task 5, Epoch 61/160 (LR 0.06822) => LSC_loss 0.25, Spatial_loss 2.55, Flat_loss 0.26, Train_acc 94.23, Test_acc 55.63
2025-02-14 21:37:31,387 [podnet.py] => Task 5, Epoch 62/160 (LR 0.06731) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.33, Test_acc 50.43
2025-02-14 21:37:33,349 [podnet.py] => Task 5, Epoch 63/160 (LR 0.06638) => LSC_loss 0.25, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 94.26, Test_acc 51.87
2025-02-14 21:37:35,338 [podnet.py] => Task 5, Epoch 64/160 (LR 0.06545) => LSC_loss 0.26, Spatial_loss 2.54, Flat_loss 0.25, Train_acc 94.10, Test_acc 52.32
2025-02-14 21:37:37,298 [podnet.py] => Task 5, Epoch 65/160 (LR 0.06451) => LSC_loss 0.26, Spatial_loss 2.56, Flat_loss 0.25, Train_acc 94.15, Test_acc 53.36
2025-02-14 21:37:39,318 [podnet.py] => Task 5, Epoch 66/160 (LR 0.06357) => LSC_loss 0.25, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 95.23, Test_acc 55.32
2025-02-14 21:37:41,305 [podnet.py] => Task 5, Epoch 67/160 (LR 0.06262) => LSC_loss 0.25, Spatial_loss 2.54, Flat_loss 0.24, Train_acc 94.79, Test_acc 54.43
2025-02-14 21:37:43,294 [podnet.py] => Task 5, Epoch 68/160 (LR 0.06167) => LSC_loss 0.26, Spatial_loss 2.46, Flat_loss 0.24, Train_acc 94.72, Test_acc 55.73
2025-02-14 21:37:45,268 [podnet.py] => Task 5, Epoch 69/160 (LR 0.06072) => LSC_loss 0.26, Spatial_loss 2.58, Flat_loss 0.25, Train_acc 94.33, Test_acc 52.93
2025-02-14 21:37:47,227 [podnet.py] => Task 5, Epoch 70/160 (LR 0.05975) => LSC_loss 0.25, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 94.95, Test_acc 50.35
2025-02-14 21:37:49,191 [podnet.py] => Task 5, Epoch 71/160 (LR 0.05879) => LSC_loss 0.24, Spatial_loss 2.42, Flat_loss 0.24, Train_acc 95.33, Test_acc 51.31
2025-02-14 21:37:51,214 [podnet.py] => Task 5, Epoch 72/160 (LR 0.05782) => LSC_loss 0.26, Spatial_loss 2.45, Flat_loss 0.24, Train_acc 94.05, Test_acc 52.83
2025-02-14 21:37:53,155 [podnet.py] => Task 5, Epoch 73/160 (LR 0.05685) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.23, Train_acc 94.46, Test_acc 52.36
2025-02-14 21:37:55,158 [podnet.py] => Task 5, Epoch 74/160 (LR 0.05588) => LSC_loss 0.25, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 94.79, Test_acc 54.79
2025-02-14 21:37:57,187 [podnet.py] => Task 5, Epoch 75/160 (LR 0.05490) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.24, Train_acc 94.28, Test_acc 56.33
2025-02-14 21:37:59,175 [podnet.py] => Task 5, Epoch 76/160 (LR 0.05392) => LSC_loss 0.25, Spatial_loss 2.38, Flat_loss 0.23, Train_acc 94.67, Test_acc 54.37
2025-02-14 21:38:01,212 [podnet.py] => Task 5, Epoch 77/160 (LR 0.05294) => LSC_loss 0.24, Spatial_loss 2.31, Flat_loss 0.22, Train_acc 95.28, Test_acc 54.09
2025-02-14 21:38:03,276 [podnet.py] => Task 5, Epoch 78/160 (LR 0.05196) => LSC_loss 0.26, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 94.46, Test_acc 54.67
2025-02-14 21:38:05,316 [podnet.py] => Task 5, Epoch 79/160 (LR 0.05098) => LSC_loss 0.25, Spatial_loss 2.35, Flat_loss 0.22, Train_acc 94.67, Test_acc 51.91
2025-02-14 21:38:07,362 [podnet.py] => Task 5, Epoch 80/160 (LR 0.05000) => LSC_loss 0.26, Spatial_loss 2.31, Flat_loss 0.23, Train_acc 94.28, Test_acc 53.97
2025-02-14 21:38:09,319 [podnet.py] => Task 5, Epoch 81/160 (LR 0.04902) => LSC_loss 0.24, Spatial_loss 2.32, Flat_loss 0.22, Train_acc 95.18, Test_acc 55.84
2025-02-14 21:38:11,320 [podnet.py] => Task 5, Epoch 82/160 (LR 0.04804) => LSC_loss 0.25, Spatial_loss 2.28, Flat_loss 0.22, Train_acc 94.85, Test_acc 56.87
2025-02-14 21:38:13,278 [podnet.py] => Task 5, Epoch 83/160 (LR 0.04706) => LSC_loss 0.24, Spatial_loss 2.34, Flat_loss 0.22, Train_acc 95.28, Test_acc 55.32
2025-02-14 21:38:15,248 [podnet.py] => Task 5, Epoch 84/160 (LR 0.04608) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.21, Train_acc 95.38, Test_acc 57.00
2025-02-14 21:38:17,198 [podnet.py] => Task 5, Epoch 85/160 (LR 0.04510) => LSC_loss 0.24, Spatial_loss 2.34, Flat_loss 0.21, Train_acc 94.95, Test_acc 53.69
2025-02-14 21:38:19,219 [podnet.py] => Task 5, Epoch 86/160 (LR 0.04412) => LSC_loss 0.24, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 95.31, Test_acc 54.09
2025-02-14 21:38:21,174 [podnet.py] => Task 5, Epoch 87/160 (LR 0.04315) => LSC_loss 0.24, Spatial_loss 2.15, Flat_loss 0.20, Train_acc 95.36, Test_acc 56.16
2025-02-14 21:38:23,181 [podnet.py] => Task 5, Epoch 88/160 (LR 0.04218) => LSC_loss 0.23, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 95.62, Test_acc 55.28
2025-02-14 21:38:25,222 [podnet.py] => Task 5, Epoch 89/160 (LR 0.04121) => LSC_loss 0.24, Spatial_loss 2.16, Flat_loss 0.20, Train_acc 95.26, Test_acc 53.65
2025-02-14 21:38:27,206 [podnet.py] => Task 5, Epoch 90/160 (LR 0.04025) => LSC_loss 0.23, Spatial_loss 2.18, Flat_loss 0.20, Train_acc 95.21, Test_acc 57.13
2025-02-14 21:38:29,128 [podnet.py] => Task 5, Epoch 91/160 (LR 0.03928) => LSC_loss 0.24, Spatial_loss 2.20, Flat_loss 0.20, Train_acc 94.64, Test_acc 57.51
2025-02-14 21:38:31,088 [podnet.py] => Task 5, Epoch 92/160 (LR 0.03833) => LSC_loss 0.24, Spatial_loss 2.19, Flat_loss 0.20, Train_acc 95.08, Test_acc 53.73
2025-02-14 21:38:33,038 [podnet.py] => Task 5, Epoch 93/160 (LR 0.03738) => LSC_loss 0.23, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.33, Test_acc 55.85
2025-02-14 21:38:34,950 [podnet.py] => Task 5, Epoch 94/160 (LR 0.03643) => LSC_loss 0.25, Spatial_loss 2.07, Flat_loss 0.19, Train_acc 95.05, Test_acc 54.32
2025-02-14 21:38:36,939 [podnet.py] => Task 5, Epoch 95/160 (LR 0.03549) => LSC_loss 0.24, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.18, Test_acc 55.08
2025-02-14 21:38:38,862 [podnet.py] => Task 5, Epoch 96/160 (LR 0.03455) => LSC_loss 0.23, Spatial_loss 2.06, Flat_loss 0.19, Train_acc 95.62, Test_acc 55.77
2025-02-14 21:38:40,851 [podnet.py] => Task 5, Epoch 97/160 (LR 0.03362) => LSC_loss 0.23, Spatial_loss 1.99, Flat_loss 0.18, Train_acc 95.74, Test_acc 54.81
2025-02-14 21:38:42,761 [podnet.py] => Task 5, Epoch 98/160 (LR 0.03269) => LSC_loss 0.23, Spatial_loss 2.05, Flat_loss 0.18, Train_acc 95.69, Test_acc 54.81
2025-02-14 21:38:44,757 [podnet.py] => Task 5, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.12, Flat_loss 0.19, Train_acc 94.72, Test_acc 55.73
2025-02-14 21:38:46,757 [podnet.py] => Task 5, Epoch 100/160 (LR 0.03087) => LSC_loss 0.24, Spatial_loss 2.00, Flat_loss 0.18, Train_acc 94.95, Test_acc 56.05
2025-02-14 21:38:48,736 [podnet.py] => Task 5, Epoch 101/160 (LR 0.02996) => LSC_loss 0.24, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.36, Test_acc 53.73
2025-02-14 21:38:50,730 [podnet.py] => Task 5, Epoch 102/160 (LR 0.02907) => LSC_loss 0.23, Spatial_loss 2.02, Flat_loss 0.18, Train_acc 95.51, Test_acc 55.71
2025-02-14 21:38:52,709 [podnet.py] => Task 5, Epoch 103/160 (LR 0.02818) => LSC_loss 0.23, Spatial_loss 1.96, Flat_loss 0.18, Train_acc 95.46, Test_acc 55.76
2025-02-14 21:38:54,636 [podnet.py] => Task 5, Epoch 104/160 (LR 0.02730) => LSC_loss 0.23, Spatial_loss 1.92, Flat_loss 0.17, Train_acc 95.72, Test_acc 55.28
2025-02-14 21:38:56,630 [podnet.py] => Task 5, Epoch 105/160 (LR 0.02643) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.18, Train_acc 95.56, Test_acc 54.52
2025-02-14 21:38:58,690 [podnet.py] => Task 5, Epoch 106/160 (LR 0.02557) => LSC_loss 0.23, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.33, Test_acc 57.44
2025-02-14 21:39:00,683 [podnet.py] => Task 5, Epoch 107/160 (LR 0.02472) => LSC_loss 0.25, Spatial_loss 2.02, Flat_loss 0.18, Train_acc 94.97, Test_acc 58.40
2025-02-14 21:39:02,699 [podnet.py] => Task 5, Epoch 108/160 (LR 0.02388) => LSC_loss 0.22, Spatial_loss 1.86, Flat_loss 0.17, Train_acc 95.90, Test_acc 56.43
2025-02-14 21:39:04,666 [podnet.py] => Task 5, Epoch 109/160 (LR 0.02304) => LSC_loss 0.22, Spatial_loss 1.84, Flat_loss 0.17, Train_acc 95.87, Test_acc 54.55
2025-02-14 21:39:06,639 [podnet.py] => Task 5, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 1.81, Flat_loss 0.17, Train_acc 95.72, Test_acc 55.73
2025-02-14 21:39:08,603 [podnet.py] => Task 5, Epoch 111/160 (LR 0.02141) => LSC_loss 0.24, Spatial_loss 1.86, Flat_loss 0.17, Train_acc 94.90, Test_acc 56.47
2025-02-14 21:39:10,614 [podnet.py] => Task 5, Epoch 112/160 (LR 0.02061) => LSC_loss 0.23, Spatial_loss 1.78, Flat_loss 0.16, Train_acc 95.23, Test_acc 56.91
2025-02-14 21:39:12,609 [podnet.py] => Task 5, Epoch 113/160 (LR 0.01982) => LSC_loss 0.23, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.72, Test_acc 54.88
2025-02-14 21:39:14,593 [podnet.py] => Task 5, Epoch 114/160 (LR 0.01905) => LSC_loss 0.23, Spatial_loss 1.81, Flat_loss 0.17, Train_acc 95.44, Test_acc 58.07
2025-02-14 21:39:16,666 [podnet.py] => Task 5, Epoch 115/160 (LR 0.01828) => LSC_loss 0.23, Spatial_loss 1.81, Flat_loss 0.16, Train_acc 95.62, Test_acc 58.41
2025-02-14 21:39:18,640 [podnet.py] => Task 5, Epoch 116/160 (LR 0.01753) => LSC_loss 0.24, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.44, Test_acc 56.04
2025-02-14 21:39:20,584 [podnet.py] => Task 5, Epoch 117/160 (LR 0.01679) => LSC_loss 0.23, Spatial_loss 1.74, Flat_loss 0.16, Train_acc 95.64, Test_acc 58.01
2025-02-14 21:39:22,583 [podnet.py] => Task 5, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.70, Flat_loss 0.15, Train_acc 95.56, Test_acc 57.20
2025-02-14 21:39:24,609 [podnet.py] => Task 5, Epoch 119/160 (LR 0.01535) => LSC_loss 0.24, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 95.26, Test_acc 55.28
2025-02-14 21:39:26,621 [podnet.py] => Task 5, Epoch 120/160 (LR 0.01464) => LSC_loss 0.23, Spatial_loss 1.71, Flat_loss 0.15, Train_acc 95.87, Test_acc 57.41
2025-02-14 21:39:28,601 [podnet.py] => Task 5, Epoch 121/160 (LR 0.01396) => LSC_loss 0.24, Spatial_loss 1.67, Flat_loss 0.15, Train_acc 95.21, Test_acc 55.81
2025-02-14 21:39:30,579 [podnet.py] => Task 5, Epoch 122/160 (LR 0.01328) => LSC_loss 0.23, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 95.95, Test_acc 58.15
2025-02-14 21:39:32,594 [podnet.py] => Task 5, Epoch 123/160 (LR 0.01262) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.15, Train_acc 95.74, Test_acc 57.08
2025-02-14 21:39:34,559 [podnet.py] => Task 5, Epoch 124/160 (LR 0.01198) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 95.77, Test_acc 57.83
2025-02-14 21:39:36,488 [podnet.py] => Task 5, Epoch 125/160 (LR 0.01135) => LSC_loss 0.24, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 95.51, Test_acc 57.57
2025-02-14 21:39:38,502 [podnet.py] => Task 5, Epoch 126/160 (LR 0.01073) => LSC_loss 0.23, Spatial_loss 1.66, Flat_loss 0.15, Train_acc 95.54, Test_acc 57.87
2025-02-14 21:39:40,517 [podnet.py] => Task 5, Epoch 127/160 (LR 0.01013) => LSC_loss 0.23, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 95.56, Test_acc 56.59
2025-02-14 21:39:42,565 [podnet.py] => Task 5, Epoch 128/160 (LR 0.00955) => LSC_loss 0.22, Spatial_loss 1.60, Flat_loss 0.14, Train_acc 96.05, Test_acc 58.29
2025-02-14 21:39:44,578 [podnet.py] => Task 5, Epoch 129/160 (LR 0.00898) => LSC_loss 0.23, Spatial_loss 1.59, Flat_loss 0.14, Train_acc 95.54, Test_acc 58.28
2025-02-14 21:39:46,563 [podnet.py] => Task 5, Epoch 130/160 (LR 0.00843) => LSC_loss 0.24, Spatial_loss 1.55, Flat_loss 0.15, Train_acc 95.26, Test_acc 57.96
2025-02-14 21:39:48,590 [podnet.py] => Task 5, Epoch 131/160 (LR 0.00789) => LSC_loss 0.24, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 95.23, Test_acc 57.64
2025-02-14 21:39:50,570 [podnet.py] => Task 5, Epoch 132/160 (LR 0.00737) => LSC_loss 0.23, Spatial_loss 1.54, Flat_loss 0.14, Train_acc 95.46, Test_acc 57.24
2025-02-14 21:39:52,614 [podnet.py] => Task 5, Epoch 133/160 (LR 0.00686) => LSC_loss 0.23, Spatial_loss 1.48, Flat_loss 0.14, Train_acc 95.49, Test_acc 57.84
2025-02-14 21:39:54,638 [podnet.py] => Task 5, Epoch 134/160 (LR 0.00638) => LSC_loss 0.23, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 95.85, Test_acc 58.81
2025-02-14 21:39:56,670 [podnet.py] => Task 5, Epoch 135/160 (LR 0.00590) => LSC_loss 0.24, Spatial_loss 1.50, Flat_loss 0.14, Train_acc 95.31, Test_acc 57.87
2025-02-14 21:39:58,658 [podnet.py] => Task 5, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 95.56, Test_acc 58.27
2025-02-14 21:40:00,690 [podnet.py] => Task 5, Epoch 137/160 (LR 0.00501) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 95.69, Test_acc 57.91
2025-02-14 21:40:02,699 [podnet.py] => Task 5, Epoch 138/160 (LR 0.00459) => LSC_loss 0.22, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.90, Test_acc 58.40
2025-02-14 21:40:04,646 [podnet.py] => Task 5, Epoch 139/160 (LR 0.00419) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.64, Test_acc 58.37
2025-02-14 21:40:06,630 [podnet.py] => Task 5, Epoch 140/160 (LR 0.00381) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 95.77, Test_acc 58.77
2025-02-14 21:40:08,566 [podnet.py] => Task 5, Epoch 141/160 (LR 0.00344) => LSC_loss 0.23, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 95.87, Test_acc 57.76
2025-02-14 21:40:10,541 [podnet.py] => Task 5, Epoch 142/160 (LR 0.00309) => LSC_loss 0.23, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 95.56, Test_acc 58.51
2025-02-14 21:40:12,542 [podnet.py] => Task 5, Epoch 143/160 (LR 0.00276) => LSC_loss 0.23, Spatial_loss 1.49, Flat_loss 0.14, Train_acc 95.56, Test_acc 58.04
2025-02-14 21:40:14,455 [podnet.py] => Task 5, Epoch 144/160 (LR 0.00245) => LSC_loss 0.22, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 96.28, Test_acc 57.93
2025-02-14 21:40:16,405 [podnet.py] => Task 5, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 96.13, Test_acc 58.60
2025-02-14 21:40:18,405 [podnet.py] => Task 5, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.72, Test_acc 58.05
2025-02-14 21:40:20,374 [podnet.py] => Task 5, Epoch 147/160 (LR 0.00162) => LSC_loss 0.23, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 95.77, Test_acc 58.59
2025-02-14 21:40:22,323 [podnet.py] => Task 5, Epoch 148/160 (LR 0.00138) => LSC_loss 0.24, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.33, Test_acc 58.16
2025-02-14 21:40:24,303 [podnet.py] => Task 5, Epoch 149/160 (LR 0.00116) => LSC_loss 0.23, Spatial_loss 1.37, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.35
2025-02-14 21:40:26,293 [podnet.py] => Task 5, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 96.08, Test_acc 58.43
2025-02-14 21:40:28,269 [podnet.py] => Task 5, Epoch 151/160 (LR 0.00078) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 95.49, Test_acc 58.39
2025-02-14 21:40:30,280 [podnet.py] => Task 5, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 95.85, Test_acc 58.56
2025-02-14 21:40:32,287 [podnet.py] => Task 5, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.69
2025-02-14 21:40:34,307 [podnet.py] => Task 5, Epoch 154/160 (LR 0.00035) => LSC_loss 0.23, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 95.92, Test_acc 58.63
2025-02-14 21:40:36,297 [podnet.py] => Task 5, Epoch 155/160 (LR 0.00024) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 95.59, Test_acc 58.60
2025-02-14 21:40:38,250 [podnet.py] => Task 5, Epoch 156/160 (LR 0.00015) => LSC_loss 0.24, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 95.59, Test_acc 58.45
2025-02-14 21:40:40,270 [podnet.py] => Task 5, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 95.87, Test_acc 58.43
2025-02-14 21:40:42,253 [podnet.py] => Task 5, Epoch 158/160 (LR 0.00004) => LSC_loss 0.22, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 96.13, Test_acc 58.72
2025-02-14 21:40:44,227 [podnet.py] => Task 5, Epoch 159/160 (LR 0.00001) => LSC_loss 0.23, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 95.87, Test_acc 58.33
2025-02-14 21:40:46,249 [podnet.py] => Task 5, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 96.23, Test_acc 57.91
2025-02-14 21:40:46,249 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:40:46,249 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:41:12,671 [podnet.py] => The size of finetune dataset: 1500
2025-02-14 21:41:14,161 [podnet.py] => Task 5, Epoch 1/20 (LR 0.00497) => LSC_loss 0.19, Spatial_loss 1.61, Flat_loss 0.15, Train_acc 95.13, Test_acc 59.21
2025-02-14 21:41:15,552 [podnet.py] => Task 5, Epoch 2/20 (LR 0.00488) => LSC_loss 0.11, Spatial_loss 1.40, Flat_loss 0.08, Train_acc 98.27, Test_acc 59.88
2025-02-14 21:41:16,956 [podnet.py] => Task 5, Epoch 3/20 (LR 0.00473) => LSC_loss 0.10, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.13, Test_acc 60.91
2025-02-14 21:41:18,396 [podnet.py] => Task 5, Epoch 4/20 (LR 0.00452) => LSC_loss 0.10, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.07, Test_acc 61.09
2025-02-14 21:41:19,826 [podnet.py] => Task 5, Epoch 5/20 (LR 0.00427) => LSC_loss 0.09, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.80, Test_acc 60.59
2025-02-14 21:41:21,236 [podnet.py] => Task 5, Epoch 6/20 (LR 0.00397) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 97.80, Test_acc 60.59
2025-02-14 21:41:22,627 [podnet.py] => Task 5, Epoch 7/20 (LR 0.00363) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.27, Test_acc 60.75
2025-02-14 21:41:24,003 [podnet.py] => Task 5, Epoch 8/20 (LR 0.00327) => LSC_loss 0.09, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.87, Test_acc 61.01
2025-02-14 21:41:25,377 [podnet.py] => Task 5, Epoch 9/20 (LR 0.00289) => LSC_loss 0.10, Spatial_loss 1.29, Flat_loss 0.05, Train_acc 98.27, Test_acc 60.72
2025-02-14 21:41:26,874 [podnet.py] => Task 5, Epoch 10/20 (LR 0.00250) => LSC_loss 0.09, Spatial_loss 1.30, Flat_loss 0.06, Train_acc 98.60, Test_acc 60.61
2025-02-14 21:41:28,306 [podnet.py] => Task 5, Epoch 11/20 (LR 0.00211) => LSC_loss 0.10, Spatial_loss 1.30, Flat_loss 0.05, Train_acc 98.87, Test_acc 60.69
2025-02-14 21:41:29,731 [podnet.py] => Task 5, Epoch 12/20 (LR 0.00173) => LSC_loss 0.09, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 98.40, Test_acc 60.87
2025-02-14 21:41:31,166 [podnet.py] => Task 5, Epoch 13/20 (LR 0.00137) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.73, Test_acc 60.85
2025-02-14 21:41:32,542 [podnet.py] => Task 5, Epoch 14/20 (LR 0.00103) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.06, Train_acc 98.67, Test_acc 60.81
2025-02-14 21:41:34,014 [podnet.py] => Task 5, Epoch 15/20 (LR 0.00073) => LSC_loss 0.09, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.27, Test_acc 61.09
2025-02-14 21:41:35,484 [podnet.py] => Task 5, Epoch 16/20 (LR 0.00048) => LSC_loss 0.09, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.40, Test_acc 61.08
2025-02-14 21:41:36,934 [podnet.py] => Task 5, Epoch 17/20 (LR 0.00027) => LSC_loss 0.09, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.53, Test_acc 60.87
2025-02-14 21:41:38,362 [podnet.py] => Task 5, Epoch 18/20 (LR 0.00012) => LSC_loss 0.09, Spatial_loss 1.24, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.77
2025-02-14 21:41:39,740 [podnet.py] => Task 5, Epoch 19/20 (LR 0.00003) => LSC_loss 0.09, Spatial_loss 1.22, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.88
2025-02-14 21:41:41,161 [podnet.py] => Task 5, Epoch 20/20 (LR 0.00000) => LSC_loss 0.09, Spatial_loss 1.18, Flat_loss 0.05, Train_acc 98.60, Test_acc 60.75
2025-02-14 21:41:41,162 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:42:09,141 [podnet.py] => Exemplar size: 1500
2025-02-14 21:42:09,141 [trainer.py] => CNN: {'total': 60.75, '00-09': 68.8, '10-19': 50.9, '20-29': 67.7, '30-39': 59.6, '40-49': 65.2, '50-59': 48.2, '60-69': 59.2, '70-79': 72.0, 'old': 59.94, 'new': 72.0}
2025-02-14 21:42:09,141 [trainer.py] => NME: {'total': 61.48, '00-09': 72.1, '10-19': 56.6, '20-29': 71.3, '30-39': 62.6, '40-49': 67.6, '50-59': 44.1, '60-69': 55.2, '70-79': 63.2, 'old': 61.36, 'new': 63.2}
2025-02-14 21:42:09,141 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75]
2025-02-14 21:42:09,141 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55]
2025-02-14 21:42:09,141 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48]
2025-02-14 21:42:09,141 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52]

2025-02-14 21:42:09,141 [trainer.py] => Average Accuracy (CNN): 68.54833333333333
2025-02-14 21:42:09,141 [trainer.py] => Average Accuracy (NME): 68.635
2025-02-14 21:42:09,142 [trainer.py] => All params: 514257
2025-02-14 21:42:09,142 [trainer.py] => Trainable params: 514257
2025-02-14 21:42:09,143 [podnet.py] => Learning on 75-80
2025-02-14 21:42:09,172 [podnet.py] => Adaptive factor: 4.0
2025-02-14 21:42:11,303 [podnet.py] => Task 6, Epoch 1/160 (LR 0.09999) => LSC_loss 1.68, Spatial_loss 4.23, Flat_loss 0.95, Train_acc 67.78, Test_acc 37.98
2025-02-14 21:42:13,344 [podnet.py] => Task 6, Epoch 2/160 (LR 0.09996) => LSC_loss 0.82, Spatial_loss 4.53, Flat_loss 0.77, Train_acc 78.22, Test_acc 39.15
2025-02-14 21:42:15,347 [podnet.py] => Task 6, Epoch 3/160 (LR 0.09991) => LSC_loss 0.64, Spatial_loss 4.24, Flat_loss 0.65, Train_acc 82.52, Test_acc 45.81
2025-02-14 21:42:17,416 [podnet.py] => Task 6, Epoch 4/160 (LR 0.09985) => LSC_loss 0.55, Spatial_loss 4.12, Flat_loss 0.59, Train_acc 84.90, Test_acc 41.86
2025-02-14 21:42:19,466 [podnet.py] => Task 6, Epoch 5/160 (LR 0.09976) => LSC_loss 0.49, Spatial_loss 3.76, Flat_loss 0.49, Train_acc 87.75, Test_acc 49.94
2025-02-14 21:42:21,520 [podnet.py] => Task 6, Epoch 6/160 (LR 0.09965) => LSC_loss 0.44, Spatial_loss 3.56, Flat_loss 0.44, Train_acc 88.72, Test_acc 49.80
2025-02-14 21:42:23,591 [podnet.py] => Task 6, Epoch 7/160 (LR 0.09953) => LSC_loss 0.43, Spatial_loss 3.66, Flat_loss 0.45, Train_acc 88.95, Test_acc 49.42
2025-02-14 21:42:25,602 [podnet.py] => Task 6, Epoch 8/160 (LR 0.09938) => LSC_loss 0.40, Spatial_loss 3.41, Flat_loss 0.40, Train_acc 89.58, Test_acc 49.40
2025-02-14 21:42:27,697 [podnet.py] => Task 6, Epoch 9/160 (LR 0.09922) => LSC_loss 0.39, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.32, Test_acc 46.72
2025-02-14 21:42:29,751 [podnet.py] => Task 6, Epoch 10/160 (LR 0.09904) => LSC_loss 0.40, Spatial_loss 3.39, Flat_loss 0.41, Train_acc 89.48, Test_acc 48.94
2025-02-14 21:42:31,774 [podnet.py] => Task 6, Epoch 11/160 (LR 0.09884) => LSC_loss 0.39, Spatial_loss 3.25, Flat_loss 0.37, Train_acc 90.65, Test_acc 50.08
2025-02-14 21:42:33,803 [podnet.py] => Task 6, Epoch 12/160 (LR 0.09862) => LSC_loss 0.40, Spatial_loss 3.37, Flat_loss 0.39, Train_acc 90.20, Test_acc 48.80
2025-02-14 21:42:35,829 [podnet.py] => Task 6, Epoch 13/160 (LR 0.09838) => LSC_loss 0.38, Spatial_loss 3.38, Flat_loss 0.38, Train_acc 89.68, Test_acc 48.35
2025-02-14 21:42:37,920 [podnet.py] => Task 6, Epoch 14/160 (LR 0.09812) => LSC_loss 0.34, Spatial_loss 3.33, Flat_loss 0.37, Train_acc 92.00, Test_acc 45.21
2025-02-14 21:42:39,923 [podnet.py] => Task 6, Epoch 15/160 (LR 0.09785) => LSC_loss 0.36, Spatial_loss 3.21, Flat_loss 0.36, Train_acc 91.10, Test_acc 48.52
2025-02-14 21:42:41,972 [podnet.py] => Task 6, Epoch 16/160 (LR 0.09755) => LSC_loss 0.38, Spatial_loss 3.39, Flat_loss 0.37, Train_acc 91.20, Test_acc 50.16
2025-02-14 21:42:44,011 [podnet.py] => Task 6, Epoch 17/160 (LR 0.09724) => LSC_loss 0.36, Spatial_loss 3.23, Flat_loss 0.36, Train_acc 91.42, Test_acc 47.85
2025-02-14 21:42:46,041 [podnet.py] => Task 6, Epoch 18/160 (LR 0.09691) => LSC_loss 0.33, Spatial_loss 3.05, Flat_loss 0.33, Train_acc 92.42, Test_acc 49.22
2025-02-14 21:42:48,048 [podnet.py] => Task 6, Epoch 19/160 (LR 0.09656) => LSC_loss 0.35, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 91.68, Test_acc 46.36
2025-02-14 21:42:50,058 [podnet.py] => Task 6, Epoch 20/160 (LR 0.09619) => LSC_loss 0.39, Spatial_loss 3.30, Flat_loss 0.37, Train_acc 90.85, Test_acc 45.68
2025-02-14 21:42:52,088 [podnet.py] => Task 6, Epoch 21/160 (LR 0.09581) => LSC_loss 0.34, Spatial_loss 3.08, Flat_loss 0.34, Train_acc 92.40, Test_acc 52.49
2025-02-14 21:42:54,122 [podnet.py] => Task 6, Epoch 22/160 (LR 0.09541) => LSC_loss 0.34, Spatial_loss 3.02, Flat_loss 0.33, Train_acc 91.92, Test_acc 51.28
2025-02-14 21:42:56,149 [podnet.py] => Task 6, Epoch 23/160 (LR 0.09499) => LSC_loss 0.33, Spatial_loss 3.13, Flat_loss 0.33, Train_acc 92.02, Test_acc 48.49
2025-02-14 21:42:58,236 [podnet.py] => Task 6, Epoch 24/160 (LR 0.09455) => LSC_loss 0.35, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.88, Test_acc 51.12
2025-02-14 21:43:00,213 [podnet.py] => Task 6, Epoch 25/160 (LR 0.09410) => LSC_loss 0.33, Spatial_loss 3.19, Flat_loss 0.35, Train_acc 91.68, Test_acc 46.69
2025-02-14 21:43:02,200 [podnet.py] => Task 6, Epoch 26/160 (LR 0.09362) => LSC_loss 0.34, Spatial_loss 3.15, Flat_loss 0.34, Train_acc 91.35, Test_acc 46.10
2025-02-14 21:43:04,308 [podnet.py] => Task 6, Epoch 27/160 (LR 0.09314) => LSC_loss 0.32, Spatial_loss 3.08, Flat_loss 0.33, Train_acc 92.35, Test_acc 45.64
2025-02-14 21:43:06,378 [podnet.py] => Task 6, Epoch 28/160 (LR 0.09263) => LSC_loss 0.31, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 92.90, Test_acc 49.25
2025-02-14 21:43:08,430 [podnet.py] => Task 6, Epoch 29/160 (LR 0.09211) => LSC_loss 0.34, Spatial_loss 3.04, Flat_loss 0.33, Train_acc 92.15, Test_acc 48.71
2025-02-14 21:43:10,468 [podnet.py] => Task 6, Epoch 30/160 (LR 0.09157) => LSC_loss 0.33, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 92.05, Test_acc 49.08
2025-02-14 21:43:12,510 [podnet.py] => Task 6, Epoch 31/160 (LR 0.09102) => LSC_loss 0.32, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 92.62, Test_acc 49.14
2025-02-14 21:43:14,526 [podnet.py] => Task 6, Epoch 32/160 (LR 0.09045) => LSC_loss 0.33, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 92.08, Test_acc 49.32
2025-02-14 21:43:16,513 [podnet.py] => Task 6, Epoch 33/160 (LR 0.08987) => LSC_loss 0.32, Spatial_loss 3.03, Flat_loss 0.33, Train_acc 92.92, Test_acc 52.25
2025-02-14 21:43:18,524 [podnet.py] => Task 6, Epoch 34/160 (LR 0.08927) => LSC_loss 0.31, Spatial_loss 2.99, Flat_loss 0.32, Train_acc 92.52, Test_acc 49.48
2025-02-14 21:43:20,581 [podnet.py] => Task 6, Epoch 35/160 (LR 0.08865) => LSC_loss 0.31, Spatial_loss 2.95, Flat_loss 0.31, Train_acc 92.95, Test_acc 49.85
2025-02-14 21:43:22,634 [podnet.py] => Task 6, Epoch 36/160 (LR 0.08802) => LSC_loss 0.31, Spatial_loss 2.92, Flat_loss 0.31, Train_acc 92.88, Test_acc 51.04
2025-02-14 21:43:24,721 [podnet.py] => Task 6, Epoch 37/160 (LR 0.08738) => LSC_loss 0.33, Spatial_loss 2.93, Flat_loss 0.32, Train_acc 92.48, Test_acc 42.90
2025-02-14 21:43:26,807 [podnet.py] => Task 6, Epoch 38/160 (LR 0.08672) => LSC_loss 0.36, Spatial_loss 3.17, Flat_loss 0.36, Train_acc 91.30, Test_acc 51.41
2025-02-14 21:43:28,862 [podnet.py] => Task 6, Epoch 39/160 (LR 0.08604) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 93.22, Test_acc 49.25
2025-02-14 21:43:30,879 [podnet.py] => Task 6, Epoch 40/160 (LR 0.08536) => LSC_loss 0.30, Spatial_loss 2.92, Flat_loss 0.30, Train_acc 93.28, Test_acc 51.05
2025-02-14 21:43:32,908 [podnet.py] => Task 6, Epoch 41/160 (LR 0.08465) => LSC_loss 0.29, Spatial_loss 2.83, Flat_loss 0.29, Train_acc 93.72, Test_acc 52.51
2025-02-14 21:43:34,979 [podnet.py] => Task 6, Epoch 42/160 (LR 0.08394) => LSC_loss 0.29, Spatial_loss 2.85, Flat_loss 0.29, Train_acc 93.58, Test_acc 52.72
2025-02-14 21:43:37,042 [podnet.py] => Task 6, Epoch 43/160 (LR 0.08321) => LSC_loss 0.31, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 92.65, Test_acc 51.80
2025-02-14 21:43:39,011 [podnet.py] => Task 6, Epoch 44/160 (LR 0.08247) => LSC_loss 0.30, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 93.22, Test_acc 50.41
2025-02-14 21:43:41,056 [podnet.py] => Task 6, Epoch 45/160 (LR 0.08172) => LSC_loss 0.31, Spatial_loss 2.87, Flat_loss 0.30, Train_acc 93.15, Test_acc 47.22
2025-02-14 21:43:43,089 [podnet.py] => Task 6, Epoch 46/160 (LR 0.08095) => LSC_loss 0.29, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 93.20, Test_acc 51.00
2025-02-14 21:43:45,183 [podnet.py] => Task 6, Epoch 47/160 (LR 0.08018) => LSC_loss 0.30, Spatial_loss 2.73, Flat_loss 0.29, Train_acc 93.25, Test_acc 49.92
2025-02-14 21:43:47,183 [podnet.py] => Task 6, Epoch 48/160 (LR 0.07939) => LSC_loss 0.28, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 93.80, Test_acc 50.38
2025-02-14 21:43:49,231 [podnet.py] => Task 6, Epoch 49/160 (LR 0.07859) => LSC_loss 0.30, Spatial_loss 2.87, Flat_loss 0.29, Train_acc 93.15, Test_acc 47.50
2025-02-14 21:43:51,291 [podnet.py] => Task 6, Epoch 50/160 (LR 0.07778) => LSC_loss 0.28, Spatial_loss 2.89, Flat_loss 0.30, Train_acc 93.78, Test_acc 51.12
2025-02-14 21:43:53,274 [podnet.py] => Task 6, Epoch 51/160 (LR 0.07696) => LSC_loss 0.29, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 93.05, Test_acc 53.30
2025-02-14 21:43:55,320 [podnet.py] => Task 6, Epoch 52/160 (LR 0.07612) => LSC_loss 0.28, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 93.92, Test_acc 50.61
2025-02-14 21:43:57,357 [podnet.py] => Task 6, Epoch 53/160 (LR 0.07528) => LSC_loss 0.29, Spatial_loss 2.71, Flat_loss 0.27, Train_acc 93.40, Test_acc 47.81
2025-02-14 21:43:59,354 [podnet.py] => Task 6, Epoch 54/160 (LR 0.07443) => LSC_loss 0.28, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 93.65, Test_acc 50.91
2025-02-14 21:44:01,412 [podnet.py] => Task 6, Epoch 55/160 (LR 0.07357) => LSC_loss 0.27, Spatial_loss 2.66, Flat_loss 0.27, Train_acc 93.82, Test_acc 49.72
2025-02-14 21:44:03,448 [podnet.py] => Task 6, Epoch 56/160 (LR 0.07270) => LSC_loss 0.29, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 93.50, Test_acc 50.26
2025-02-14 21:44:05,507 [podnet.py] => Task 6, Epoch 57/160 (LR 0.07182) => LSC_loss 0.30, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 93.42, Test_acc 49.06
2025-02-14 21:44:07,593 [podnet.py] => Task 6, Epoch 58/160 (LR 0.07093) => LSC_loss 0.29, Spatial_loss 2.76, Flat_loss 0.29, Train_acc 93.55, Test_acc 51.70
2025-02-14 21:44:09,584 [podnet.py] => Task 6, Epoch 59/160 (LR 0.07004) => LSC_loss 0.28, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 93.85, Test_acc 53.24
2025-02-14 21:44:11,634 [podnet.py] => Task 6, Epoch 60/160 (LR 0.06913) => LSC_loss 0.28, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.78, Test_acc 50.49
2025-02-14 21:44:13,659 [podnet.py] => Task 6, Epoch 61/160 (LR 0.06822) => LSC_loss 0.27, Spatial_loss 2.77, Flat_loss 0.28, Train_acc 93.80, Test_acc 52.34
2025-02-14 21:44:15,656 [podnet.py] => Task 6, Epoch 62/160 (LR 0.06731) => LSC_loss 0.28, Spatial_loss 2.70, Flat_loss 0.27, Train_acc 94.22, Test_acc 48.96
2025-02-14 21:44:17,685 [podnet.py] => Task 6, Epoch 63/160 (LR 0.06638) => LSC_loss 0.31, Spatial_loss 2.75, Flat_loss 0.29, Train_acc 93.58, Test_acc 51.55
2025-02-14 21:44:19,683 [podnet.py] => Task 6, Epoch 64/160 (LR 0.06545) => LSC_loss 0.27, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.38, Test_acc 52.38
2025-02-14 21:44:21,778 [podnet.py] => Task 6, Epoch 65/160 (LR 0.06451) => LSC_loss 0.29, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.85, Test_acc 53.05
2025-02-14 21:44:23,812 [podnet.py] => Task 6, Epoch 66/160 (LR 0.06357) => LSC_loss 0.29, Spatial_loss 2.61, Flat_loss 0.27, Train_acc 93.30, Test_acc 49.65
2025-02-14 21:44:25,812 [podnet.py] => Task 6, Epoch 67/160 (LR 0.06262) => LSC_loss 0.28, Spatial_loss 2.63, Flat_loss 0.27, Train_acc 93.68, Test_acc 51.39
2025-02-14 21:44:27,824 [podnet.py] => Task 6, Epoch 68/160 (LR 0.06167) => LSC_loss 0.28, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.70, Test_acc 52.14
2025-02-14 21:44:29,895 [podnet.py] => Task 6, Epoch 69/160 (LR 0.06072) => LSC_loss 0.27, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 93.52, Test_acc 51.85
2025-02-14 21:44:31,897 [podnet.py] => Task 6, Epoch 70/160 (LR 0.05975) => LSC_loss 0.27, Spatial_loss 2.51, Flat_loss 0.25, Train_acc 94.15, Test_acc 52.21
2025-02-14 21:44:33,927 [podnet.py] => Task 6, Epoch 71/160 (LR 0.05879) => LSC_loss 0.26, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 94.88, Test_acc 52.81
2025-02-14 21:44:35,919 [podnet.py] => Task 6, Epoch 72/160 (LR 0.05782) => LSC_loss 0.26, Spatial_loss 2.39, Flat_loss 0.23, Train_acc 94.82, Test_acc 48.99
2025-02-14 21:44:37,966 [podnet.py] => Task 6, Epoch 73/160 (LR 0.05685) => LSC_loss 0.27, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 94.12, Test_acc 49.22
2025-02-14 21:44:40,023 [podnet.py] => Task 6, Epoch 74/160 (LR 0.05588) => LSC_loss 0.27, Spatial_loss 2.46, Flat_loss 0.25, Train_acc 93.85, Test_acc 48.69
2025-02-14 21:44:42,100 [podnet.py] => Task 6, Epoch 75/160 (LR 0.05490) => LSC_loss 0.27, Spatial_loss 2.51, Flat_loss 0.25, Train_acc 94.12, Test_acc 52.68
2025-02-14 21:44:44,131 [podnet.py] => Task 6, Epoch 76/160 (LR 0.05392) => LSC_loss 0.27, Spatial_loss 2.53, Flat_loss 0.26, Train_acc 94.20, Test_acc 51.74
2025-02-14 21:44:46,182 [podnet.py] => Task 6, Epoch 77/160 (LR 0.05294) => LSC_loss 0.27, Spatial_loss 2.52, Flat_loss 0.25, Train_acc 93.88, Test_acc 48.98
2025-02-14 21:44:48,210 [podnet.py] => Task 6, Epoch 78/160 (LR 0.05196) => LSC_loss 0.27, Spatial_loss 2.55, Flat_loss 0.25, Train_acc 94.32, Test_acc 50.98
2025-02-14 21:44:50,258 [podnet.py] => Task 6, Epoch 79/160 (LR 0.05098) => LSC_loss 0.27, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.20, Test_acc 51.79
2025-02-14 21:44:52,281 [podnet.py] => Task 6, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 2.48, Flat_loss 0.24, Train_acc 93.35, Test_acc 44.65
2025-02-14 21:44:54,335 [podnet.py] => Task 6, Epoch 81/160 (LR 0.04902) => LSC_loss 0.28, Spatial_loss 2.57, Flat_loss 0.26, Train_acc 94.25, Test_acc 52.62
2025-02-14 21:44:56,407 [podnet.py] => Task 6, Epoch 82/160 (LR 0.04804) => LSC_loss 0.26, Spatial_loss 2.40, Flat_loss 0.24, Train_acc 94.78, Test_acc 52.24
2025-02-14 21:44:58,442 [podnet.py] => Task 6, Epoch 83/160 (LR 0.04706) => LSC_loss 0.27, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 94.48, Test_acc 50.19
2025-02-14 21:45:00,490 [podnet.py] => Task 6, Epoch 84/160 (LR 0.04608) => LSC_loss 0.25, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 94.72, Test_acc 54.92
2025-02-14 21:45:02,565 [podnet.py] => Task 6, Epoch 85/160 (LR 0.04510) => LSC_loss 0.26, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.75, Test_acc 52.16
2025-02-14 21:45:04,661 [podnet.py] => Task 6, Epoch 86/160 (LR 0.04412) => LSC_loss 0.27, Spatial_loss 2.35, Flat_loss 0.23, Train_acc 93.92, Test_acc 52.69
2025-02-14 21:45:06,685 [podnet.py] => Task 6, Epoch 87/160 (LR 0.04315) => LSC_loss 0.25, Spatial_loss 2.36, Flat_loss 0.23, Train_acc 94.92, Test_acc 52.98
2025-02-14 21:45:08,701 [podnet.py] => Task 6, Epoch 88/160 (LR 0.04218) => LSC_loss 0.25, Spatial_loss 2.22, Flat_loss 0.22, Train_acc 94.95, Test_acc 53.74
2025-02-14 21:45:10,700 [podnet.py] => Task 6, Epoch 89/160 (LR 0.04121) => LSC_loss 0.27, Spatial_loss 2.29, Flat_loss 0.23, Train_acc 94.45, Test_acc 53.60
2025-02-14 21:45:12,711 [podnet.py] => Task 6, Epoch 90/160 (LR 0.04025) => LSC_loss 0.25, Spatial_loss 2.26, Flat_loss 0.22, Train_acc 94.50, Test_acc 51.32
2025-02-14 21:45:14,730 [podnet.py] => Task 6, Epoch 91/160 (LR 0.03928) => LSC_loss 0.26, Spatial_loss 2.25, Flat_loss 0.22, Train_acc 94.82, Test_acc 55.26
2025-02-14 21:45:16,800 [podnet.py] => Task 6, Epoch 92/160 (LR 0.03833) => LSC_loss 0.25, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.82, Test_acc 53.14
2025-02-14 21:45:18,894 [podnet.py] => Task 6, Epoch 93/160 (LR 0.03738) => LSC_loss 0.26, Spatial_loss 2.16, Flat_loss 0.21, Train_acc 94.25, Test_acc 54.69
2025-02-14 21:45:20,951 [podnet.py] => Task 6, Epoch 94/160 (LR 0.03643) => LSC_loss 0.26, Spatial_loss 2.20, Flat_loss 0.21, Train_acc 95.08, Test_acc 54.14
2025-02-14 21:45:23,006 [podnet.py] => Task 6, Epoch 95/160 (LR 0.03549) => LSC_loss 0.27, Spatial_loss 2.26, Flat_loss 0.21, Train_acc 94.80, Test_acc 54.36
2025-02-14 21:45:25,053 [podnet.py] => Task 6, Epoch 96/160 (LR 0.03455) => LSC_loss 0.25, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.72, Test_acc 53.19
2025-02-14 21:45:27,089 [podnet.py] => Task 6, Epoch 97/160 (LR 0.03362) => LSC_loss 0.26, Spatial_loss 2.11, Flat_loss 0.20, Train_acc 94.98, Test_acc 53.65
2025-02-14 21:45:29,188 [podnet.py] => Task 6, Epoch 98/160 (LR 0.03269) => LSC_loss 0.25, Spatial_loss 2.17, Flat_loss 0.21, Train_acc 95.10, Test_acc 51.91
2025-02-14 21:45:31,203 [podnet.py] => Task 6, Epoch 99/160 (LR 0.03178) => LSC_loss 0.25, Spatial_loss 2.12, Flat_loss 0.21, Train_acc 94.48, Test_acc 53.18
2025-02-14 21:45:33,292 [podnet.py] => Task 6, Epoch 100/160 (LR 0.03087) => LSC_loss 0.24, Spatial_loss 2.04, Flat_loss 0.20, Train_acc 95.10, Test_acc 53.54
2025-02-14 21:45:35,316 [podnet.py] => Task 6, Epoch 101/160 (LR 0.02996) => LSC_loss 0.26, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 95.00, Test_acc 53.72
2025-02-14 21:45:37,325 [podnet.py] => Task 6, Epoch 102/160 (LR 0.02907) => LSC_loss 0.26, Spatial_loss 2.09, Flat_loss 0.20, Train_acc 94.58, Test_acc 53.41
2025-02-14 21:45:39,368 [podnet.py] => Task 6, Epoch 103/160 (LR 0.02818) => LSC_loss 0.26, Spatial_loss 2.04, Flat_loss 0.19, Train_acc 94.88, Test_acc 52.31
2025-02-14 21:45:41,431 [podnet.py] => Task 6, Epoch 104/160 (LR 0.02730) => LSC_loss 0.24, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 95.15, Test_acc 53.26
2025-02-14 21:45:43,441 [podnet.py] => Task 6, Epoch 105/160 (LR 0.02643) => LSC_loss 0.25, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 95.48, Test_acc 54.04
2025-02-14 21:45:45,487 [podnet.py] => Task 6, Epoch 106/160 (LR 0.02557) => LSC_loss 0.24, Spatial_loss 1.95, Flat_loss 0.19, Train_acc 95.10, Test_acc 56.41
2025-02-14 21:45:47,518 [podnet.py] => Task 6, Epoch 107/160 (LR 0.02472) => LSC_loss 0.25, Spatial_loss 1.91, Flat_loss 0.19, Train_acc 95.10, Test_acc 54.14
2025-02-14 21:45:49,550 [podnet.py] => Task 6, Epoch 108/160 (LR 0.02388) => LSC_loss 0.25, Spatial_loss 1.96, Flat_loss 0.19, Train_acc 94.92, Test_acc 54.38
2025-02-14 21:45:51,532 [podnet.py] => Task 6, Epoch 109/160 (LR 0.02304) => LSC_loss 0.25, Spatial_loss 1.95, Flat_loss 0.18, Train_acc 95.15, Test_acc 54.19
2025-02-14 21:45:53,561 [podnet.py] => Task 6, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 1.92, Flat_loss 0.19, Train_acc 95.48, Test_acc 53.59
2025-02-14 21:45:55,637 [podnet.py] => Task 6, Epoch 111/160 (LR 0.02141) => LSC_loss 0.26, Spatial_loss 1.87, Flat_loss 0.18, Train_acc 94.98, Test_acc 53.71
2025-02-14 21:45:57,690 [podnet.py] => Task 6, Epoch 112/160 (LR 0.02061) => LSC_loss 0.25, Spatial_loss 1.88, Flat_loss 0.19, Train_acc 95.08, Test_acc 51.39
2025-02-14 21:45:59,754 [podnet.py] => Task 6, Epoch 113/160 (LR 0.01982) => LSC_loss 0.25, Spatial_loss 1.88, Flat_loss 0.18, Train_acc 95.30, Test_acc 54.18
2025-02-14 21:46:01,783 [podnet.py] => Task 6, Epoch 114/160 (LR 0.01905) => LSC_loss 0.25, Spatial_loss 1.86, Flat_loss 0.18, Train_acc 94.68, Test_acc 54.90
2025-02-14 21:46:03,816 [podnet.py] => Task 6, Epoch 115/160 (LR 0.01828) => LSC_loss 0.24, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 95.82, Test_acc 55.21
2025-02-14 21:46:05,819 [podnet.py] => Task 6, Epoch 116/160 (LR 0.01753) => LSC_loss 0.25, Spatial_loss 1.91, Flat_loss 0.17, Train_acc 95.18, Test_acc 55.60
2025-02-14 21:46:07,878 [podnet.py] => Task 6, Epoch 117/160 (LR 0.01679) => LSC_loss 0.25, Spatial_loss 1.86, Flat_loss 0.18, Train_acc 95.40, Test_acc 55.30
2025-02-14 21:46:09,903 [podnet.py] => Task 6, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 1.76, Flat_loss 0.16, Train_acc 95.40, Test_acc 54.62
2025-02-14 21:46:11,940 [podnet.py] => Task 6, Epoch 119/160 (LR 0.01535) => LSC_loss 0.24, Spatial_loss 1.70, Flat_loss 0.16, Train_acc 95.18, Test_acc 55.01
2025-02-14 21:46:13,976 [podnet.py] => Task 6, Epoch 120/160 (LR 0.01464) => LSC_loss 0.25, Spatial_loss 1.77, Flat_loss 0.17, Train_acc 94.92, Test_acc 54.29
2025-02-14 21:46:16,007 [podnet.py] => Task 6, Epoch 121/160 (LR 0.01396) => LSC_loss 0.25, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.58, Test_acc 55.62
2025-02-14 21:46:18,005 [podnet.py] => Task 6, Epoch 122/160 (LR 0.01328) => LSC_loss 0.24, Spatial_loss 1.73, Flat_loss 0.16, Train_acc 95.12, Test_acc 53.92
2025-02-14 21:46:20,086 [podnet.py] => Task 6, Epoch 123/160 (LR 0.01262) => LSC_loss 0.24, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.45, Test_acc 53.56
2025-02-14 21:46:22,123 [podnet.py] => Task 6, Epoch 124/160 (LR 0.01198) => LSC_loss 0.24, Spatial_loss 1.64, Flat_loss 0.15, Train_acc 95.48, Test_acc 52.75
2025-02-14 21:46:24,130 [podnet.py] => Task 6, Epoch 125/160 (LR 0.01135) => LSC_loss 0.25, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 95.12, Test_acc 54.74
2025-02-14 21:46:26,075 [podnet.py] => Task 6, Epoch 126/160 (LR 0.01073) => LSC_loss 0.25, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 94.95, Test_acc 55.00
2025-02-14 21:46:28,097 [podnet.py] => Task 6, Epoch 127/160 (LR 0.01013) => LSC_loss 0.24, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 95.38, Test_acc 54.28
2025-02-14 21:46:30,103 [podnet.py] => Task 6, Epoch 128/160 (LR 0.00955) => LSC_loss 0.25, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 94.85, Test_acc 56.50
2025-02-14 21:46:32,107 [podnet.py] => Task 6, Epoch 129/160 (LR 0.00898) => LSC_loss 0.24, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 95.40, Test_acc 54.65
2025-02-14 21:46:34,143 [podnet.py] => Task 6, Epoch 130/160 (LR 0.00843) => LSC_loss 0.25, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 94.92, Test_acc 56.30
2025-02-14 21:46:36,204 [podnet.py] => Task 6, Epoch 131/160 (LR 0.00789) => LSC_loss 0.25, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 95.20, Test_acc 55.51
2025-02-14 21:46:38,284 [podnet.py] => Task 6, Epoch 132/160 (LR 0.00737) => LSC_loss 0.23, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.62, Test_acc 55.39
2025-02-14 21:46:40,332 [podnet.py] => Task 6, Epoch 133/160 (LR 0.00686) => LSC_loss 0.25, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 95.82, Test_acc 56.21
2025-02-14 21:46:42,294 [podnet.py] => Task 6, Epoch 134/160 (LR 0.00638) => LSC_loss 0.25, Spatial_loss 1.55, Flat_loss 0.15, Train_acc 95.10, Test_acc 55.79
2025-02-14 21:46:44,278 [podnet.py] => Task 6, Epoch 135/160 (LR 0.00590) => LSC_loss 0.24, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 95.52, Test_acc 56.05
2025-02-14 21:46:46,329 [podnet.py] => Task 6, Epoch 136/160 (LR 0.00545) => LSC_loss 0.23, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 95.48, Test_acc 55.72
2025-02-14 21:46:48,385 [podnet.py] => Task 6, Epoch 137/160 (LR 0.00501) => LSC_loss 0.25, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 94.78, Test_acc 54.99
2025-02-14 21:46:50,437 [podnet.py] => Task 6, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.82, Test_acc 56.80
2025-02-14 21:46:52,460 [podnet.py] => Task 6, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.42, Test_acc 56.14
2025-02-14 21:46:54,470 [podnet.py] => Task 6, Epoch 140/160 (LR 0.00381) => LSC_loss 0.25, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 95.55, Test_acc 55.98
2025-02-14 21:46:56,472 [podnet.py] => Task 6, Epoch 141/160 (LR 0.00344) => LSC_loss 0.25, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 95.40, Test_acc 56.01
2025-02-14 21:46:58,557 [podnet.py] => Task 6, Epoch 142/160 (LR 0.00309) => LSC_loss 0.24, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.68, Test_acc 56.02
2025-02-14 21:47:00,606 [podnet.py] => Task 6, Epoch 143/160 (LR 0.00276) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 94.85, Test_acc 56.31
2025-02-14 21:47:02,583 [podnet.py] => Task 6, Epoch 144/160 (LR 0.00245) => LSC_loss 0.24, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 95.68, Test_acc 56.06
2025-02-14 21:47:04,618 [podnet.py] => Task 6, Epoch 145/160 (LR 0.00215) => LSC_loss 0.25, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 95.05, Test_acc 56.10
2025-02-14 21:47:06,634 [podnet.py] => Task 6, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 95.55, Test_acc 55.70
2025-02-14 21:47:08,701 [podnet.py] => Task 6, Epoch 147/160 (LR 0.00162) => LSC_loss 0.24, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.42, Test_acc 56.14
2025-02-14 21:47:10,725 [podnet.py] => Task 6, Epoch 148/160 (LR 0.00138) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.28, Test_acc 56.28
2025-02-14 21:47:12,776 [podnet.py] => Task 6, Epoch 149/160 (LR 0.00116) => LSC_loss 0.24, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 94.90, Test_acc 56.26
2025-02-14 21:47:14,879 [podnet.py] => Task 6, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 96.32, Test_acc 55.90
2025-02-14 21:47:16,946 [podnet.py] => Task 6, Epoch 151/160 (LR 0.00078) => LSC_loss 0.24, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 95.25, Test_acc 56.45
2025-02-14 21:47:18,980 [podnet.py] => Task 6, Epoch 152/160 (LR 0.00062) => LSC_loss 0.26, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.42, Test_acc 56.14
2025-02-14 21:47:21,036 [podnet.py] => Task 6, Epoch 153/160 (LR 0.00047) => LSC_loss 0.24, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 95.65, Test_acc 56.15
2025-02-14 21:47:23,153 [podnet.py] => Task 6, Epoch 154/160 (LR 0.00035) => LSC_loss 0.25, Spatial_loss 1.41, Flat_loss 0.14, Train_acc 95.45, Test_acc 56.25
2025-02-14 21:47:25,208 [podnet.py] => Task 6, Epoch 155/160 (LR 0.00024) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.80, Test_acc 56.28
2025-02-14 21:47:27,270 [podnet.py] => Task 6, Epoch 156/160 (LR 0.00015) => LSC_loss 0.23, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 95.75, Test_acc 56.54
2025-02-14 21:47:29,350 [podnet.py] => Task 6, Epoch 157/160 (LR 0.00009) => LSC_loss 0.23, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 95.78, Test_acc 56.44
2025-02-14 21:47:31,343 [podnet.py] => Task 6, Epoch 158/160 (LR 0.00004) => LSC_loss 0.24, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 95.32, Test_acc 56.24
2025-02-14 21:47:33,401 [podnet.py] => Task 6, Epoch 159/160 (LR 0.00001) => LSC_loss 0.24, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 95.22, Test_acc 56.36
2025-02-14 21:47:35,449 [podnet.py] => Task 6, Epoch 160/160 (LR 0.00000) => LSC_loss 0.25, Spatial_loss 1.33, Flat_loss 0.14, Train_acc 95.58, Test_acc 56.41
2025-02-14 21:47:35,449 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:47:35,450 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:48:02,499 [podnet.py] => The size of finetune dataset: 1600
2025-02-14 21:48:03,908 [podnet.py] => Task 6, Epoch 1/20 (LR 0.00497) => LSC_loss 0.20, Spatial_loss 1.85, Flat_loss 0.15, Train_acc 94.75, Test_acc 54.84
2025-02-14 21:48:05,313 [podnet.py] => Task 6, Epoch 2/20 (LR 0.00488) => LSC_loss 0.13, Spatial_loss 1.47, Flat_loss 0.08, Train_acc 97.56, Test_acc 57.60
2025-02-14 21:48:06,752 [podnet.py] => Task 6, Epoch 3/20 (LR 0.00473) => LSC_loss 0.12, Spatial_loss 1.45, Flat_loss 0.07, Train_acc 98.19, Test_acc 58.51
2025-02-14 21:48:08,190 [podnet.py] => Task 6, Epoch 4/20 (LR 0.00452) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.00, Test_acc 58.24
2025-02-14 21:48:09,633 [podnet.py] => Task 6, Epoch 5/20 (LR 0.00427) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.25, Test_acc 58.04
2025-02-14 21:48:11,046 [podnet.py] => Task 6, Epoch 6/20 (LR 0.00397) => LSC_loss 0.11, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.75, Test_acc 57.84
2025-02-14 21:48:12,513 [podnet.py] => Task 6, Epoch 7/20 (LR 0.00363) => LSC_loss 0.11, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.69, Test_acc 58.12
2025-02-14 21:48:13,987 [podnet.py] => Task 6, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.19, Test_acc 58.31
2025-02-14 21:48:15,418 [podnet.py] => Task 6, Epoch 9/20 (LR 0.00289) => LSC_loss 0.11, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.69, Test_acc 58.12
2025-02-14 21:48:16,915 [podnet.py] => Task 6, Epoch 10/20 (LR 0.00250) => LSC_loss 0.11, Spatial_loss 1.26, Flat_loss 0.06, Train_acc 98.25, Test_acc 58.10
2025-02-14 21:48:18,393 [podnet.py] => Task 6, Epoch 11/20 (LR 0.00211) => LSC_loss 0.11, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.62, Test_acc 58.22
2025-02-14 21:48:19,898 [podnet.py] => Task 6, Epoch 12/20 (LR 0.00173) => LSC_loss 0.11, Spatial_loss 1.30, Flat_loss 0.05, Train_acc 98.44, Test_acc 58.18
2025-02-14 21:48:21,345 [podnet.py] => Task 6, Epoch 13/20 (LR 0.00137) => LSC_loss 0.11, Spatial_loss 1.25, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.28
2025-02-14 21:48:22,834 [podnet.py] => Task 6, Epoch 14/20 (LR 0.00103) => LSC_loss 0.10, Spatial_loss 1.31, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.16
2025-02-14 21:48:24,285 [podnet.py] => Task 6, Epoch 15/20 (LR 0.00073) => LSC_loss 0.10, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.69, Test_acc 58.04
2025-02-14 21:48:25,755 [podnet.py] => Task 6, Epoch 16/20 (LR 0.00048) => LSC_loss 0.11, Spatial_loss 1.27, Flat_loss 0.05, Train_acc 98.56, Test_acc 58.24
2025-02-14 21:48:27,159 [podnet.py] => Task 6, Epoch 17/20 (LR 0.00027) => LSC_loss 0.11, Spatial_loss 1.26, Flat_loss 0.05, Train_acc 98.69, Test_acc 58.21
2025-02-14 21:48:28,670 [podnet.py] => Task 6, Epoch 18/20 (LR 0.00012) => LSC_loss 0.11, Spatial_loss 1.23, Flat_loss 0.05, Train_acc 98.38, Test_acc 58.10
2025-02-14 21:48:30,144 [podnet.py] => Task 6, Epoch 19/20 (LR 0.00003) => LSC_loss 0.11, Spatial_loss 1.21, Flat_loss 0.05, Train_acc 98.81, Test_acc 58.36
2025-02-14 21:48:31,630 [podnet.py] => Task 6, Epoch 20/20 (LR 0.00000) => LSC_loss 0.11, Spatial_loss 1.19, Flat_loss 0.05, Train_acc 98.75, Test_acc 58.02
2025-02-14 21:48:31,631 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:49:01,126 [podnet.py] => Exemplar size: 1600
2025-02-14 21:49:01,126 [trainer.py] => CNN: {'total': 58.02, '00-09': 66.0, '10-19': 50.2, '20-29': 66.3, '30-39': 57.3, '40-49': 61.8, '50-59': 41.7, '60-69': 57.8, '70-79': 63.1, 'old': 57.73, 'new': 62.4}
2025-02-14 21:49:01,126 [trainer.py] => NME: {'total': 59.02, '00-09': 70.5, '10-19': 56.0, '20-29': 70.4, '30-39': 60.2, '40-49': 64.8, '50-59': 38.9, '60-69': 53.4, '70-79': 58.0, 'old': 59.15, 'new': 57.2}
2025-02-14 21:49:01,127 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02]
2025-02-14 21:49:01,127 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08]
2025-02-14 21:49:01,127 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02]
2025-02-14 21:49:01,127 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16]

2025-02-14 21:49:01,127 [trainer.py] => Average Accuracy (CNN): 67.0442857142857
2025-02-14 21:49:01,127 [trainer.py] => Average Accuracy (NME): 67.26142857142858
2025-02-14 21:49:01,127 [trainer.py] => All params: 517457
2025-02-14 21:49:01,127 [trainer.py] => Trainable params: 517457
2025-02-14 21:49:01,128 [podnet.py] => Learning on 80-85
2025-02-14 21:49:01,160 [podnet.py] => Adaptive factor: 4.123105625617661
2025-02-14 21:49:03,471 [podnet.py] => Task 7, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 4.29, Flat_loss 0.89, Train_acc 69.02, Test_acc 15.26
2025-02-14 21:49:05,591 [podnet.py] => Task 7, Epoch 2/160 (LR 0.09996) => LSC_loss 1.96, Spatial_loss 7.49, Flat_loss 1.64, Train_acc 60.10, Test_acc 23.33
2025-02-14 21:49:07,656 [podnet.py] => Task 7, Epoch 3/160 (LR 0.09991) => LSC_loss 1.31, Spatial_loss 6.18, Flat_loss 1.27, Train_acc 68.98, Test_acc 22.01
2025-02-14 21:49:09,814 [podnet.py] => Task 7, Epoch 4/160 (LR 0.09985) => LSC_loss 1.18, Spatial_loss 6.33, Flat_loss 1.20, Train_acc 70.41, Test_acc 40.25
2025-02-14 21:49:11,884 [podnet.py] => Task 7, Epoch 5/160 (LR 0.09976) => LSC_loss 0.88, Spatial_loss 5.42, Flat_loss 0.97, Train_acc 76.98, Test_acc 36.62
2025-02-14 21:49:13,958 [podnet.py] => Task 7, Epoch 6/160 (LR 0.09965) => LSC_loss 0.82, Spatial_loss 5.14, Flat_loss 0.87, Train_acc 78.90, Test_acc 41.51
2025-02-14 21:49:16,029 [podnet.py] => Task 7, Epoch 7/160 (LR 0.09953) => LSC_loss 0.74, Spatial_loss 5.02, Flat_loss 0.83, Train_acc 80.34, Test_acc 44.76
2025-02-14 21:49:18,131 [podnet.py] => Task 7, Epoch 8/160 (LR 0.09938) => LSC_loss 0.71, Spatial_loss 4.81, Flat_loss 0.76, Train_acc 82.22, Test_acc 39.46
2025-02-14 21:49:20,202 [podnet.py] => Task 7, Epoch 9/160 (LR 0.09922) => LSC_loss 0.69, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 81.51, Test_acc 38.56
2025-02-14 21:49:22,323 [podnet.py] => Task 7, Epoch 10/160 (LR 0.09904) => LSC_loss 0.77, Spatial_loss 5.19, Flat_loss 0.84, Train_acc 80.05, Test_acc 40.32
2025-02-14 21:49:24,369 [podnet.py] => Task 7, Epoch 11/160 (LR 0.09884) => LSC_loss 0.82, Spatial_loss 5.41, Flat_loss 0.86, Train_acc 79.68, Test_acc 24.84
2025-02-14 21:49:26,437 [podnet.py] => Task 7, Epoch 12/160 (LR 0.09862) => LSC_loss 0.83, Spatial_loss 5.33, Flat_loss 0.93, Train_acc 79.41, Test_acc 37.15
2025-02-14 21:49:28,527 [podnet.py] => Task 7, Epoch 13/160 (LR 0.09838) => LSC_loss 0.67, Spatial_loss 5.03, Flat_loss 0.78, Train_acc 82.49, Test_acc 42.59
2025-02-14 21:49:30,623 [podnet.py] => Task 7, Epoch 14/160 (LR 0.09812) => LSC_loss 0.54, Spatial_loss 4.44, Flat_loss 0.66, Train_acc 86.27, Test_acc 43.21
2025-02-14 21:49:32,697 [podnet.py] => Task 7, Epoch 15/160 (LR 0.09785) => LSC_loss 0.48, Spatial_loss 4.30, Flat_loss 0.60, Train_acc 87.88, Test_acc 45.96
2025-02-14 21:49:34,794 [podnet.py] => Task 7, Epoch 16/160 (LR 0.09755) => LSC_loss 0.51, Spatial_loss 4.36, Flat_loss 0.61, Train_acc 87.61, Test_acc 30.49
2025-02-14 21:49:36,931 [podnet.py] => Task 7, Epoch 17/160 (LR 0.09724) => LSC_loss 0.76, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 82.07, Test_acc 44.40
2025-02-14 21:49:39,027 [podnet.py] => Task 7, Epoch 18/160 (LR 0.09691) => LSC_loss 0.66, Spatial_loss 4.70, Flat_loss 0.69, Train_acc 85.22, Test_acc 44.59
2025-02-14 21:49:41,106 [podnet.py] => Task 7, Epoch 19/160 (LR 0.09656) => LSC_loss 0.63, Spatial_loss 4.74, Flat_loss 0.71, Train_acc 83.98, Test_acc 43.38
2025-02-14 21:49:43,199 [podnet.py] => Task 7, Epoch 20/160 (LR 0.09619) => LSC_loss 0.59, Spatial_loss 4.65, Flat_loss 0.68, Train_acc 85.49, Test_acc 44.56
2025-02-14 21:49:45,338 [podnet.py] => Task 7, Epoch 21/160 (LR 0.09581) => LSC_loss 0.44, Spatial_loss 4.15, Flat_loss 0.56, Train_acc 88.90, Test_acc 48.98
2025-02-14 21:49:47,463 [podnet.py] => Task 7, Epoch 22/160 (LR 0.09541) => LSC_loss 0.39, Spatial_loss 3.92, Flat_loss 0.49, Train_acc 91.05, Test_acc 46.92
2025-02-14 21:49:49,532 [podnet.py] => Task 7, Epoch 23/160 (LR 0.09499) => LSC_loss 0.40, Spatial_loss 3.90, Flat_loss 0.49, Train_acc 90.41, Test_acc 49.24
2025-02-14 21:49:51,659 [podnet.py] => Task 7, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.80, Flat_loss 0.48, Train_acc 91.39, Test_acc 51.48
2025-02-14 21:49:53,782 [podnet.py] => Task 7, Epoch 25/160 (LR 0.09410) => LSC_loss 0.39, Spatial_loss 3.68, Flat_loss 0.44, Train_acc 92.10, Test_acc 43.84
2025-02-14 21:49:55,905 [podnet.py] => Task 7, Epoch 26/160 (LR 0.09362) => LSC_loss 0.43, Spatial_loss 3.98, Flat_loss 0.52, Train_acc 88.78, Test_acc 50.72
2025-02-14 21:49:57,963 [podnet.py] => Task 7, Epoch 27/160 (LR 0.09314) => LSC_loss 0.37, Spatial_loss 3.60, Flat_loss 0.43, Train_acc 92.59, Test_acc 46.45
2025-02-14 21:50:00,048 [podnet.py] => Task 7, Epoch 28/160 (LR 0.09263) => LSC_loss 0.44, Spatial_loss 3.88, Flat_loss 0.50, Train_acc 89.71, Test_acc 48.05
2025-02-14 21:50:02,136 [podnet.py] => Task 7, Epoch 29/160 (LR 0.09211) => LSC_loss 0.53, Spatial_loss 4.46, Flat_loss 0.59, Train_acc 86.80, Test_acc 44.42
2025-02-14 21:50:04,213 [podnet.py] => Task 7, Epoch 30/160 (LR 0.09157) => LSC_loss 0.51, Spatial_loss 4.26, Flat_loss 0.56, Train_acc 86.51, Test_acc 43.78
2025-02-14 21:50:06,267 [podnet.py] => Task 7, Epoch 31/160 (LR 0.09102) => LSC_loss 0.53, Spatial_loss 4.39, Flat_loss 0.64, Train_acc 86.17, Test_acc 37.99
2025-02-14 21:50:08,329 [podnet.py] => Task 7, Epoch 32/160 (LR 0.09045) => LSC_loss 0.48, Spatial_loss 4.36, Flat_loss 0.60, Train_acc 88.44, Test_acc 41.18
2025-02-14 21:50:10,433 [podnet.py] => Task 7, Epoch 33/160 (LR 0.08987) => LSC_loss 0.41, Spatial_loss 3.90, Flat_loss 0.50, Train_acc 90.85, Test_acc 49.82
2025-02-14 21:50:12,582 [podnet.py] => Task 7, Epoch 34/160 (LR 0.08927) => LSC_loss 0.45, Spatial_loss 4.08, Flat_loss 0.54, Train_acc 88.83, Test_acc 47.73
2025-02-14 21:50:14,626 [podnet.py] => Task 7, Epoch 35/160 (LR 0.08865) => LSC_loss 0.42, Spatial_loss 3.85, Flat_loss 0.51, Train_acc 90.95, Test_acc 39.74
2025-02-14 21:50:16,802 [podnet.py] => Task 7, Epoch 36/160 (LR 0.08802) => LSC_loss 0.61, Spatial_loss 4.70, Flat_loss 0.68, Train_acc 85.93, Test_acc 44.15
2025-02-14 21:50:18,867 [podnet.py] => Task 7, Epoch 37/160 (LR 0.08738) => LSC_loss 0.55, Spatial_loss 4.51, Flat_loss 0.63, Train_acc 85.29, Test_acc 42.72
2025-02-14 21:50:20,974 [podnet.py] => Task 7, Epoch 38/160 (LR 0.08672) => LSC_loss 0.50, Spatial_loss 4.35, Flat_loss 0.60, Train_acc 88.93, Test_acc 40.95
2025-02-14 21:50:23,044 [podnet.py] => Task 7, Epoch 39/160 (LR 0.08604) => LSC_loss 0.46, Spatial_loss 4.20, Flat_loss 0.58, Train_acc 88.27, Test_acc 47.59
2025-02-14 21:50:25,116 [podnet.py] => Task 7, Epoch 40/160 (LR 0.08536) => LSC_loss 0.43, Spatial_loss 3.91, Flat_loss 0.50, Train_acc 91.20, Test_acc 46.71
2025-02-14 21:50:27,230 [podnet.py] => Task 7, Epoch 41/160 (LR 0.08465) => LSC_loss 0.44, Spatial_loss 4.08, Flat_loss 0.53, Train_acc 89.44, Test_acc 45.46
2025-02-14 21:50:29,355 [podnet.py] => Task 7, Epoch 42/160 (LR 0.08394) => LSC_loss 0.48, Spatial_loss 4.20, Flat_loss 0.56, Train_acc 88.51, Test_acc 48.08
2025-02-14 21:50:31,420 [podnet.py] => Task 7, Epoch 43/160 (LR 0.08321) => LSC_loss 0.67, Spatial_loss 4.81, Flat_loss 0.69, Train_acc 82.63, Test_acc 41.80
2025-02-14 21:50:33,527 [podnet.py] => Task 7, Epoch 44/160 (LR 0.08247) => LSC_loss 0.53, Spatial_loss 4.42, Flat_loss 0.63, Train_acc 86.51, Test_acc 45.07
2025-02-14 21:50:35,615 [podnet.py] => Task 7, Epoch 45/160 (LR 0.08172) => LSC_loss 0.50, Spatial_loss 4.19, Flat_loss 0.58, Train_acc 89.24, Test_acc 44.51
2025-02-14 21:50:37,772 [podnet.py] => Task 7, Epoch 46/160 (LR 0.08095) => LSC_loss 0.50, Spatial_loss 4.58, Flat_loss 0.64, Train_acc 87.49, Test_acc 44.64
2025-02-14 21:50:39,812 [podnet.py] => Task 7, Epoch 47/160 (LR 0.08018) => LSC_loss 0.42, Spatial_loss 4.13, Flat_loss 0.55, Train_acc 89.80, Test_acc 49.64
2025-02-14 21:50:41,968 [podnet.py] => Task 7, Epoch 48/160 (LR 0.07939) => LSC_loss 0.43, Spatial_loss 3.79, Flat_loss 0.49, Train_acc 92.34, Test_acc 34.29
2025-02-14 21:50:44,069 [podnet.py] => Task 7, Epoch 49/160 (LR 0.07859) => LSC_loss 0.76, Spatial_loss 5.15, Flat_loss 0.78, Train_acc 82.00, Test_acc 34.24
2025-02-14 21:50:46,162 [podnet.py] => Task 7, Epoch 50/160 (LR 0.07778) => LSC_loss 0.70, Spatial_loss 5.02, Flat_loss 0.79, Train_acc 82.66, Test_acc 42.85
2025-02-14 21:50:48,299 [podnet.py] => Task 7, Epoch 51/160 (LR 0.07696) => LSC_loss 0.51, Spatial_loss 4.52, Flat_loss 0.65, Train_acc 87.49, Test_acc 47.14
2025-02-14 21:50:50,360 [podnet.py] => Task 7, Epoch 52/160 (LR 0.07612) => LSC_loss 0.47, Spatial_loss 4.28, Flat_loss 0.59, Train_acc 88.95, Test_acc 41.54
2025-02-14 21:50:52,477 [podnet.py] => Task 7, Epoch 53/160 (LR 0.07528) => LSC_loss 0.52, Spatial_loss 4.29, Flat_loss 0.62, Train_acc 88.39, Test_acc 43.86
2025-02-14 21:50:54,613 [podnet.py] => Task 7, Epoch 54/160 (LR 0.07443) => LSC_loss 0.54, Spatial_loss 4.51, Flat_loss 0.64, Train_acc 86.61, Test_acc 45.40
2025-02-14 21:50:56,693 [podnet.py] => Task 7, Epoch 55/160 (LR 0.07357) => LSC_loss 0.46, Spatial_loss 4.31, Flat_loss 0.62, Train_acc 88.66, Test_acc 49.44
2025-02-14 21:50:58,827 [podnet.py] => Task 7, Epoch 56/160 (LR 0.07270) => LSC_loss 0.36, Spatial_loss 4.06, Flat_loss 0.53, Train_acc 91.68, Test_acc 48.33
2025-02-14 21:51:00,909 [podnet.py] => Task 7, Epoch 57/160 (LR 0.07182) => LSC_loss 0.38, Spatial_loss 3.83, Flat_loss 0.48, Train_acc 91.78, Test_acc 47.21
2025-02-14 21:51:03,067 [podnet.py] => Task 7, Epoch 58/160 (LR 0.07093) => LSC_loss 0.39, Spatial_loss 3.94, Flat_loss 0.49, Train_acc 91.51, Test_acc 49.44
2025-02-14 21:51:05,173 [podnet.py] => Task 7, Epoch 59/160 (LR 0.07004) => LSC_loss 0.41, Spatial_loss 3.97, Flat_loss 0.51, Train_acc 90.98, Test_acc 41.04
2025-02-14 21:51:07,291 [podnet.py] => Task 7, Epoch 60/160 (LR 0.06913) => LSC_loss 0.47, Spatial_loss 4.08, Flat_loss 0.54, Train_acc 89.88, Test_acc 48.69
2025-02-14 21:51:09,387 [podnet.py] => Task 7, Epoch 61/160 (LR 0.06822) => LSC_loss 0.52, Spatial_loss 4.36, Flat_loss 0.60, Train_acc 87.32, Test_acc 47.47
2025-02-14 21:51:11,490 [podnet.py] => Task 7, Epoch 62/160 (LR 0.06731) => LSC_loss 0.38, Spatial_loss 3.99, Flat_loss 0.51, Train_acc 91.34, Test_acc 49.22
2025-02-14 21:51:13,560 [podnet.py] => Task 7, Epoch 63/160 (LR 0.06638) => LSC_loss 0.41, Spatial_loss 4.02, Flat_loss 0.52, Train_acc 90.61, Test_acc 47.80
2025-02-14 21:51:15,674 [podnet.py] => Task 7, Epoch 64/160 (LR 0.06545) => LSC_loss 0.48, Spatial_loss 3.80, Flat_loss 0.49, Train_acc 92.22, Test_acc 42.99
2025-02-14 21:51:17,739 [podnet.py] => Task 7, Epoch 65/160 (LR 0.06451) => LSC_loss 1.15, Spatial_loss 6.15, Flat_loss 1.00, Train_acc 73.61, Test_acc 42.86
2025-02-14 21:51:19,775 [podnet.py] => Task 7, Epoch 66/160 (LR 0.06357) => LSC_loss 0.66, Spatial_loss 4.87, Flat_loss 0.74, Train_acc 82.88, Test_acc 44.31
2025-02-14 21:51:21,886 [podnet.py] => Task 7, Epoch 67/160 (LR 0.06262) => LSC_loss 0.46, Spatial_loss 4.32, Flat_loss 0.60, Train_acc 88.78, Test_acc 48.94
2025-02-14 21:51:23,957 [podnet.py] => Task 7, Epoch 68/160 (LR 0.06167) => LSC_loss 0.51, Spatial_loss 4.13, Flat_loss 0.58, Train_acc 89.29, Test_acc 50.01
2025-02-14 21:51:25,992 [podnet.py] => Task 7, Epoch 69/160 (LR 0.06072) => LSC_loss 0.39, Spatial_loss 3.96, Flat_loss 0.54, Train_acc 90.88, Test_acc 47.35
2025-02-14 21:51:28,113 [podnet.py] => Task 7, Epoch 70/160 (LR 0.05975) => LSC_loss 0.36, Spatial_loss 3.66, Flat_loss 0.45, Train_acc 92.83, Test_acc 52.21
2025-02-14 21:51:30,172 [podnet.py] => Task 7, Epoch 71/160 (LR 0.05879) => LSC_loss 0.37, Spatial_loss 3.75, Flat_loss 0.47, Train_acc 92.68, Test_acc 44.64
2025-02-14 21:51:32,266 [podnet.py] => Task 7, Epoch 72/160 (LR 0.05782) => LSC_loss 0.50, Spatial_loss 4.37, Flat_loss 0.60, Train_acc 87.98, Test_acc 49.16
2025-02-14 21:51:34,330 [podnet.py] => Task 7, Epoch 73/160 (LR 0.05685) => LSC_loss 0.39, Spatial_loss 3.84, Flat_loss 0.52, Train_acc 91.49, Test_acc 49.32
2025-02-14 21:51:36,443 [podnet.py] => Task 7, Epoch 74/160 (LR 0.05588) => LSC_loss 0.35, Spatial_loss 3.64, Flat_loss 0.45, Train_acc 93.27, Test_acc 48.75
2025-02-14 21:51:38,513 [podnet.py] => Task 7, Epoch 75/160 (LR 0.05490) => LSC_loss 0.49, Spatial_loss 4.17, Flat_loss 0.56, Train_acc 89.00, Test_acc 49.36
2025-02-14 21:51:40,593 [podnet.py] => Task 7, Epoch 76/160 (LR 0.05392) => LSC_loss 0.44, Spatial_loss 3.92, Flat_loss 0.53, Train_acc 90.54, Test_acc 46.65
2025-02-14 21:51:42,627 [podnet.py] => Task 7, Epoch 77/160 (LR 0.05294) => LSC_loss 0.62, Spatial_loss 4.83, Flat_loss 0.68, Train_acc 84.32, Test_acc 45.33
2025-02-14 21:51:44,759 [podnet.py] => Task 7, Epoch 78/160 (LR 0.05196) => LSC_loss 0.55, Spatial_loss 4.34, Flat_loss 0.62, Train_acc 86.76, Test_acc 49.84
2025-02-14 21:51:46,825 [podnet.py] => Task 7, Epoch 79/160 (LR 0.05098) => LSC_loss 0.33, Spatial_loss 3.63, Flat_loss 0.46, Train_acc 92.39, Test_acc 52.07
2025-02-14 21:51:48,911 [podnet.py] => Task 7, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 3.44, Flat_loss 0.42, Train_acc 94.66, Test_acc 50.52
2025-02-14 21:51:50,986 [podnet.py] => Task 7, Epoch 81/160 (LR 0.04902) => LSC_loss 0.38, Spatial_loss 3.73, Flat_loss 0.46, Train_acc 92.66, Test_acc 50.02
2025-02-14 21:51:53,093 [podnet.py] => Task 7, Epoch 82/160 (LR 0.04804) => LSC_loss 0.41, Spatial_loss 3.78, Flat_loss 0.49, Train_acc 91.46, Test_acc 50.60
2025-02-14 21:51:55,175 [podnet.py] => Task 7, Epoch 83/160 (LR 0.04706) => LSC_loss 0.36, Spatial_loss 3.76, Flat_loss 0.46, Train_acc 92.32, Test_acc 49.78
2025-02-14 21:51:57,280 [podnet.py] => Task 7, Epoch 84/160 (LR 0.04608) => LSC_loss 0.33, Spatial_loss 3.77, Flat_loss 0.46, Train_acc 92.73, Test_acc 51.22
2025-02-14 21:51:59,362 [podnet.py] => Task 7, Epoch 85/160 (LR 0.04510) => LSC_loss 0.28, Spatial_loss 3.34, Flat_loss 0.40, Train_acc 94.59, Test_acc 51.74
2025-02-14 21:52:01,452 [podnet.py] => Task 7, Epoch 86/160 (LR 0.04412) => LSC_loss 0.27, Spatial_loss 3.20, Flat_loss 0.38, Train_acc 95.71, Test_acc 53.61
2025-02-14 21:52:03,501 [podnet.py] => Task 7, Epoch 87/160 (LR 0.04315) => LSC_loss 0.32, Spatial_loss 3.36, Flat_loss 0.41, Train_acc 93.37, Test_acc 50.39
2025-02-14 21:52:05,566 [podnet.py] => Task 7, Epoch 88/160 (LR 0.04218) => LSC_loss 0.32, Spatial_loss 3.42, Flat_loss 0.42, Train_acc 93.10, Test_acc 52.01
2025-02-14 21:52:07,695 [podnet.py] => Task 7, Epoch 89/160 (LR 0.04121) => LSC_loss 0.25, Spatial_loss 3.20, Flat_loss 0.36, Train_acc 95.27, Test_acc 53.24
2025-02-14 21:52:09,776 [podnet.py] => Task 7, Epoch 90/160 (LR 0.04025) => LSC_loss 0.29, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 96.12, Test_acc 51.39
2025-02-14 21:52:11,898 [podnet.py] => Task 7, Epoch 91/160 (LR 0.03928) => LSC_loss 0.30, Spatial_loss 3.26, Flat_loss 0.40, Train_acc 93.07, Test_acc 52.34
2025-02-14 21:52:13,997 [podnet.py] => Task 7, Epoch 92/160 (LR 0.03833) => LSC_loss 0.27, Spatial_loss 3.17, Flat_loss 0.36, Train_acc 95.41, Test_acc 52.45
2025-02-14 21:52:16,128 [podnet.py] => Task 7, Epoch 93/160 (LR 0.03738) => LSC_loss 0.32, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 95.17, Test_acc 53.09
2025-02-14 21:52:18,248 [podnet.py] => Task 7, Epoch 94/160 (LR 0.03643) => LSC_loss 0.27, Spatial_loss 3.23, Flat_loss 0.37, Train_acc 94.68, Test_acc 51.40
2025-02-14 21:52:20,313 [podnet.py] => Task 7, Epoch 95/160 (LR 0.03549) => LSC_loss 0.25, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 95.41, Test_acc 53.54
2025-02-14 21:52:22,389 [podnet.py] => Task 7, Epoch 96/160 (LR 0.03455) => LSC_loss 0.26, Spatial_loss 3.04, Flat_loss 0.34, Train_acc 95.00, Test_acc 53.20
2025-02-14 21:52:24,459 [podnet.py] => Task 7, Epoch 97/160 (LR 0.03362) => LSC_loss 0.26, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 95.71, Test_acc 51.09
2025-02-14 21:52:26,562 [podnet.py] => Task 7, Epoch 98/160 (LR 0.03269) => LSC_loss 0.43, Spatial_loss 3.59, Flat_loss 0.46, Train_acc 90.88, Test_acc 50.60
2025-02-14 21:52:28,675 [podnet.py] => Task 7, Epoch 99/160 (LR 0.03178) => LSC_loss 0.31, Spatial_loss 3.16, Flat_loss 0.37, Train_acc 94.56, Test_acc 54.12
2025-02-14 21:52:30,770 [podnet.py] => Task 7, Epoch 100/160 (LR 0.03087) => LSC_loss 0.28, Spatial_loss 3.11, Flat_loss 0.36, Train_acc 95.05, Test_acc 52.33
2025-02-14 21:52:32,818 [podnet.py] => Task 7, Epoch 101/160 (LR 0.02996) => LSC_loss 0.28, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 95.29, Test_acc 54.06
2025-02-14 21:52:34,885 [podnet.py] => Task 7, Epoch 102/160 (LR 0.02907) => LSC_loss 0.31, Spatial_loss 3.23, Flat_loss 0.39, Train_acc 94.46, Test_acc 52.58
2025-02-14 21:52:37,002 [podnet.py] => Task 7, Epoch 103/160 (LR 0.02818) => LSC_loss 0.30, Spatial_loss 3.05, Flat_loss 0.36, Train_acc 95.41, Test_acc 51.40
2025-02-14 21:52:39,078 [podnet.py] => Task 7, Epoch 104/160 (LR 0.02730) => LSC_loss 0.27, Spatial_loss 3.01, Flat_loss 0.34, Train_acc 94.90, Test_acc 52.41
2025-02-14 21:52:41,126 [podnet.py] => Task 7, Epoch 105/160 (LR 0.02643) => LSC_loss 0.25, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 95.71, Test_acc 53.94
2025-02-14 21:52:43,188 [podnet.py] => Task 7, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 2.88, Flat_loss 0.32, Train_acc 95.49, Test_acc 53.28
2025-02-14 21:52:45,255 [podnet.py] => Task 7, Epoch 107/160 (LR 0.02472) => LSC_loss 0.30, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 94.83, Test_acc 52.71
2025-02-14 21:52:47,382 [podnet.py] => Task 7, Epoch 108/160 (LR 0.02388) => LSC_loss 0.30, Spatial_loss 2.98, Flat_loss 0.35, Train_acc 95.34, Test_acc 54.69
2025-02-14 21:52:49,469 [podnet.py] => Task 7, Epoch 109/160 (LR 0.02304) => LSC_loss 0.24, Spatial_loss 2.83, Flat_loss 0.32, Train_acc 94.83, Test_acc 53.49
2025-02-14 21:52:51,597 [podnet.py] => Task 7, Epoch 110/160 (LR 0.02222) => LSC_loss 0.23, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 96.44, Test_acc 54.87
2025-02-14 21:52:53,720 [podnet.py] => Task 7, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 95.98, Test_acc 53.78
2025-02-14 21:52:55,810 [podnet.py] => Task 7, Epoch 112/160 (LR 0.02061) => LSC_loss 0.28, Spatial_loss 2.97, Flat_loss 0.34, Train_acc 94.54, Test_acc 53.36
2025-02-14 21:52:57,943 [podnet.py] => Task 7, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 96.39, Test_acc 53.47
2025-02-14 21:53:00,031 [podnet.py] => Task 7, Epoch 114/160 (LR 0.01905) => LSC_loss 0.26, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 95.76, Test_acc 52.72
2025-02-14 21:53:02,144 [podnet.py] => Task 7, Epoch 115/160 (LR 0.01828) => LSC_loss 0.28, Spatial_loss 2.73, Flat_loss 0.31, Train_acc 95.85, Test_acc 52.55
2025-02-14 21:53:04,275 [podnet.py] => Task 7, Epoch 116/160 (LR 0.01753) => LSC_loss 0.33, Spatial_loss 2.93, Flat_loss 0.33, Train_acc 95.32, Test_acc 54.80
2025-02-14 21:53:06,351 [podnet.py] => Task 7, Epoch 117/160 (LR 0.01679) => LSC_loss 0.34, Spatial_loss 2.93, Flat_loss 0.33, Train_acc 94.80, Test_acc 53.72
2025-02-14 21:53:08,448 [podnet.py] => Task 7, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 2.75, Flat_loss 0.31, Train_acc 96.51, Test_acc 54.67
2025-02-14 21:53:10,621 [podnet.py] => Task 7, Epoch 119/160 (LR 0.01535) => LSC_loss 0.22, Spatial_loss 2.60, Flat_loss 0.28, Train_acc 96.07, Test_acc 54.28
2025-02-14 21:53:12,724 [podnet.py] => Task 7, Epoch 120/160 (LR 0.01464) => LSC_loss 0.21, Spatial_loss 2.51, Flat_loss 0.26, Train_acc 96.78, Test_acc 54.22
2025-02-14 21:53:14,784 [podnet.py] => Task 7, Epoch 121/160 (LR 0.01396) => LSC_loss 0.22, Spatial_loss 2.50, Flat_loss 0.25, Train_acc 96.71, Test_acc 53.88
2025-02-14 21:53:16,910 [podnet.py] => Task 7, Epoch 122/160 (LR 0.01328) => LSC_loss 0.23, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 97.20, Test_acc 53.39
2025-02-14 21:53:18,976 [podnet.py] => Task 7, Epoch 123/160 (LR 0.01262) => LSC_loss 0.31, Spatial_loss 2.69, Flat_loss 0.30, Train_acc 96.05, Test_acc 53.76
2025-02-14 21:53:21,045 [podnet.py] => Task 7, Epoch 124/160 (LR 0.01198) => LSC_loss 0.26, Spatial_loss 2.84, Flat_loss 0.32, Train_acc 94.73, Test_acc 54.73
2025-02-14 21:53:23,107 [podnet.py] => Task 7, Epoch 125/160 (LR 0.01135) => LSC_loss 0.23, Spatial_loss 2.48, Flat_loss 0.28, Train_acc 96.80, Test_acc 54.73
2025-02-14 21:53:25,204 [podnet.py] => Task 7, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 2.60, Flat_loss 0.30, Train_acc 96.12, Test_acc 54.49
2025-02-14 21:53:27,289 [podnet.py] => Task 7, Epoch 127/160 (LR 0.01013) => LSC_loss 0.23, Spatial_loss 2.61, Flat_loss 0.30, Train_acc 96.10, Test_acc 54.92
2025-02-14 21:53:29,364 [podnet.py] => Task 7, Epoch 128/160 (LR 0.00955) => LSC_loss 0.28, Spatial_loss 2.60, Flat_loss 0.28, Train_acc 96.02, Test_acc 54.36
2025-02-14 21:53:31,472 [podnet.py] => Task 7, Epoch 129/160 (LR 0.00898) => LSC_loss 0.22, Spatial_loss 2.53, Flat_loss 0.28, Train_acc 96.78, Test_acc 55.42
2025-02-14 21:53:33,636 [podnet.py] => Task 7, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 96.76, Test_acc 54.71
2025-02-14 21:53:35,746 [podnet.py] => Task 7, Epoch 131/160 (LR 0.00789) => LSC_loss 0.20, Spatial_loss 2.42, Flat_loss 0.25, Train_acc 97.59, Test_acc 55.09
2025-02-14 21:53:37,855 [podnet.py] => Task 7, Epoch 132/160 (LR 0.00737) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.26, Train_acc 96.68, Test_acc 55.24
2025-02-14 21:53:39,964 [podnet.py] => Task 7, Epoch 133/160 (LR 0.00686) => LSC_loss 0.30, Spatial_loss 2.49, Flat_loss 0.28, Train_acc 96.66, Test_acc 55.89
2025-02-14 21:53:41,980 [podnet.py] => Task 7, Epoch 134/160 (LR 0.00638) => LSC_loss 0.24, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 95.54, Test_acc 53.99
2025-02-14 21:53:44,089 [podnet.py] => Task 7, Epoch 135/160 (LR 0.00590) => LSC_loss 0.21, Spatial_loss 2.46, Flat_loss 0.25, Train_acc 96.32, Test_acc 55.29
2025-02-14 21:53:46,181 [podnet.py] => Task 7, Epoch 136/160 (LR 0.00545) => LSC_loss 0.21, Spatial_loss 2.40, Flat_loss 0.24, Train_acc 97.00, Test_acc 55.22
2025-02-14 21:53:48,220 [podnet.py] => Task 7, Epoch 137/160 (LR 0.00501) => LSC_loss 0.24, Spatial_loss 2.32, Flat_loss 0.25, Train_acc 96.93, Test_acc 55.01
2025-02-14 21:53:50,339 [podnet.py] => Task 7, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 97.02, Test_acc 54.39
2025-02-14 21:53:52,441 [podnet.py] => Task 7, Epoch 139/160 (LR 0.00419) => LSC_loss 0.24, Spatial_loss 2.39, Flat_loss 0.26, Train_acc 97.05, Test_acc 55.38
2025-02-14 21:53:54,521 [podnet.py] => Task 7, Epoch 140/160 (LR 0.00381) => LSC_loss 0.21, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.15, Test_acc 55.06
2025-02-14 21:53:56,688 [podnet.py] => Task 7, Epoch 141/160 (LR 0.00344) => LSC_loss 0.20, Spatial_loss 2.26, Flat_loss 0.24, Train_acc 97.27, Test_acc 55.28
2025-02-14 21:53:58,768 [podnet.py] => Task 7, Epoch 142/160 (LR 0.00309) => LSC_loss 0.21, Spatial_loss 2.26, Flat_loss 0.24, Train_acc 97.27, Test_acc 55.12
2025-02-14 21:54:00,875 [podnet.py] => Task 7, Epoch 143/160 (LR 0.00276) => LSC_loss 0.24, Spatial_loss 2.25, Flat_loss 0.24, Train_acc 97.29, Test_acc 55.33
2025-02-14 21:54:02,943 [podnet.py] => Task 7, Epoch 144/160 (LR 0.00245) => LSC_loss 0.22, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 96.78, Test_acc 55.39
2025-02-14 21:54:05,060 [podnet.py] => Task 7, Epoch 145/160 (LR 0.00215) => LSC_loss 0.20, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.46, Test_acc 55.34
2025-02-14 21:54:07,202 [podnet.py] => Task 7, Epoch 146/160 (LR 0.00188) => LSC_loss 0.20, Spatial_loss 2.23, Flat_loss 0.24, Train_acc 97.37, Test_acc 55.46
2025-02-14 21:54:09,341 [podnet.py] => Task 7, Epoch 147/160 (LR 0.00162) => LSC_loss 0.21, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.12, Test_acc 54.87
2025-02-14 21:54:11,439 [podnet.py] => Task 7, Epoch 148/160 (LR 0.00138) => LSC_loss 0.21, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 97.44, Test_acc 55.00
2025-02-14 21:54:13,541 [podnet.py] => Task 7, Epoch 149/160 (LR 0.00116) => LSC_loss 0.22, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 97.10, Test_acc 55.29
2025-02-14 21:54:15,626 [podnet.py] => Task 7, Epoch 150/160 (LR 0.00096) => LSC_loss 0.20, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 97.83, Test_acc 55.80
2025-02-14 21:54:17,777 [podnet.py] => Task 7, Epoch 151/160 (LR 0.00078) => LSC_loss 0.21, Spatial_loss 2.25, Flat_loss 0.24, Train_acc 97.05, Test_acc 55.58
2025-02-14 21:54:19,830 [podnet.py] => Task 7, Epoch 152/160 (LR 0.00062) => LSC_loss 0.19, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 97.05, Test_acc 54.82
2025-02-14 21:54:21,938 [podnet.py] => Task 7, Epoch 153/160 (LR 0.00047) => LSC_loss 0.22, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.22, Test_acc 55.28
2025-02-14 21:54:24,032 [podnet.py] => Task 7, Epoch 154/160 (LR 0.00035) => LSC_loss 0.20, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 97.00, Test_acc 55.18
2025-02-14 21:54:26,108 [podnet.py] => Task 7, Epoch 155/160 (LR 0.00024) => LSC_loss 0.23, Spatial_loss 2.25, Flat_loss 0.23, Train_acc 97.56, Test_acc 54.87
2025-02-14 21:54:28,187 [podnet.py] => Task 7, Epoch 156/160 (LR 0.00015) => LSC_loss 0.19, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 97.59, Test_acc 55.01
2025-02-14 21:54:30,309 [podnet.py] => Task 7, Epoch 157/160 (LR 0.00009) => LSC_loss 0.22, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 97.39, Test_acc 55.33
2025-02-14 21:54:32,409 [podnet.py] => Task 7, Epoch 158/160 (LR 0.00004) => LSC_loss 0.21, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 97.41, Test_acc 54.96
2025-02-14 21:54:34,531 [podnet.py] => Task 7, Epoch 159/160 (LR 0.00001) => LSC_loss 0.22, Spatial_loss 2.19, Flat_loss 0.24, Train_acc 97.00, Test_acc 55.09
2025-02-14 21:54:36,688 [podnet.py] => Task 7, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 97.37, Test_acc 54.89
2025-02-14 21:54:36,688 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 21:54:36,689 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:55:05,757 [podnet.py] => The size of finetune dataset: 1700
2025-02-14 21:55:07,223 [podnet.py] => Task 7, Epoch 1/20 (LR 0.00497) => LSC_loss 0.26, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 93.59, Test_acc 55.02
2025-02-14 21:55:08,778 [podnet.py] => Task 7, Epoch 2/20 (LR 0.00488) => LSC_loss 0.16, Spatial_loss 2.23, Flat_loss 0.18, Train_acc 98.06, Test_acc 55.56
2025-02-14 21:55:10,324 [podnet.py] => Task 7, Epoch 3/20 (LR 0.00473) => LSC_loss 0.14, Spatial_loss 2.21, Flat_loss 0.17, Train_acc 98.65, Test_acc 55.80
2025-02-14 21:55:11,845 [podnet.py] => Task 7, Epoch 4/20 (LR 0.00452) => LSC_loss 0.14, Spatial_loss 2.20, Flat_loss 0.17, Train_acc 98.94, Test_acc 56.45
2025-02-14 21:55:13,409 [podnet.py] => Task 7, Epoch 5/20 (LR 0.00427) => LSC_loss 0.14, Spatial_loss 2.13, Flat_loss 0.16, Train_acc 98.71, Test_acc 56.25
2025-02-14 21:55:14,942 [podnet.py] => Task 7, Epoch 6/20 (LR 0.00397) => LSC_loss 0.14, Spatial_loss 2.15, Flat_loss 0.16, Train_acc 98.76, Test_acc 56.18
2025-02-14 21:55:16,462 [podnet.py] => Task 7, Epoch 7/20 (LR 0.00363) => LSC_loss 0.14, Spatial_loss 2.15, Flat_loss 0.16, Train_acc 98.94, Test_acc 56.21
2025-02-14 21:55:17,995 [podnet.py] => Task 7, Epoch 8/20 (LR 0.00327) => LSC_loss 0.14, Spatial_loss 2.14, Flat_loss 0.16, Train_acc 98.65, Test_acc 56.00
2025-02-14 21:55:19,512 [podnet.py] => Task 7, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 2.19, Flat_loss 0.16, Train_acc 98.94, Test_acc 56.29
2025-02-14 21:55:21,041 [podnet.py] => Task 7, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 2.14, Flat_loss 0.16, Train_acc 99.06, Test_acc 56.73
2025-02-14 21:55:22,589 [podnet.py] => Task 7, Epoch 11/20 (LR 0.00211) => LSC_loss 0.13, Spatial_loss 2.11, Flat_loss 0.16, Train_acc 99.00, Test_acc 56.44
2025-02-14 21:55:24,108 [podnet.py] => Task 7, Epoch 12/20 (LR 0.00173) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.53, Test_acc 56.40
2025-02-14 21:55:25,634 [podnet.py] => Task 7, Epoch 13/20 (LR 0.00137) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.88, Test_acc 56.26
2025-02-14 21:55:27,168 [podnet.py] => Task 7, Epoch 14/20 (LR 0.00103) => LSC_loss 0.15, Spatial_loss 2.14, Flat_loss 0.15, Train_acc 98.35, Test_acc 56.45
2025-02-14 21:55:28,711 [podnet.py] => Task 7, Epoch 15/20 (LR 0.00073) => LSC_loss 0.15, Spatial_loss 2.09, Flat_loss 0.16, Train_acc 98.47, Test_acc 56.54
2025-02-14 21:55:30,254 [podnet.py] => Task 7, Epoch 16/20 (LR 0.00048) => LSC_loss 0.14, Spatial_loss 2.11, Flat_loss 0.15, Train_acc 98.76, Test_acc 56.31
2025-02-14 21:55:31,768 [podnet.py] => Task 7, Epoch 17/20 (LR 0.00027) => LSC_loss 0.14, Spatial_loss 2.07, Flat_loss 0.15, Train_acc 98.71, Test_acc 56.24
2025-02-14 21:55:33,229 [podnet.py] => Task 7, Epoch 18/20 (LR 0.00012) => LSC_loss 0.13, Spatial_loss 2.10, Flat_loss 0.15, Train_acc 99.29, Test_acc 56.40
2025-02-14 21:55:34,788 [podnet.py] => Task 7, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 2.09, Flat_loss 0.15, Train_acc 99.06, Test_acc 56.40
2025-02-14 21:55:36,279 [podnet.py] => Task 7, Epoch 20/20 (LR 0.00000) => LSC_loss 0.14, Spatial_loss 2.11, Flat_loss 0.16, Train_acc 99.06, Test_acc 56.36
2025-02-14 21:55:36,282 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 21:56:07,543 [podnet.py] => Exemplar size: 1700
2025-02-14 21:56:07,544 [trainer.py] => CNN: {'total': 56.36, '00-09': 65.4, '10-19': 47.1, '20-29': 62.9, '30-39': 55.8, '40-49': 61.9, '50-59': 42.3, '60-69': 52.2, '70-79': 54.8, '80-89': 73.4, 'old': 55.3, 'new': 73.4}
2025-02-14 21:56:07,544 [trainer.py] => NME: {'total': 57.14, '00-09': 69.9, '10-19': 52.0, '20-29': 68.1, '30-39': 58.8, '40-49': 63.6, '50-59': 38.7, '60-69': 50.1, '70-79': 51.3, '80-89': 66.4, 'old': 56.56, 'new': 66.4}
2025-02-14 21:56:07,544 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36]
2025-02-14 21:56:07,544 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08]
2025-02-14 21:56:07,544 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14]
2025-02-14 21:56:07,544 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48]

2025-02-14 21:56:07,544 [trainer.py] => Average Accuracy (CNN): 65.70875
2025-02-14 21:56:07,544 [trainer.py] => Average Accuracy (NME): 65.99625
2025-02-14 21:56:07,544 [trainer.py] => All params: 520657
2025-02-14 21:56:07,545 [trainer.py] => Trainable params: 520657
2025-02-14 21:56:07,545 [podnet.py] => Learning on 85-90
2025-02-14 21:56:07,576 [podnet.py] => Adaptive factor: 4.242640687119285
2025-02-14 21:56:09,778 [podnet.py] => Task 8, Epoch 1/160 (LR 0.09999) => LSC_loss 1.82, Spatial_loss 3.17, Flat_loss 0.61, Train_acc 65.36, Test_acc 44.04
2025-02-14 21:56:11,970 [podnet.py] => Task 8, Epoch 2/160 (LR 0.09996) => LSC_loss 0.72, Spatial_loss 3.10, Flat_loss 0.40, Train_acc 80.60, Test_acc 48.79
2025-02-14 21:56:14,063 [podnet.py] => Task 8, Epoch 3/160 (LR 0.09991) => LSC_loss 0.62, Spatial_loss 3.04, Flat_loss 0.36, Train_acc 83.93, Test_acc 46.53
2025-02-14 21:56:16,171 [podnet.py] => Task 8, Epoch 4/160 (LR 0.09985) => LSC_loss 0.62, Spatial_loss 3.01, Flat_loss 0.35, Train_acc 83.74, Test_acc 46.76
2025-02-14 21:56:18,248 [podnet.py] => Task 8, Epoch 5/160 (LR 0.09976) => LSC_loss 0.57, Spatial_loss 2.89, Flat_loss 0.33, Train_acc 85.07, Test_acc 47.01
2025-02-14 21:56:20,379 [podnet.py] => Task 8, Epoch 6/160 (LR 0.09965) => LSC_loss 0.56, Spatial_loss 3.01, Flat_loss 0.33, Train_acc 85.76, Test_acc 49.01
2025-02-14 21:56:22,485 [podnet.py] => Task 8, Epoch 7/160 (LR 0.09953) => LSC_loss 0.52, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 85.95, Test_acc 47.14
2025-02-14 21:56:24,594 [podnet.py] => Task 8, Epoch 8/160 (LR 0.09938) => LSC_loss 0.52, Spatial_loss 3.14, Flat_loss 0.34, Train_acc 86.33, Test_acc 43.11
2025-02-14 21:56:26,711 [podnet.py] => Task 8, Epoch 9/160 (LR 0.09922) => LSC_loss 0.51, Spatial_loss 2.94, Flat_loss 0.32, Train_acc 86.67, Test_acc 49.08
2025-02-14 21:56:28,831 [podnet.py] => Task 8, Epoch 10/160 (LR 0.09904) => LSC_loss 0.47, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 88.38, Test_acc 47.50
2025-02-14 21:56:31,028 [podnet.py] => Task 8, Epoch 11/160 (LR 0.09884) => LSC_loss 0.48, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 88.29, Test_acc 49.18
2025-02-14 21:56:33,166 [podnet.py] => Task 8, Epoch 12/160 (LR 0.09862) => LSC_loss 0.47, Spatial_loss 2.88, Flat_loss 0.31, Train_acc 88.31, Test_acc 48.18
2025-02-14 21:56:35,357 [podnet.py] => Task 8, Epoch 13/160 (LR 0.09838) => LSC_loss 0.47, Spatial_loss 2.96, Flat_loss 0.32, Train_acc 88.12, Test_acc 47.87
2025-02-14 21:56:37,488 [podnet.py] => Task 8, Epoch 14/160 (LR 0.09812) => LSC_loss 0.46, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 88.57, Test_acc 48.57
2025-02-14 21:56:39,561 [podnet.py] => Task 8, Epoch 15/160 (LR 0.09785) => LSC_loss 0.45, Spatial_loss 2.90, Flat_loss 0.32, Train_acc 88.86, Test_acc 47.09
2025-02-14 21:56:41,737 [podnet.py] => Task 8, Epoch 16/160 (LR 0.09755) => LSC_loss 0.44, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 88.83, Test_acc 49.42
2025-02-14 21:56:43,868 [podnet.py] => Task 8, Epoch 17/160 (LR 0.09724) => LSC_loss 0.43, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 89.36, Test_acc 48.12
2025-02-14 21:56:46,016 [podnet.py] => Task 8, Epoch 18/160 (LR 0.09691) => LSC_loss 0.43, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 89.69, Test_acc 49.40
2025-02-14 21:56:48,171 [podnet.py] => Task 8, Epoch 19/160 (LR 0.09656) => LSC_loss 0.41, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 90.24, Test_acc 46.47
2025-02-14 21:56:50,302 [podnet.py] => Task 8, Epoch 20/160 (LR 0.09619) => LSC_loss 0.40, Spatial_loss 2.92, Flat_loss 0.31, Train_acc 90.00, Test_acc 46.10
2025-02-14 21:56:52,392 [podnet.py] => Task 8, Epoch 21/160 (LR 0.09581) => LSC_loss 0.43, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 89.10, Test_acc 45.21
2025-02-14 21:56:54,466 [podnet.py] => Task 8, Epoch 22/160 (LR 0.09541) => LSC_loss 0.42, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 89.31, Test_acc 49.02
2025-02-14 21:56:56,544 [podnet.py] => Task 8, Epoch 23/160 (LR 0.09499) => LSC_loss 0.41, Spatial_loss 2.95, Flat_loss 0.33, Train_acc 90.10, Test_acc 45.64
2025-02-14 21:56:58,666 [podnet.py] => Task 8, Epoch 24/160 (LR 0.09455) => LSC_loss 0.40, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 90.38, Test_acc 46.77
2025-02-14 21:57:00,764 [podnet.py] => Task 8, Epoch 25/160 (LR 0.09410) => LSC_loss 0.40, Spatial_loss 2.91, Flat_loss 0.32, Train_acc 90.45, Test_acc 48.62
2025-02-14 21:57:02,947 [podnet.py] => Task 8, Epoch 26/160 (LR 0.09362) => LSC_loss 0.39, Spatial_loss 2.85, Flat_loss 0.30, Train_acc 90.55, Test_acc 47.79
2025-02-14 21:57:05,060 [podnet.py] => Task 8, Epoch 27/160 (LR 0.09314) => LSC_loss 0.38, Spatial_loss 2.89, Flat_loss 0.32, Train_acc 90.45, Test_acc 47.33
2025-02-14 21:57:07,198 [podnet.py] => Task 8, Epoch 28/160 (LR 0.09263) => LSC_loss 0.36, Spatial_loss 2.85, Flat_loss 0.31, Train_acc 91.79, Test_acc 48.02
2025-02-14 21:57:09,362 [podnet.py] => Task 8, Epoch 29/160 (LR 0.09211) => LSC_loss 0.39, Spatial_loss 2.83, Flat_loss 0.31, Train_acc 90.93, Test_acc 46.43
2025-02-14 21:57:11,458 [podnet.py] => Task 8, Epoch 30/160 (LR 0.09157) => LSC_loss 0.37, Spatial_loss 2.89, Flat_loss 0.32, Train_acc 90.86, Test_acc 49.57
2025-02-14 21:57:13,558 [podnet.py] => Task 8, Epoch 31/160 (LR 0.09102) => LSC_loss 0.38, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 91.02, Test_acc 45.46
2025-02-14 21:57:15,710 [podnet.py] => Task 8, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 2.91, Flat_loss 0.32, Train_acc 91.45, Test_acc 48.24
2025-02-14 21:57:17,797 [podnet.py] => Task 8, Epoch 33/160 (LR 0.08987) => LSC_loss 0.36, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 91.67, Test_acc 46.87
2025-02-14 21:57:19,923 [podnet.py] => Task 8, Epoch 34/160 (LR 0.08927) => LSC_loss 0.37, Spatial_loss 2.82, Flat_loss 0.30, Train_acc 91.50, Test_acc 43.30
2025-02-14 21:57:22,042 [podnet.py] => Task 8, Epoch 35/160 (LR 0.08865) => LSC_loss 0.38, Spatial_loss 2.83, Flat_loss 0.31, Train_acc 90.98, Test_acc 45.80
2025-02-14 21:57:24,176 [podnet.py] => Task 8, Epoch 36/160 (LR 0.08802) => LSC_loss 0.36, Spatial_loss 2.84, Flat_loss 0.31, Train_acc 91.76, Test_acc 45.57
2025-02-14 21:57:26,252 [podnet.py] => Task 8, Epoch 37/160 (LR 0.08738) => LSC_loss 0.37, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 91.40, Test_acc 48.17
2025-02-14 21:57:28,399 [podnet.py] => Task 8, Epoch 38/160 (LR 0.08672) => LSC_loss 0.37, Spatial_loss 2.71, Flat_loss 0.30, Train_acc 91.05, Test_acc 49.16
2025-02-14 21:57:30,509 [podnet.py] => Task 8, Epoch 39/160 (LR 0.08604) => LSC_loss 0.35, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 91.74, Test_acc 46.08
2025-02-14 21:57:32,647 [podnet.py] => Task 8, Epoch 40/160 (LR 0.08536) => LSC_loss 0.37, Spatial_loss 2.80, Flat_loss 0.30, Train_acc 91.21, Test_acc 50.04
2025-02-14 21:57:34,839 [podnet.py] => Task 8, Epoch 41/160 (LR 0.08465) => LSC_loss 0.36, Spatial_loss 2.75, Flat_loss 0.30, Train_acc 91.05, Test_acc 47.06
2025-02-14 21:57:36,939 [podnet.py] => Task 8, Epoch 42/160 (LR 0.08394) => LSC_loss 0.36, Spatial_loss 2.78, Flat_loss 0.30, Train_acc 91.62, Test_acc 49.96
2025-02-14 21:57:38,988 [podnet.py] => Task 8, Epoch 43/160 (LR 0.08321) => LSC_loss 0.35, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 91.88, Test_acc 45.87
2025-02-14 21:57:41,074 [podnet.py] => Task 8, Epoch 44/160 (LR 0.08247) => LSC_loss 0.34, Spatial_loss 2.81, Flat_loss 0.30, Train_acc 92.55, Test_acc 50.60
2025-02-14 21:57:43,200 [podnet.py] => Task 8, Epoch 45/160 (LR 0.08172) => LSC_loss 0.34, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 92.24, Test_acc 49.17
2025-02-14 21:57:45,334 [podnet.py] => Task 8, Epoch 46/160 (LR 0.08095) => LSC_loss 0.36, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 91.60, Test_acc 47.06
2025-02-14 21:57:47,445 [podnet.py] => Task 8, Epoch 47/160 (LR 0.08018) => LSC_loss 0.36, Spatial_loss 2.88, Flat_loss 0.32, Train_acc 90.83, Test_acc 49.03
2025-02-14 21:57:49,585 [podnet.py] => Task 8, Epoch 48/160 (LR 0.07939) => LSC_loss 0.35, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 91.76, Test_acc 46.63
2025-02-14 21:57:51,711 [podnet.py] => Task 8, Epoch 49/160 (LR 0.07859) => LSC_loss 0.34, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 91.90, Test_acc 48.92
2025-02-14 21:57:53,808 [podnet.py] => Task 8, Epoch 50/160 (LR 0.07778) => LSC_loss 0.33, Spatial_loss 2.69, Flat_loss 0.29, Train_acc 92.60, Test_acc 48.77
2025-02-14 21:57:55,913 [podnet.py] => Task 8, Epoch 51/160 (LR 0.07696) => LSC_loss 0.34, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 92.19, Test_acc 48.19
2025-02-14 21:57:58,033 [podnet.py] => Task 8, Epoch 52/160 (LR 0.07612) => LSC_loss 0.35, Spatial_loss 2.68, Flat_loss 0.29, Train_acc 92.17, Test_acc 49.22
2025-02-14 21:58:00,139 [podnet.py] => Task 8, Epoch 53/160 (LR 0.07528) => LSC_loss 0.35, Spatial_loss 2.74, Flat_loss 0.29, Train_acc 91.69, Test_acc 48.12
2025-02-14 21:58:02,313 [podnet.py] => Task 8, Epoch 54/160 (LR 0.07443) => LSC_loss 0.33, Spatial_loss 2.65, Flat_loss 0.29, Train_acc 92.21, Test_acc 48.44
2025-02-14 21:58:04,437 [podnet.py] => Task 8, Epoch 55/160 (LR 0.07357) => LSC_loss 0.34, Spatial_loss 2.70, Flat_loss 0.29, Train_acc 91.64, Test_acc 47.43
2025-02-14 21:58:06,594 [podnet.py] => Task 8, Epoch 56/160 (LR 0.07270) => LSC_loss 0.33, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 92.29, Test_acc 48.10
2025-02-14 21:58:08,704 [podnet.py] => Task 8, Epoch 57/160 (LR 0.07182) => LSC_loss 0.33, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 92.48, Test_acc 49.66
2025-02-14 21:58:10,847 [podnet.py] => Task 8, Epoch 58/160 (LR 0.07093) => LSC_loss 0.34, Spatial_loss 2.67, Flat_loss 0.29, Train_acc 92.17, Test_acc 48.26
2025-02-14 21:58:12,980 [podnet.py] => Task 8, Epoch 59/160 (LR 0.07004) => LSC_loss 0.36, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 91.64, Test_acc 48.12
2025-02-14 21:58:15,056 [podnet.py] => Task 8, Epoch 60/160 (LR 0.06913) => LSC_loss 0.35, Spatial_loss 2.64, Flat_loss 0.29, Train_acc 91.83, Test_acc 46.61
2025-02-14 21:58:17,145 [podnet.py] => Task 8, Epoch 61/160 (LR 0.06822) => LSC_loss 0.33, Spatial_loss 2.55, Flat_loss 0.27, Train_acc 92.40, Test_acc 48.58
2025-02-14 21:58:19,277 [podnet.py] => Task 8, Epoch 62/160 (LR 0.06731) => LSC_loss 0.33, Spatial_loss 2.61, Flat_loss 0.28, Train_acc 92.48, Test_acc 48.37
2025-02-14 21:58:21,388 [podnet.py] => Task 8, Epoch 63/160 (LR 0.06638) => LSC_loss 0.32, Spatial_loss 2.51, Flat_loss 0.27, Train_acc 92.83, Test_acc 47.61
2025-02-14 21:58:23,461 [podnet.py] => Task 8, Epoch 64/160 (LR 0.06545) => LSC_loss 0.32, Spatial_loss 2.49, Flat_loss 0.27, Train_acc 92.74, Test_acc 51.29
2025-02-14 21:58:25,569 [podnet.py] => Task 8, Epoch 65/160 (LR 0.06451) => LSC_loss 0.32, Spatial_loss 2.52, Flat_loss 0.26, Train_acc 92.74, Test_acc 48.62
2025-02-14 21:58:27,727 [podnet.py] => Task 8, Epoch 66/160 (LR 0.06357) => LSC_loss 0.33, Spatial_loss 2.57, Flat_loss 0.27, Train_acc 92.21, Test_acc 46.66
2025-02-14 21:58:29,856 [podnet.py] => Task 8, Epoch 67/160 (LR 0.06262) => LSC_loss 0.32, Spatial_loss 2.50, Flat_loss 0.26, Train_acc 92.48, Test_acc 48.14
2025-02-14 21:58:31,993 [podnet.py] => Task 8, Epoch 68/160 (LR 0.06167) => LSC_loss 0.31, Spatial_loss 2.47, Flat_loss 0.26, Train_acc 93.21, Test_acc 48.03
2025-02-14 21:58:34,103 [podnet.py] => Task 8, Epoch 69/160 (LR 0.06072) => LSC_loss 0.31, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 93.21, Test_acc 48.61
2025-02-14 21:58:36,238 [podnet.py] => Task 8, Epoch 70/160 (LR 0.05975) => LSC_loss 0.33, Spatial_loss 2.50, Flat_loss 0.27, Train_acc 92.52, Test_acc 50.64
2025-02-14 21:58:38,407 [podnet.py] => Task 8, Epoch 71/160 (LR 0.05879) => LSC_loss 0.32, Spatial_loss 2.49, Flat_loss 0.26, Train_acc 93.02, Test_acc 47.59
2025-02-14 21:58:40,465 [podnet.py] => Task 8, Epoch 72/160 (LR 0.05782) => LSC_loss 0.32, Spatial_loss 2.40, Flat_loss 0.26, Train_acc 92.43, Test_acc 48.72
2025-02-14 21:58:42,592 [podnet.py] => Task 8, Epoch 73/160 (LR 0.05685) => LSC_loss 0.33, Spatial_loss 2.46, Flat_loss 0.26, Train_acc 91.93, Test_acc 50.58
2025-02-14 21:58:44,727 [podnet.py] => Task 8, Epoch 74/160 (LR 0.05588) => LSC_loss 0.32, Spatial_loss 2.48, Flat_loss 0.26, Train_acc 92.36, Test_acc 48.72
2025-02-14 21:58:46,871 [podnet.py] => Task 8, Epoch 75/160 (LR 0.05490) => LSC_loss 0.32, Spatial_loss 2.45, Flat_loss 0.26, Train_acc 93.40, Test_acc 48.51
2025-02-14 21:58:48,981 [podnet.py] => Task 8, Epoch 76/160 (LR 0.05392) => LSC_loss 0.31, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 93.38, Test_acc 50.18
2025-02-14 21:58:51,137 [podnet.py] => Task 8, Epoch 77/160 (LR 0.05294) => LSC_loss 0.30, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 93.00, Test_acc 48.88
2025-02-14 21:58:53,243 [podnet.py] => Task 8, Epoch 78/160 (LR 0.05196) => LSC_loss 0.32, Spatial_loss 2.43, Flat_loss 0.26, Train_acc 93.10, Test_acc 44.89
2025-02-14 21:58:55,392 [podnet.py] => Task 8, Epoch 79/160 (LR 0.05098) => LSC_loss 0.32, Spatial_loss 2.43, Flat_loss 0.25, Train_acc 92.88, Test_acc 49.38
2025-02-14 21:58:57,584 [podnet.py] => Task 8, Epoch 80/160 (LR 0.05000) => LSC_loss 0.31, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 93.14, Test_acc 48.68
2025-02-14 21:58:59,728 [podnet.py] => Task 8, Epoch 81/160 (LR 0.04902) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 93.76, Test_acc 49.51
2025-02-14 21:59:01,880 [podnet.py] => Task 8, Epoch 82/160 (LR 0.04804) => LSC_loss 0.30, Spatial_loss 2.29, Flat_loss 0.24, Train_acc 94.05, Test_acc 49.77
2025-02-14 21:59:04,021 [podnet.py] => Task 8, Epoch 83/160 (LR 0.04706) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.24, Train_acc 93.62, Test_acc 51.00
2025-02-14 21:59:06,131 [podnet.py] => Task 8, Epoch 84/160 (LR 0.04608) => LSC_loss 0.31, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 93.43, Test_acc 50.08
2025-02-14 21:59:08,187 [podnet.py] => Task 8, Epoch 85/160 (LR 0.04510) => LSC_loss 0.31, Spatial_loss 2.31, Flat_loss 0.24, Train_acc 93.57, Test_acc 50.27
2025-02-14 21:59:10,325 [podnet.py] => Task 8, Epoch 86/160 (LR 0.04412) => LSC_loss 0.31, Spatial_loss 2.24, Flat_loss 0.24, Train_acc 93.00, Test_acc 49.49
2025-02-14 21:59:12,415 [podnet.py] => Task 8, Epoch 87/160 (LR 0.04315) => LSC_loss 0.31, Spatial_loss 2.27, Flat_loss 0.24, Train_acc 93.17, Test_acc 48.32
2025-02-14 21:59:14,610 [podnet.py] => Task 8, Epoch 88/160 (LR 0.04218) => LSC_loss 0.30, Spatial_loss 2.23, Flat_loss 0.23, Train_acc 93.86, Test_acc 51.67
2025-02-14 21:59:16,747 [podnet.py] => Task 8, Epoch 89/160 (LR 0.04121) => LSC_loss 0.29, Spatial_loss 2.16, Flat_loss 0.23, Train_acc 93.52, Test_acc 51.44
2025-02-14 21:59:18,875 [podnet.py] => Task 8, Epoch 90/160 (LR 0.04025) => LSC_loss 0.30, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 93.48, Test_acc 51.60
2025-02-14 21:59:21,055 [podnet.py] => Task 8, Epoch 91/160 (LR 0.03928) => LSC_loss 0.30, Spatial_loss 2.21, Flat_loss 0.22, Train_acc 93.98, Test_acc 47.09
2025-02-14 21:59:23,134 [podnet.py] => Task 8, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.19, Flat_loss 0.22, Train_acc 93.81, Test_acc 50.74
2025-02-14 21:59:25,249 [podnet.py] => Task 8, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.12, Flat_loss 0.22, Train_acc 93.69, Test_acc 48.10
2025-02-14 21:59:27,389 [podnet.py] => Task 8, Epoch 94/160 (LR 0.03643) => LSC_loss 0.29, Spatial_loss 2.19, Flat_loss 0.22, Train_acc 94.00, Test_acc 49.51
2025-02-14 21:59:29,497 [podnet.py] => Task 8, Epoch 95/160 (LR 0.03549) => LSC_loss 0.30, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 93.57, Test_acc 50.60
2025-02-14 21:59:31,619 [podnet.py] => Task 8, Epoch 96/160 (LR 0.03455) => LSC_loss 0.29, Spatial_loss 2.10, Flat_loss 0.21, Train_acc 93.74, Test_acc 50.83
2025-02-14 21:59:33,787 [podnet.py] => Task 8, Epoch 97/160 (LR 0.03362) => LSC_loss 0.29, Spatial_loss 2.08, Flat_loss 0.21, Train_acc 93.95, Test_acc 49.63
2025-02-14 21:59:35,879 [podnet.py] => Task 8, Epoch 98/160 (LR 0.03269) => LSC_loss 0.29, Spatial_loss 2.09, Flat_loss 0.22, Train_acc 93.24, Test_acc 50.10
2025-02-14 21:59:37,971 [podnet.py] => Task 8, Epoch 99/160 (LR 0.03178) => LSC_loss 0.29, Spatial_loss 2.08, Flat_loss 0.21, Train_acc 94.48, Test_acc 50.67
2025-02-14 21:59:40,156 [podnet.py] => Task 8, Epoch 100/160 (LR 0.03087) => LSC_loss 0.30, Spatial_loss 2.05, Flat_loss 0.21, Train_acc 93.83, Test_acc 51.98
2025-02-14 21:59:42,321 [podnet.py] => Task 8, Epoch 101/160 (LR 0.02996) => LSC_loss 0.29, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 93.98, Test_acc 51.01
2025-02-14 21:59:44,408 [podnet.py] => Task 8, Epoch 102/160 (LR 0.02907) => LSC_loss 0.30, Spatial_loss 1.98, Flat_loss 0.20, Train_acc 93.40, Test_acc 50.08
2025-02-14 21:59:46,500 [podnet.py] => Task 8, Epoch 103/160 (LR 0.02818) => LSC_loss 0.28, Spatial_loss 1.96, Flat_loss 0.20, Train_acc 94.33, Test_acc 51.22
2025-02-14 21:59:48,639 [podnet.py] => Task 8, Epoch 104/160 (LR 0.02730) => LSC_loss 0.29, Spatial_loss 2.02, Flat_loss 0.20, Train_acc 93.98, Test_acc 49.80
2025-02-14 21:59:50,834 [podnet.py] => Task 8, Epoch 105/160 (LR 0.02643) => LSC_loss 0.28, Spatial_loss 1.99, Flat_loss 0.20, Train_acc 94.55, Test_acc 50.86
2025-02-14 21:59:52,964 [podnet.py] => Task 8, Epoch 106/160 (LR 0.02557) => LSC_loss 0.28, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 94.48, Test_acc 48.04
2025-02-14 21:59:55,056 [podnet.py] => Task 8, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 1.95, Flat_loss 0.20, Train_acc 94.71, Test_acc 49.62
2025-02-14 21:59:57,086 [podnet.py] => Task 8, Epoch 108/160 (LR 0.02388) => LSC_loss 0.30, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 93.62, Test_acc 52.53
2025-02-14 21:59:59,184 [podnet.py] => Task 8, Epoch 109/160 (LR 0.02304) => LSC_loss 0.29, Spatial_loss 1.86, Flat_loss 0.19, Train_acc 94.29, Test_acc 51.02
2025-02-14 22:00:01,323 [podnet.py] => Task 8, Epoch 110/160 (LR 0.02222) => LSC_loss 0.28, Spatial_loss 1.86, Flat_loss 0.19, Train_acc 94.48, Test_acc 52.48
2025-02-14 22:00:03,476 [podnet.py] => Task 8, Epoch 111/160 (LR 0.02141) => LSC_loss 0.28, Spatial_loss 1.87, Flat_loss 0.19, Train_acc 94.14, Test_acc 52.02
2025-02-14 22:00:05,581 [podnet.py] => Task 8, Epoch 112/160 (LR 0.02061) => LSC_loss 0.29, Spatial_loss 1.85, Flat_loss 0.19, Train_acc 93.98, Test_acc 51.96
2025-02-14 22:00:07,736 [podnet.py] => Task 8, Epoch 113/160 (LR 0.01982) => LSC_loss 0.29, Spatial_loss 1.89, Flat_loss 0.20, Train_acc 94.10, Test_acc 51.64
2025-02-14 22:00:09,870 [podnet.py] => Task 8, Epoch 114/160 (LR 0.01905) => LSC_loss 0.28, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 94.76, Test_acc 52.94
2025-02-14 22:00:11,998 [podnet.py] => Task 8, Epoch 115/160 (LR 0.01828) => LSC_loss 0.28, Spatial_loss 1.82, Flat_loss 0.18, Train_acc 94.45, Test_acc 51.96
2025-02-14 22:00:14,095 [podnet.py] => Task 8, Epoch 116/160 (LR 0.01753) => LSC_loss 0.28, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 94.50, Test_acc 52.09
2025-02-14 22:00:16,229 [podnet.py] => Task 8, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 94.64, Test_acc 50.70
2025-02-14 22:00:18,351 [podnet.py] => Task 8, Epoch 118/160 (LR 0.01606) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 94.24, Test_acc 50.90
2025-02-14 22:00:20,468 [podnet.py] => Task 8, Epoch 119/160 (LR 0.01535) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 94.50, Test_acc 52.28
2025-02-14 22:00:22,557 [podnet.py] => Task 8, Epoch 120/160 (LR 0.01464) => LSC_loss 0.28, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 94.43, Test_acc 52.29
2025-02-14 22:00:24,679 [podnet.py] => Task 8, Epoch 121/160 (LR 0.01396) => LSC_loss 0.28, Spatial_loss 1.74, Flat_loss 0.18, Train_acc 93.88, Test_acc 51.54
2025-02-14 22:00:26,798 [podnet.py] => Task 8, Epoch 122/160 (LR 0.01328) => LSC_loss 0.27, Spatial_loss 1.65, Flat_loss 0.17, Train_acc 94.88, Test_acc 51.88
2025-02-14 22:00:28,898 [podnet.py] => Task 8, Epoch 123/160 (LR 0.01262) => LSC_loss 0.28, Spatial_loss 1.68, Flat_loss 0.17, Train_acc 94.79, Test_acc 52.22
2025-02-14 22:00:31,008 [podnet.py] => Task 8, Epoch 124/160 (LR 0.01198) => LSC_loss 0.27, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 94.45, Test_acc 51.09
2025-02-14 22:00:33,144 [podnet.py] => Task 8, Epoch 125/160 (LR 0.01135) => LSC_loss 0.29, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 94.24, Test_acc 50.83
2025-02-14 22:00:35,305 [podnet.py] => Task 8, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.70, Flat_loss 0.17, Train_acc 95.00, Test_acc 50.50
2025-02-14 22:00:37,367 [podnet.py] => Task 8, Epoch 127/160 (LR 0.01013) => LSC_loss 0.29, Spatial_loss 1.62, Flat_loss 0.17, Train_acc 94.05, Test_acc 50.77
2025-02-14 22:00:39,590 [podnet.py] => Task 8, Epoch 128/160 (LR 0.00955) => LSC_loss 0.27, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 94.76, Test_acc 51.28
2025-02-14 22:00:41,704 [podnet.py] => Task 8, Epoch 129/160 (LR 0.00898) => LSC_loss 0.28, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 94.83, Test_acc 51.83
2025-02-14 22:00:43,851 [podnet.py] => Task 8, Epoch 130/160 (LR 0.00843) => LSC_loss 0.28, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 94.19, Test_acc 52.67
2025-02-14 22:00:45,961 [podnet.py] => Task 8, Epoch 131/160 (LR 0.00789) => LSC_loss 0.28, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 94.19, Test_acc 52.22
2025-02-14 22:00:48,022 [podnet.py] => Task 8, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.57, Flat_loss 0.17, Train_acc 94.98, Test_acc 52.48
2025-02-14 22:00:50,208 [podnet.py] => Task 8, Epoch 133/160 (LR 0.00686) => LSC_loss 0.29, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 93.64, Test_acc 51.79
2025-02-14 22:00:52,353 [podnet.py] => Task 8, Epoch 134/160 (LR 0.00638) => LSC_loss 0.27, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 95.10, Test_acc 52.19
2025-02-14 22:00:54,479 [podnet.py] => Task 8, Epoch 135/160 (LR 0.00590) => LSC_loss 0.28, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 94.67, Test_acc 51.07
2025-02-14 22:00:56,591 [podnet.py] => Task 8, Epoch 136/160 (LR 0.00545) => LSC_loss 0.28, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 94.62, Test_acc 52.61
2025-02-14 22:00:58,700 [podnet.py] => Task 8, Epoch 137/160 (LR 0.00501) => LSC_loss 0.28, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 94.48, Test_acc 52.73
2025-02-14 22:01:00,793 [podnet.py] => Task 8, Epoch 138/160 (LR 0.00459) => LSC_loss 0.28, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 94.14, Test_acc 52.30
2025-02-14 22:01:02,892 [podnet.py] => Task 8, Epoch 139/160 (LR 0.00419) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 94.83, Test_acc 52.57
2025-02-14 22:01:04,979 [podnet.py] => Task 8, Epoch 140/160 (LR 0.00381) => LSC_loss 0.28, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 94.67, Test_acc 51.70
2025-02-14 22:01:07,103 [podnet.py] => Task 8, Epoch 141/160 (LR 0.00344) => LSC_loss 0.28, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 94.50, Test_acc 52.09
2025-02-14 22:01:09,272 [podnet.py] => Task 8, Epoch 142/160 (LR 0.00309) => LSC_loss 0.28, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 94.55, Test_acc 51.90
2025-02-14 22:01:11,313 [podnet.py] => Task 8, Epoch 143/160 (LR 0.00276) => LSC_loss 0.27, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 95.17, Test_acc 53.01
2025-02-14 22:01:13,352 [podnet.py] => Task 8, Epoch 144/160 (LR 0.00245) => LSC_loss 0.28, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 94.64, Test_acc 52.67
2025-02-14 22:01:15,427 [podnet.py] => Task 8, Epoch 145/160 (LR 0.00215) => LSC_loss 0.27, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 95.40, Test_acc 52.09
2025-02-14 22:01:17,457 [podnet.py] => Task 8, Epoch 146/160 (LR 0.00188) => LSC_loss 0.28, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 94.74, Test_acc 52.56
2025-02-14 22:01:19,570 [podnet.py] => Task 8, Epoch 147/160 (LR 0.00162) => LSC_loss 0.28, Spatial_loss 1.39, Flat_loss 0.16, Train_acc 94.64, Test_acc 52.16
2025-02-14 22:01:21,720 [podnet.py] => Task 8, Epoch 148/160 (LR 0.00138) => LSC_loss 0.27, Spatial_loss 1.41, Flat_loss 0.15, Train_acc 95.05, Test_acc 52.51
2025-02-14 22:01:23,864 [podnet.py] => Task 8, Epoch 149/160 (LR 0.00116) => LSC_loss 0.27, Spatial_loss 1.39, Flat_loss 0.16, Train_acc 94.95, Test_acc 52.69
2025-02-14 22:01:25,906 [podnet.py] => Task 8, Epoch 150/160 (LR 0.00096) => LSC_loss 0.27, Spatial_loss 1.38, Flat_loss 0.15, Train_acc 94.52, Test_acc 52.22
2025-02-14 22:01:27,981 [podnet.py] => Task 8, Epoch 151/160 (LR 0.00078) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.62, Test_acc 52.80
2025-02-14 22:01:30,062 [podnet.py] => Task 8, Epoch 152/160 (LR 0.00062) => LSC_loss 0.27, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.88, Test_acc 52.89
2025-02-14 22:01:32,183 [podnet.py] => Task 8, Epoch 153/160 (LR 0.00047) => LSC_loss 0.28, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 94.31, Test_acc 52.76
2025-02-14 22:01:34,230 [podnet.py] => Task 8, Epoch 154/160 (LR 0.00035) => LSC_loss 0.27, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 94.98, Test_acc 52.32
2025-02-14 22:01:36,319 [podnet.py] => Task 8, Epoch 155/160 (LR 0.00024) => LSC_loss 0.27, Spatial_loss 1.34, Flat_loss 0.15, Train_acc 95.12, Test_acc 52.87
2025-02-14 22:01:38,465 [podnet.py] => Task 8, Epoch 156/160 (LR 0.00015) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.24, Test_acc 52.69
2025-02-14 22:01:40,557 [podnet.py] => Task 8, Epoch 157/160 (LR 0.00009) => LSC_loss 0.27, Spatial_loss 1.33, Flat_loss 0.15, Train_acc 94.79, Test_acc 52.47
2025-02-14 22:01:42,695 [podnet.py] => Task 8, Epoch 158/160 (LR 0.00004) => LSC_loss 0.28, Spatial_loss 1.33, Flat_loss 0.15, Train_acc 94.76, Test_acc 52.76
2025-02-14 22:01:44,826 [podnet.py] => Task 8, Epoch 159/160 (LR 0.00001) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 94.90, Test_acc 52.56
2025-02-14 22:01:46,998 [podnet.py] => Task 8, Epoch 160/160 (LR 0.00000) => LSC_loss 0.27, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 94.69, Test_acc 52.68
2025-02-14 22:01:46,998 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 22:01:46,999 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:02:18,048 [podnet.py] => The size of finetune dataset: 1800
2025-02-14 22:02:19,630 [podnet.py] => Task 8, Epoch 1/20 (LR 0.00497) => LSC_loss 0.23, Spatial_loss 1.93, Flat_loss 0.19, Train_acc 95.28, Test_acc 52.59
2025-02-14 22:02:21,257 [podnet.py] => Task 8, Epoch 2/20 (LR 0.00488) => LSC_loss 0.14, Spatial_loss 1.74, Flat_loss 0.11, Train_acc 98.22, Test_acc 53.37
2025-02-14 22:02:22,860 [podnet.py] => Task 8, Epoch 3/20 (LR 0.00473) => LSC_loss 0.12, Spatial_loss 1.62, Flat_loss 0.09, Train_acc 98.39, Test_acc 53.96
2025-02-14 22:02:24,448 [podnet.py] => Task 8, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.70, Flat_loss 0.10, Train_acc 98.33, Test_acc 53.49
2025-02-14 22:02:25,982 [podnet.py] => Task 8, Epoch 5/20 (LR 0.00427) => LSC_loss 0.12, Spatial_loss 1.65, Flat_loss 0.09, Train_acc 98.50, Test_acc 54.52
2025-02-14 22:02:27,606 [podnet.py] => Task 8, Epoch 6/20 (LR 0.00397) => LSC_loss 0.16, Spatial_loss 1.56, Flat_loss 0.08, Train_acc 98.33, Test_acc 53.92
2025-02-14 22:02:29,266 [podnet.py] => Task 8, Epoch 7/20 (LR 0.00363) => LSC_loss 0.13, Spatial_loss 1.63, Flat_loss 0.09, Train_acc 98.83, Test_acc 54.50
2025-02-14 22:02:30,861 [podnet.py] => Task 8, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.55, Flat_loss 0.09, Train_acc 98.67, Test_acc 54.11
2025-02-14 22:02:32,445 [podnet.py] => Task 8, Epoch 9/20 (LR 0.00289) => LSC_loss 0.16, Spatial_loss 1.63, Flat_loss 0.09, Train_acc 98.67, Test_acc 54.29
2025-02-14 22:02:34,068 [podnet.py] => Task 8, Epoch 10/20 (LR 0.00250) => LSC_loss 0.12, Spatial_loss 1.66, Flat_loss 0.09, Train_acc 98.44, Test_acc 54.61
2025-02-14 22:02:35,627 [podnet.py] => Task 8, Epoch 11/20 (LR 0.00211) => LSC_loss 0.16, Spatial_loss 1.68, Flat_loss 0.11, Train_acc 98.94, Test_acc 54.43
2025-02-14 22:02:37,214 [podnet.py] => Task 8, Epoch 12/20 (LR 0.00173) => LSC_loss 0.16, Spatial_loss 1.61, Flat_loss 0.09, Train_acc 98.17, Test_acc 54.24
2025-02-14 22:02:38,813 [podnet.py] => Task 8, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.65, Flat_loss 0.09, Train_acc 98.39, Test_acc 54.16
2025-02-14 22:02:40,475 [podnet.py] => Task 8, Epoch 14/20 (LR 0.00103) => LSC_loss 0.21, Spatial_loss 1.62, Flat_loss 0.10, Train_acc 98.28, Test_acc 54.54
2025-02-14 22:02:42,101 [podnet.py] => Task 8, Epoch 15/20 (LR 0.00073) => LSC_loss 0.23, Spatial_loss 1.65, Flat_loss 0.10, Train_acc 98.67, Test_acc 54.70
2025-02-14 22:02:43,681 [podnet.py] => Task 8, Epoch 16/20 (LR 0.00048) => LSC_loss 0.17, Spatial_loss 1.62, Flat_loss 0.10, Train_acc 98.06, Test_acc 54.76
2025-02-14 22:02:45,286 [podnet.py] => Task 8, Epoch 17/20 (LR 0.00027) => LSC_loss 0.14, Spatial_loss 1.56, Flat_loss 0.09, Train_acc 98.94, Test_acc 55.16
2025-02-14 22:02:46,922 [podnet.py] => Task 8, Epoch 18/20 (LR 0.00012) => LSC_loss 0.17, Spatial_loss 1.49, Flat_loss 0.08, Train_acc 98.44, Test_acc 54.54
2025-02-14 22:02:48,523 [podnet.py] => Task 8, Epoch 19/20 (LR 0.00003) => LSC_loss 0.11, Spatial_loss 1.53, Flat_loss 0.09, Train_acc 99.00, Test_acc 54.56
2025-02-14 22:02:50,121 [podnet.py] => Task 8, Epoch 20/20 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.55, Flat_loss 0.09, Train_acc 98.11, Test_acc 54.70
2025-02-14 22:02:50,122 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:03:23,388 [podnet.py] => Exemplar size: 1800
2025-02-14 22:03:23,389 [trainer.py] => CNN: {'total': 54.7, '00-09': 63.4, '10-19': 46.3, '20-29': 63.7, '30-39': 52.7, '40-49': 63.0, '50-59': 41.5, '60-69': 49.4, '70-79': 50.0, '80-89': 62.3, 'old': 54.74, 'new': 54.0}
2025-02-14 22:03:23,389 [trainer.py] => NME: {'total': 55.1, '00-09': 67.3, '10-19': 50.8, '20-29': 66.6, '30-39': 57.0, '40-49': 63.7, '50-59': 37.4, '60-69': 47.3, '70-79': 48.2, '80-89': 57.6, 'old': 55.38, 'new': 50.4}
2025-02-14 22:03:23,389 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7]
2025-02-14 22:03:23,389 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5]
2025-02-14 22:03:23,389 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1]
2025-02-14 22:03:23,389 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73]

2025-02-14 22:03:23,389 [trainer.py] => Average Accuracy (CNN): 64.48555555555555
2025-02-14 22:03:23,389 [trainer.py] => Average Accuracy (NME): 64.78555555555556
2025-02-14 22:03:23,389 [trainer.py] => All params: 523857
2025-02-14 22:03:23,390 [trainer.py] => Trainable params: 523857
2025-02-14 22:03:23,390 [podnet.py] => Learning on 90-95
2025-02-14 22:03:23,422 [podnet.py] => Adaptive factor: 4.358898943540674
2025-02-14 22:03:25,682 [podnet.py] => Task 9, Epoch 1/160 (LR 0.09999) => LSC_loss 1.80, Spatial_loss 4.52, Flat_loss 0.94, Train_acc 64.49, Test_acc 32.48
2025-02-14 22:03:27,871 [podnet.py] => Task 9, Epoch 2/160 (LR 0.09996) => LSC_loss 0.84, Spatial_loss 4.69, Flat_loss 0.77, Train_acc 76.51, Test_acc 39.77
2025-02-14 22:03:30,053 [podnet.py] => Task 9, Epoch 3/160 (LR 0.09991) => LSC_loss 0.68, Spatial_loss 4.22, Flat_loss 0.63, Train_acc 81.37, Test_acc 41.41
2025-02-14 22:03:32,192 [podnet.py] => Task 9, Epoch 4/160 (LR 0.09985) => LSC_loss 0.64, Spatial_loss 4.08, Flat_loss 0.57, Train_acc 82.63, Test_acc 47.07
2025-02-14 22:03:34,371 [podnet.py] => Task 9, Epoch 5/160 (LR 0.09976) => LSC_loss 0.55, Spatial_loss 3.79, Flat_loss 0.49, Train_acc 85.70, Test_acc 42.02
2025-02-14 22:03:36,543 [podnet.py] => Task 9, Epoch 6/160 (LR 0.09965) => LSC_loss 0.52, Spatial_loss 3.66, Flat_loss 0.46, Train_acc 86.63, Test_acc 41.42
2025-02-14 22:03:38,684 [podnet.py] => Task 9, Epoch 7/160 (LR 0.09953) => LSC_loss 0.51, Spatial_loss 3.72, Flat_loss 0.46, Train_acc 85.84, Test_acc 37.24
2025-02-14 22:03:40,903 [podnet.py] => Task 9, Epoch 8/160 (LR 0.09938) => LSC_loss 0.50, Spatial_loss 3.73, Flat_loss 0.46, Train_acc 86.88, Test_acc 43.61
2025-02-14 22:03:43,078 [podnet.py] => Task 9, Epoch 9/160 (LR 0.09922) => LSC_loss 0.48, Spatial_loss 3.59, Flat_loss 0.44, Train_acc 87.70, Test_acc 41.72
2025-02-14 22:03:45,206 [podnet.py] => Task 9, Epoch 10/160 (LR 0.09904) => LSC_loss 0.49, Spatial_loss 3.54, Flat_loss 0.43, Train_acc 87.70, Test_acc 38.02
2025-02-14 22:03:47,316 [podnet.py] => Task 9, Epoch 11/160 (LR 0.09884) => LSC_loss 0.44, Spatial_loss 3.51, Flat_loss 0.42, Train_acc 88.70, Test_acc 43.38
2025-02-14 22:03:49,459 [podnet.py] => Task 9, Epoch 12/160 (LR 0.09862) => LSC_loss 0.43, Spatial_loss 3.42, Flat_loss 0.39, Train_acc 89.53, Test_acc 38.67
2025-02-14 22:03:51,641 [podnet.py] => Task 9, Epoch 13/160 (LR 0.09838) => LSC_loss 0.41, Spatial_loss 3.38, Flat_loss 0.39, Train_acc 90.23, Test_acc 44.20
2025-02-14 22:03:53,844 [podnet.py] => Task 9, Epoch 14/160 (LR 0.09812) => LSC_loss 0.41, Spatial_loss 3.41, Flat_loss 0.38, Train_acc 89.74, Test_acc 45.32
2025-02-14 22:03:56,007 [podnet.py] => Task 9, Epoch 15/160 (LR 0.09785) => LSC_loss 0.44, Spatial_loss 3.38, Flat_loss 0.40, Train_acc 89.21, Test_acc 46.37
2025-02-14 22:03:58,211 [podnet.py] => Task 9, Epoch 16/160 (LR 0.09755) => LSC_loss 0.40, Spatial_loss 3.32, Flat_loss 0.39, Train_acc 90.09, Test_acc 46.40
2025-02-14 22:04:00,369 [podnet.py] => Task 9, Epoch 17/160 (LR 0.09724) => LSC_loss 0.42, Spatial_loss 3.34, Flat_loss 0.38, Train_acc 89.05, Test_acc 46.04
2025-02-14 22:04:02,606 [podnet.py] => Task 9, Epoch 18/160 (LR 0.09691) => LSC_loss 0.40, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.51, Test_acc 41.19
2025-02-14 22:04:04,762 [podnet.py] => Task 9, Epoch 19/160 (LR 0.09656) => LSC_loss 0.38, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.07, Test_acc 42.76
2025-02-14 22:04:06,869 [podnet.py] => Task 9, Epoch 20/160 (LR 0.09619) => LSC_loss 0.37, Spatial_loss 3.10, Flat_loss 0.35, Train_acc 91.33, Test_acc 44.77
2025-02-14 22:04:09,003 [podnet.py] => Task 9, Epoch 21/160 (LR 0.09581) => LSC_loss 0.38, Spatial_loss 3.27, Flat_loss 0.38, Train_acc 90.81, Test_acc 41.54
2025-02-14 22:04:11,194 [podnet.py] => Task 9, Epoch 22/160 (LR 0.09541) => LSC_loss 0.37, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 91.35, Test_acc 48.21
2025-02-14 22:04:13,335 [podnet.py] => Task 9, Epoch 23/160 (LR 0.09499) => LSC_loss 0.39, Spatial_loss 3.18, Flat_loss 0.37, Train_acc 90.60, Test_acc 42.69
2025-02-14 22:04:15,514 [podnet.py] => Task 9, Epoch 24/160 (LR 0.09455) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.38, Train_acc 90.47, Test_acc 42.45
2025-02-14 22:04:17,725 [podnet.py] => Task 9, Epoch 25/160 (LR 0.09410) => LSC_loss 0.38, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.26, Test_acc 43.03
2025-02-14 22:04:19,913 [podnet.py] => Task 9, Epoch 26/160 (LR 0.09362) => LSC_loss 0.37, Spatial_loss 3.14, Flat_loss 0.36, Train_acc 91.47, Test_acc 42.88
2025-02-14 22:04:22,059 [podnet.py] => Task 9, Epoch 27/160 (LR 0.09314) => LSC_loss 0.37, Spatial_loss 3.19, Flat_loss 0.36, Train_acc 91.05, Test_acc 46.63
2025-02-14 22:04:24,241 [podnet.py] => Task 9, Epoch 28/160 (LR 0.09263) => LSC_loss 0.38, Spatial_loss 3.19, Flat_loss 0.36, Train_acc 90.77, Test_acc 47.05
2025-02-14 22:04:26,470 [podnet.py] => Task 9, Epoch 29/160 (LR 0.09211) => LSC_loss 0.38, Spatial_loss 3.19, Flat_loss 0.37, Train_acc 90.58, Test_acc 45.05
2025-02-14 22:04:28,638 [podnet.py] => Task 9, Epoch 30/160 (LR 0.09157) => LSC_loss 0.36, Spatial_loss 3.09, Flat_loss 0.35, Train_acc 91.67, Test_acc 44.35
2025-02-14 22:04:30,798 [podnet.py] => Task 9, Epoch 31/160 (LR 0.09102) => LSC_loss 0.37, Spatial_loss 3.20, Flat_loss 0.36, Train_acc 91.58, Test_acc 44.32
2025-02-14 22:04:32,970 [podnet.py] => Task 9, Epoch 32/160 (LR 0.09045) => LSC_loss 0.37, Spatial_loss 3.17, Flat_loss 0.35, Train_acc 91.16, Test_acc 44.05
2025-02-14 22:04:35,194 [podnet.py] => Task 9, Epoch 33/160 (LR 0.08987) => LSC_loss 0.36, Spatial_loss 3.12, Flat_loss 0.34, Train_acc 91.88, Test_acc 44.64
2025-02-14 22:04:37,437 [podnet.py] => Task 9, Epoch 34/160 (LR 0.08927) => LSC_loss 0.38, Spatial_loss 3.25, Flat_loss 0.37, Train_acc 90.51, Test_acc 47.61
2025-02-14 22:04:39,537 [podnet.py] => Task 9, Epoch 35/160 (LR 0.08865) => LSC_loss 0.36, Spatial_loss 3.09, Flat_loss 0.35, Train_acc 91.40, Test_acc 46.53
2025-02-14 22:04:41,733 [podnet.py] => Task 9, Epoch 36/160 (LR 0.08802) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.77, Test_acc 41.42
2025-02-14 22:04:43,902 [podnet.py] => Task 9, Epoch 37/160 (LR 0.08738) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.35, Train_acc 91.07, Test_acc 45.86
2025-02-14 22:04:46,095 [podnet.py] => Task 9, Epoch 38/160 (LR 0.08672) => LSC_loss 0.35, Spatial_loss 3.09, Flat_loss 0.34, Train_acc 91.91, Test_acc 45.98
2025-02-14 22:04:48,225 [podnet.py] => Task 9, Epoch 39/160 (LR 0.08604) => LSC_loss 0.36, Spatial_loss 3.14, Flat_loss 0.35, Train_acc 91.60, Test_acc 44.42
2025-02-14 22:04:50,436 [podnet.py] => Task 9, Epoch 40/160 (LR 0.08536) => LSC_loss 0.33, Spatial_loss 3.05, Flat_loss 0.33, Train_acc 92.70, Test_acc 44.45
2025-02-14 22:04:52,616 [podnet.py] => Task 9, Epoch 41/160 (LR 0.08465) => LSC_loss 0.34, Spatial_loss 3.10, Flat_loss 0.33, Train_acc 92.37, Test_acc 42.42
2025-02-14 22:04:54,778 [podnet.py] => Task 9, Epoch 42/160 (LR 0.08394) => LSC_loss 0.33, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 92.44, Test_acc 43.42
2025-02-14 22:04:57,004 [podnet.py] => Task 9, Epoch 43/160 (LR 0.08321) => LSC_loss 0.33, Spatial_loss 3.01, Flat_loss 0.34, Train_acc 92.09, Test_acc 44.57
2025-02-14 22:04:59,120 [podnet.py] => Task 9, Epoch 44/160 (LR 0.08247) => LSC_loss 0.36, Spatial_loss 3.04, Flat_loss 0.34, Train_acc 91.42, Test_acc 40.09
2025-02-14 22:05:01,302 [podnet.py] => Task 9, Epoch 45/160 (LR 0.08172) => LSC_loss 0.33, Spatial_loss 3.00, Flat_loss 0.33, Train_acc 92.16, Test_acc 46.06
2025-02-14 22:05:03,412 [podnet.py] => Task 9, Epoch 46/160 (LR 0.08095) => LSC_loss 0.32, Spatial_loss 2.95, Flat_loss 0.32, Train_acc 93.07, Test_acc 46.45
2025-02-14 22:05:05,549 [podnet.py] => Task 9, Epoch 47/160 (LR 0.08018) => LSC_loss 0.32, Spatial_loss 2.92, Flat_loss 0.32, Train_acc 92.47, Test_acc 45.80
2025-02-14 22:05:07,766 [podnet.py] => Task 9, Epoch 48/160 (LR 0.07939) => LSC_loss 0.35, Spatial_loss 3.03, Flat_loss 0.34, Train_acc 91.81, Test_acc 41.78
2025-02-14 22:05:09,906 [podnet.py] => Task 9, Epoch 49/160 (LR 0.07859) => LSC_loss 0.34, Spatial_loss 3.06, Flat_loss 0.34, Train_acc 92.42, Test_acc 45.22
2025-02-14 22:05:12,096 [podnet.py] => Task 9, Epoch 50/160 (LR 0.07778) => LSC_loss 0.33, Spatial_loss 2.98, Flat_loss 0.33, Train_acc 92.56, Test_acc 47.04
2025-02-14 22:05:14,233 [podnet.py] => Task 9, Epoch 51/160 (LR 0.07696) => LSC_loss 0.33, Spatial_loss 2.95, Flat_loss 0.33, Train_acc 92.51, Test_acc 46.07
2025-02-14 22:05:16,412 [podnet.py] => Task 9, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.00, Flat_loss 0.33, Train_acc 92.16, Test_acc 47.26
2025-02-14 22:05:18,602 [podnet.py] => Task 9, Epoch 53/160 (LR 0.07528) => LSC_loss 0.31, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 92.77, Test_acc 47.01
2025-02-14 22:05:20,822 [podnet.py] => Task 9, Epoch 54/160 (LR 0.07443) => LSC_loss 0.32, Spatial_loss 2.87, Flat_loss 0.32, Train_acc 92.88, Test_acc 45.25
2025-02-14 22:05:22,971 [podnet.py] => Task 9, Epoch 55/160 (LR 0.07357) => LSC_loss 0.32, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 92.65, Test_acc 47.73
2025-02-14 22:05:25,124 [podnet.py] => Task 9, Epoch 56/160 (LR 0.07270) => LSC_loss 0.30, Spatial_loss 2.76, Flat_loss 0.30, Train_acc 93.63, Test_acc 46.46
2025-02-14 22:05:27,352 [podnet.py] => Task 9, Epoch 57/160 (LR 0.07182) => LSC_loss 0.32, Spatial_loss 2.87, Flat_loss 0.31, Train_acc 92.35, Test_acc 49.14
2025-02-14 22:05:29,517 [podnet.py] => Task 9, Epoch 58/160 (LR 0.07093) => LSC_loss 0.33, Spatial_loss 2.81, Flat_loss 0.31, Train_acc 92.37, Test_acc 48.19
2025-02-14 22:05:31,712 [podnet.py] => Task 9, Epoch 59/160 (LR 0.07004) => LSC_loss 0.31, Spatial_loss 2.82, Flat_loss 0.30, Train_acc 93.05, Test_acc 45.09
2025-02-14 22:05:33,897 [podnet.py] => Task 9, Epoch 60/160 (LR 0.06913) => LSC_loss 0.32, Spatial_loss 2.81, Flat_loss 0.30, Train_acc 92.79, Test_acc 49.66
2025-02-14 22:05:36,157 [podnet.py] => Task 9, Epoch 61/160 (LR 0.06822) => LSC_loss 0.33, Spatial_loss 2.80, Flat_loss 0.31, Train_acc 92.56, Test_acc 48.17
2025-02-14 22:05:38,320 [podnet.py] => Task 9, Epoch 62/160 (LR 0.06731) => LSC_loss 0.31, Spatial_loss 2.78, Flat_loss 0.30, Train_acc 93.09, Test_acc 48.58
2025-02-14 22:05:40,496 [podnet.py] => Task 9, Epoch 63/160 (LR 0.06638) => LSC_loss 0.31, Spatial_loss 2.79, Flat_loss 0.30, Train_acc 93.65, Test_acc 46.38
2025-02-14 22:05:42,608 [podnet.py] => Task 9, Epoch 64/160 (LR 0.06545) => LSC_loss 0.32, Spatial_loss 2.84, Flat_loss 0.31, Train_acc 92.35, Test_acc 46.98
2025-02-14 22:05:44,816 [podnet.py] => Task 9, Epoch 65/160 (LR 0.06451) => LSC_loss 0.32, Spatial_loss 2.73, Flat_loss 0.29, Train_acc 93.16, Test_acc 44.45
2025-02-14 22:05:46,988 [podnet.py] => Task 9, Epoch 66/160 (LR 0.06357) => LSC_loss 0.30, Spatial_loss 2.71, Flat_loss 0.29, Train_acc 93.21, Test_acc 46.84
2025-02-14 22:05:49,094 [podnet.py] => Task 9, Epoch 67/160 (LR 0.06262) => LSC_loss 0.31, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 92.79, Test_acc 46.07
2025-02-14 22:05:51,270 [podnet.py] => Task 9, Epoch 68/160 (LR 0.06167) => LSC_loss 0.32, Spatial_loss 2.64, Flat_loss 0.28, Train_acc 92.91, Test_acc 48.80
2025-02-14 22:05:53,465 [podnet.py] => Task 9, Epoch 69/160 (LR 0.06072) => LSC_loss 0.30, Spatial_loss 2.72, Flat_loss 0.29, Train_acc 93.63, Test_acc 46.13
2025-02-14 22:05:55,672 [podnet.py] => Task 9, Epoch 70/160 (LR 0.05975) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.30, Train_acc 93.00, Test_acc 47.11
2025-02-14 22:05:57,804 [podnet.py] => Task 9, Epoch 71/160 (LR 0.05879) => LSC_loss 0.29, Spatial_loss 2.67, Flat_loss 0.28, Train_acc 93.56, Test_acc 48.26
2025-02-14 22:05:59,981 [podnet.py] => Task 9, Epoch 72/160 (LR 0.05782) => LSC_loss 0.29, Spatial_loss 2.67, Flat_loss 0.28, Train_acc 93.37, Test_acc 48.38
2025-02-14 22:06:02,120 [podnet.py] => Task 9, Epoch 73/160 (LR 0.05685) => LSC_loss 0.30, Spatial_loss 2.62, Flat_loss 0.28, Train_acc 93.70, Test_acc 50.04
2025-02-14 22:06:04,307 [podnet.py] => Task 9, Epoch 74/160 (LR 0.05588) => LSC_loss 0.29, Spatial_loss 2.59, Flat_loss 0.27, Train_acc 93.56, Test_acc 48.39
2025-02-14 22:06:06,448 [podnet.py] => Task 9, Epoch 75/160 (LR 0.05490) => LSC_loss 0.30, Spatial_loss 2.66, Flat_loss 0.28, Train_acc 93.74, Test_acc 46.37
2025-02-14 22:06:08,578 [podnet.py] => Task 9, Epoch 76/160 (LR 0.05392) => LSC_loss 0.29, Spatial_loss 2.63, Flat_loss 0.28, Train_acc 93.74, Test_acc 49.20
2025-02-14 22:06:10,726 [podnet.py] => Task 9, Epoch 77/160 (LR 0.05294) => LSC_loss 0.30, Spatial_loss 2.62, Flat_loss 0.27, Train_acc 93.07, Test_acc 46.84
2025-02-14 22:06:12,882 [podnet.py] => Task 9, Epoch 78/160 (LR 0.05196) => LSC_loss 0.29, Spatial_loss 2.65, Flat_loss 0.28, Train_acc 93.74, Test_acc 47.74
2025-02-14 22:06:15,062 [podnet.py] => Task 9, Epoch 79/160 (LR 0.05098) => LSC_loss 0.30, Spatial_loss 2.54, Flat_loss 0.27, Train_acc 93.91, Test_acc 47.74
2025-02-14 22:06:17,232 [podnet.py] => Task 9, Epoch 80/160 (LR 0.05000) => LSC_loss 0.30, Spatial_loss 2.50, Flat_loss 0.27, Train_acc 93.51, Test_acc 48.44
2025-02-14 22:06:19,415 [podnet.py] => Task 9, Epoch 81/160 (LR 0.04902) => LSC_loss 0.28, Spatial_loss 2.52, Flat_loss 0.27, Train_acc 93.98, Test_acc 48.49
2025-02-14 22:06:21,587 [podnet.py] => Task 9, Epoch 82/160 (LR 0.04804) => LSC_loss 0.29, Spatial_loss 2.50, Flat_loss 0.26, Train_acc 93.70, Test_acc 48.35
2025-02-14 22:06:23,800 [podnet.py] => Task 9, Epoch 83/160 (LR 0.04706) => LSC_loss 0.28, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.28, Test_acc 49.09
2025-02-14 22:06:25,947 [podnet.py] => Task 9, Epoch 84/160 (LR 0.04608) => LSC_loss 0.29, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 94.02, Test_acc 47.54
2025-02-14 22:06:28,130 [podnet.py] => Task 9, Epoch 85/160 (LR 0.04510) => LSC_loss 0.29, Spatial_loss 2.44, Flat_loss 0.26, Train_acc 93.91, Test_acc 47.00
2025-02-14 22:06:30,299 [podnet.py] => Task 9, Epoch 86/160 (LR 0.04412) => LSC_loss 0.28, Spatial_loss 2.44, Flat_loss 0.25, Train_acc 94.23, Test_acc 49.13
2025-02-14 22:06:32,530 [podnet.py] => Task 9, Epoch 87/160 (LR 0.04315) => LSC_loss 0.29, Spatial_loss 2.47, Flat_loss 0.25, Train_acc 94.28, Test_acc 50.33
2025-02-14 22:06:34,666 [podnet.py] => Task 9, Epoch 88/160 (LR 0.04218) => LSC_loss 0.29, Spatial_loss 2.45, Flat_loss 0.25, Train_acc 94.51, Test_acc 44.69
2025-02-14 22:06:36,768 [podnet.py] => Task 9, Epoch 89/160 (LR 0.04121) => LSC_loss 0.29, Spatial_loss 2.40, Flat_loss 0.25, Train_acc 94.00, Test_acc 46.02
2025-02-14 22:06:38,958 [podnet.py] => Task 9, Epoch 90/160 (LR 0.04025) => LSC_loss 0.29, Spatial_loss 2.38, Flat_loss 0.25, Train_acc 93.63, Test_acc 47.88
2025-02-14 22:06:41,109 [podnet.py] => Task 9, Epoch 91/160 (LR 0.03928) => LSC_loss 0.27, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 94.70, Test_acc 47.86
2025-02-14 22:06:43,306 [podnet.py] => Task 9, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.25, Train_acc 93.72, Test_acc 48.15
2025-02-14 22:06:45,461 [podnet.py] => Task 9, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.37, Flat_loss 0.24, Train_acc 94.14, Test_acc 50.44
2025-02-14 22:06:47,638 [podnet.py] => Task 9, Epoch 94/160 (LR 0.03643) => LSC_loss 0.28, Spatial_loss 2.41, Flat_loss 0.25, Train_acc 94.28, Test_acc 49.05
2025-02-14 22:06:49,799 [podnet.py] => Task 9, Epoch 95/160 (LR 0.03549) => LSC_loss 0.28, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 94.35, Test_acc 48.66
2025-02-14 22:06:51,936 [podnet.py] => Task 9, Epoch 96/160 (LR 0.03455) => LSC_loss 0.28, Spatial_loss 2.19, Flat_loss 0.23, Train_acc 94.37, Test_acc 49.40
2025-02-14 22:06:54,175 [podnet.py] => Task 9, Epoch 97/160 (LR 0.03362) => LSC_loss 0.27, Spatial_loss 2.24, Flat_loss 0.23, Train_acc 94.40, Test_acc 47.07
2025-02-14 22:06:56,421 [podnet.py] => Task 9, Epoch 98/160 (LR 0.03269) => LSC_loss 0.27, Spatial_loss 2.23, Flat_loss 0.22, Train_acc 94.49, Test_acc 48.38
2025-02-14 22:06:58,574 [podnet.py] => Task 9, Epoch 99/160 (LR 0.03178) => LSC_loss 0.26, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 95.00, Test_acc 49.46
2025-02-14 22:07:00,747 [podnet.py] => Task 9, Epoch 100/160 (LR 0.03087) => LSC_loss 0.27, Spatial_loss 2.22, Flat_loss 0.23, Train_acc 95.14, Test_acc 50.07
2025-02-14 22:07:02,992 [podnet.py] => Task 9, Epoch 101/160 (LR 0.02996) => LSC_loss 0.27, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 94.74, Test_acc 48.64
2025-02-14 22:07:05,180 [podnet.py] => Task 9, Epoch 102/160 (LR 0.02907) => LSC_loss 0.27, Spatial_loss 2.26, Flat_loss 0.23, Train_acc 94.79, Test_acc 50.13
2025-02-14 22:07:07,322 [podnet.py] => Task 9, Epoch 103/160 (LR 0.02818) => LSC_loss 0.26, Spatial_loss 2.20, Flat_loss 0.23, Train_acc 94.77, Test_acc 49.16
2025-02-14 22:07:09,550 [podnet.py] => Task 9, Epoch 104/160 (LR 0.02730) => LSC_loss 0.27, Spatial_loss 2.17, Flat_loss 0.22, Train_acc 94.56, Test_acc 48.56
2025-02-14 22:07:11,737 [podnet.py] => Task 9, Epoch 105/160 (LR 0.02643) => LSC_loss 0.26, Spatial_loss 2.12, Flat_loss 0.22, Train_acc 94.60, Test_acc 49.57
2025-02-14 22:07:13,936 [podnet.py] => Task 9, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 2.07, Flat_loss 0.21, Train_acc 94.21, Test_acc 50.65
2025-02-14 22:07:16,096 [podnet.py] => Task 9, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 2.09, Flat_loss 0.21, Train_acc 94.56, Test_acc 49.59
2025-02-14 22:07:18,202 [podnet.py] => Task 9, Epoch 108/160 (LR 0.02388) => LSC_loss 0.28, Spatial_loss 2.10, Flat_loss 0.22, Train_acc 94.16, Test_acc 50.08
2025-02-14 22:07:20,372 [podnet.py] => Task 9, Epoch 109/160 (LR 0.02304) => LSC_loss 0.27, Spatial_loss 2.07, Flat_loss 0.21, Train_acc 94.40, Test_acc 50.66
2025-02-14 22:07:22,567 [podnet.py] => Task 9, Epoch 110/160 (LR 0.02222) => LSC_loss 0.27, Spatial_loss 1.99, Flat_loss 0.21, Train_acc 94.58, Test_acc 49.76
2025-02-14 22:07:24,778 [podnet.py] => Task 9, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 1.98, Flat_loss 0.21, Train_acc 94.65, Test_acc 50.74
2025-02-14 22:07:26,992 [podnet.py] => Task 9, Epoch 112/160 (LR 0.02061) => LSC_loss 0.26, Spatial_loss 1.99, Flat_loss 0.20, Train_acc 94.67, Test_acc 51.11
2025-02-14 22:07:29,179 [podnet.py] => Task 9, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 1.90, Flat_loss 0.20, Train_acc 95.42, Test_acc 49.39
2025-02-14 22:07:31,367 [podnet.py] => Task 9, Epoch 114/160 (LR 0.01905) => LSC_loss 0.28, Spatial_loss 2.05, Flat_loss 0.21, Train_acc 94.02, Test_acc 49.80
2025-02-14 22:07:33,534 [podnet.py] => Task 9, Epoch 115/160 (LR 0.01828) => LSC_loss 0.27, Spatial_loss 1.97, Flat_loss 0.21, Train_acc 94.42, Test_acc 49.83
2025-02-14 22:07:35,665 [podnet.py] => Task 9, Epoch 116/160 (LR 0.01753) => LSC_loss 0.26, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 95.21, Test_acc 49.72
2025-02-14 22:07:37,846 [podnet.py] => Task 9, Epoch 117/160 (LR 0.01679) => LSC_loss 0.26, Spatial_loss 1.95, Flat_loss 0.20, Train_acc 94.70, Test_acc 49.36
2025-02-14 22:07:40,061 [podnet.py] => Task 9, Epoch 118/160 (LR 0.01606) => LSC_loss 0.27, Spatial_loss 1.87, Flat_loss 0.20, Train_acc 94.74, Test_acc 50.75
2025-02-14 22:07:42,289 [podnet.py] => Task 9, Epoch 119/160 (LR 0.01535) => LSC_loss 0.27, Spatial_loss 1.91, Flat_loss 0.20, Train_acc 94.35, Test_acc 50.07
2025-02-14 22:07:44,421 [podnet.py] => Task 9, Epoch 120/160 (LR 0.01464) => LSC_loss 0.26, Spatial_loss 1.83, Flat_loss 0.20, Train_acc 94.49, Test_acc 51.34
2025-02-14 22:07:46,643 [podnet.py] => Task 9, Epoch 121/160 (LR 0.01396) => LSC_loss 0.27, Spatial_loss 1.89, Flat_loss 0.19, Train_acc 94.81, Test_acc 50.56
2025-02-14 22:07:48,858 [podnet.py] => Task 9, Epoch 122/160 (LR 0.01328) => LSC_loss 0.26, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 95.21, Test_acc 51.21
2025-02-14 22:07:50,995 [podnet.py] => Task 9, Epoch 123/160 (LR 0.01262) => LSC_loss 0.25, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 95.00, Test_acc 51.27
2025-02-14 22:07:53,155 [podnet.py] => Task 9, Epoch 124/160 (LR 0.01198) => LSC_loss 0.26, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 95.09, Test_acc 50.41
2025-02-14 22:07:55,308 [podnet.py] => Task 9, Epoch 125/160 (LR 0.01135) => LSC_loss 0.26, Spatial_loss 1.84, Flat_loss 0.19, Train_acc 95.35, Test_acc 51.87
2025-02-14 22:07:57,511 [podnet.py] => Task 9, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 94.91, Test_acc 50.85
2025-02-14 22:07:59,685 [podnet.py] => Task 9, Epoch 127/160 (LR 0.01013) => LSC_loss 0.27, Spatial_loss 1.79, Flat_loss 0.19, Train_acc 94.88, Test_acc 50.92
2025-02-14 22:08:01,848 [podnet.py] => Task 9, Epoch 128/160 (LR 0.00955) => LSC_loss 0.26, Spatial_loss 1.79, Flat_loss 0.18, Train_acc 95.07, Test_acc 51.48
2025-02-14 22:08:04,003 [podnet.py] => Task 9, Epoch 129/160 (LR 0.00898) => LSC_loss 0.27, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 94.81, Test_acc 50.86
2025-02-14 22:08:06,157 [podnet.py] => Task 9, Epoch 130/160 (LR 0.00843) => LSC_loss 0.26, Spatial_loss 1.73, Flat_loss 0.18, Train_acc 94.98, Test_acc 51.11
2025-02-14 22:08:08,352 [podnet.py] => Task 9, Epoch 131/160 (LR 0.00789) => LSC_loss 0.26, Spatial_loss 1.71, Flat_loss 0.18, Train_acc 94.63, Test_acc 51.98
2025-02-14 22:08:10,561 [podnet.py] => Task 9, Epoch 132/160 (LR 0.00737) => LSC_loss 0.26, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 95.14, Test_acc 50.80
2025-02-14 22:08:12,718 [podnet.py] => Task 9, Epoch 133/160 (LR 0.00686) => LSC_loss 0.26, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 94.95, Test_acc 50.63
2025-02-14 22:08:14,873 [podnet.py] => Task 9, Epoch 134/160 (LR 0.00638) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 94.81, Test_acc 50.56
2025-02-14 22:08:17,039 [podnet.py] => Task 9, Epoch 135/160 (LR 0.00590) => LSC_loss 0.26, Spatial_loss 1.63, Flat_loss 0.18, Train_acc 95.63, Test_acc 51.15
2025-02-14 22:08:19,243 [podnet.py] => Task 9, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.17, Train_acc 95.37, Test_acc 51.62
2025-02-14 22:08:21,440 [podnet.py] => Task 9, Epoch 137/160 (LR 0.00501) => LSC_loss 0.26, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 94.93, Test_acc 51.38
2025-02-14 22:08:23,592 [podnet.py] => Task 9, Epoch 138/160 (LR 0.00459) => LSC_loss 0.26, Spatial_loss 1.61, Flat_loss 0.17, Train_acc 94.81, Test_acc 52.18
2025-02-14 22:08:25,758 [podnet.py] => Task 9, Epoch 139/160 (LR 0.00419) => LSC_loss 0.26, Spatial_loss 1.65, Flat_loss 0.18, Train_acc 95.28, Test_acc 51.48
2025-02-14 22:08:27,943 [podnet.py] => Task 9, Epoch 140/160 (LR 0.00381) => LSC_loss 0.26, Spatial_loss 1.57, Flat_loss 0.17, Train_acc 95.05, Test_acc 51.84
2025-02-14 22:08:30,119 [podnet.py] => Task 9, Epoch 141/160 (LR 0.00344) => LSC_loss 0.26, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 95.35, Test_acc 51.56
2025-02-14 22:08:32,219 [podnet.py] => Task 9, Epoch 142/160 (LR 0.00309) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 95.21, Test_acc 51.99
2025-02-14 22:08:34,389 [podnet.py] => Task 9, Epoch 143/160 (LR 0.00276) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.21, Test_acc 51.74
2025-02-14 22:08:36,525 [podnet.py] => Task 9, Epoch 144/160 (LR 0.00245) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 95.05, Test_acc 51.52
2025-02-14 22:08:38,759 [podnet.py] => Task 9, Epoch 145/160 (LR 0.00215) => LSC_loss 0.25, Spatial_loss 1.56, Flat_loss 0.17, Train_acc 95.44, Test_acc 51.47
2025-02-14 22:08:40,904 [podnet.py] => Task 9, Epoch 146/160 (LR 0.00188) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.16, Train_acc 95.44, Test_acc 51.85
2025-02-14 22:08:43,093 [podnet.py] => Task 9, Epoch 147/160 (LR 0.00162) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.37, Test_acc 51.82
2025-02-14 22:08:45,293 [podnet.py] => Task 9, Epoch 148/160 (LR 0.00138) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.16, Train_acc 95.58, Test_acc 51.93
2025-02-14 22:08:47,447 [podnet.py] => Task 9, Epoch 149/160 (LR 0.00116) => LSC_loss 0.26, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.51, Test_acc 51.68
2025-02-14 22:08:49,615 [podnet.py] => Task 9, Epoch 150/160 (LR 0.00096) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.19, Test_acc 52.05
2025-02-14 22:08:51,748 [podnet.py] => Task 9, Epoch 151/160 (LR 0.00078) => LSC_loss 0.25, Spatial_loss 1.51, Flat_loss 0.16, Train_acc 95.44, Test_acc 51.84
2025-02-14 22:08:53,903 [podnet.py] => Task 9, Epoch 152/160 (LR 0.00062) => LSC_loss 0.25, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 95.51, Test_acc 51.97
2025-02-14 22:08:56,125 [podnet.py] => Task 9, Epoch 153/160 (LR 0.00047) => LSC_loss 0.26, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 95.26, Test_acc 51.73
2025-02-14 22:08:58,318 [podnet.py] => Task 9, Epoch 154/160 (LR 0.00035) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.37, Test_acc 51.63
2025-02-14 22:09:00,495 [podnet.py] => Task 9, Epoch 155/160 (LR 0.00024) => LSC_loss 0.25, Spatial_loss 1.43, Flat_loss 0.16, Train_acc 95.16, Test_acc 51.58
2025-02-14 22:09:02,678 [podnet.py] => Task 9, Epoch 156/160 (LR 0.00015) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.60, Test_acc 52.23
2025-02-14 22:09:04,851 [podnet.py] => Task 9, Epoch 157/160 (LR 0.00009) => LSC_loss 0.26, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 95.23, Test_acc 51.91
2025-02-14 22:09:07,044 [podnet.py] => Task 9, Epoch 158/160 (LR 0.00004) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 95.02, Test_acc 52.00
2025-02-14 22:09:09,233 [podnet.py] => Task 9, Epoch 159/160 (LR 0.00001) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 95.47, Test_acc 51.71
2025-02-14 22:09:11,479 [podnet.py] => Task 9, Epoch 160/160 (LR 0.00000) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 95.33, Test_acc 51.62
2025-02-14 22:09:11,481 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 22:09:11,481 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:09:44,193 [podnet.py] => The size of finetune dataset: 1900
2025-02-14 22:09:45,816 [podnet.py] => Task 9, Epoch 1/20 (LR 0.00497) => LSC_loss 0.18, Spatial_loss 1.91, Flat_loss 0.15, Train_acc 96.95, Test_acc 51.20
2025-02-14 22:09:47,461 [podnet.py] => Task 9, Epoch 2/20 (LR 0.00488) => LSC_loss 0.13, Spatial_loss 1.55, Flat_loss 0.08, Train_acc 98.37, Test_acc 53.00
2025-02-14 22:09:49,062 [podnet.py] => Task 9, Epoch 3/20 (LR 0.00473) => LSC_loss 0.13, Spatial_loss 1.44, Flat_loss 0.07, Train_acc 98.26, Test_acc 52.92
2025-02-14 22:09:50,639 [podnet.py] => Task 9, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.47, Flat_loss 0.07, Train_acc 98.68, Test_acc 53.24
2025-02-14 22:09:52,257 [podnet.py] => Task 9, Epoch 5/20 (LR 0.00427) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.16, Test_acc 52.92
2025-02-14 22:09:53,834 [podnet.py] => Task 9, Epoch 6/20 (LR 0.00397) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.32, Test_acc 53.14
2025-02-14 22:09:55,456 [podnet.py] => Task 9, Epoch 7/20 (LR 0.00363) => LSC_loss 0.12, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.32, Test_acc 52.76
2025-02-14 22:09:57,073 [podnet.py] => Task 9, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.39
2025-02-14 22:09:58,681 [podnet.py] => Task 9, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 1.39, Flat_loss 0.07, Train_acc 98.47, Test_acc 53.13
2025-02-14 22:10:00,294 [podnet.py] => Task 9, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 1.38, Flat_loss 0.06, Train_acc 98.37, Test_acc 52.96
2025-02-14 22:10:01,890 [podnet.py] => Task 9, Epoch 11/20 (LR 0.00211) => LSC_loss 0.12, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.27
2025-02-14 22:10:03,511 [podnet.py] => Task 9, Epoch 12/20 (LR 0.00173) => LSC_loss 0.12, Spatial_loss 1.40, Flat_loss 0.06, Train_acc 98.68, Test_acc 52.95
2025-02-14 22:10:05,154 [podnet.py] => Task 9, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.09
2025-02-14 22:10:06,776 [podnet.py] => Task 9, Epoch 14/20 (LR 0.00103) => LSC_loss 0.13, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 98.37, Test_acc 53.05
2025-02-14 22:10:08,444 [podnet.py] => Task 9, Epoch 15/20 (LR 0.00073) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.58, Test_acc 53.02
2025-02-14 22:10:10,048 [podnet.py] => Task 9, Epoch 16/20 (LR 0.00048) => LSC_loss 0.13, Spatial_loss 1.37, Flat_loss 0.06, Train_acc 98.32, Test_acc 53.05
2025-02-14 22:10:11,620 [podnet.py] => Task 9, Epoch 17/20 (LR 0.00027) => LSC_loss 0.12, Spatial_loss 1.35, Flat_loss 0.06, Train_acc 98.58, Test_acc 53.06
2025-02-14 22:10:13,330 [podnet.py] => Task 9, Epoch 18/20 (LR 0.00012) => LSC_loss 0.12, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.53, Test_acc 53.05
2025-02-14 22:10:14,960 [podnet.py] => Task 9, Epoch 19/20 (LR 0.00003) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.63, Test_acc 53.35
2025-02-14 22:10:16,670 [podnet.py] => Task 9, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 1.29, Flat_loss 0.06, Train_acc 98.79, Test_acc 53.21
2025-02-14 22:10:16,671 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:10:51,609 [podnet.py] => Exemplar size: 1900
2025-02-14 22:10:51,609 [trainer.py] => CNN: {'total': 53.21, '00-09': 60.6, '10-19': 44.5, '20-29': 62.0, '30-39': 52.1, '40-49': 60.5, '50-59': 39.9, '60-69': 48.4, '70-79': 48.9, '80-89': 58.9, '90-99': 59.4, 'old': 52.87, 'new': 59.4}
2025-02-14 22:10:51,610 [trainer.py] => NME: {'total': 53.65, '00-09': 67.0, '10-19': 49.2, '20-29': 65.5, '30-39': 55.1, '40-49': 61.9, '50-59': 36.7, '60-69': 45.4, '70-79': 45.9, '80-89': 55.2, '90-99': 55.6, 'old': 53.54, 'new': 55.6}
2025-02-14 22:10:51,610 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7, 53.21]
2025-02-14 22:10:51,610 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5, 80.15]
2025-02-14 22:10:51,610 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1, 53.65]
2025-02-14 22:10:51,610 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73, 81.57]

2025-02-14 22:10:51,610 [trainer.py] => Average Accuracy (CNN): 63.358000000000004
2025-02-14 22:10:51,610 [trainer.py] => Average Accuracy (NME): 63.672000000000004
2025-02-14 22:10:51,610 [trainer.py] => All params: 527057
2025-02-14 22:10:51,611 [trainer.py] => Trainable params: 527057
2025-02-14 22:10:51,611 [podnet.py] => Learning on 95-100
2025-02-14 22:10:51,645 [podnet.py] => Adaptive factor: 4.47213595499958
2025-02-14 22:10:53,967 [podnet.py] => Task 10, Epoch 1/160 (LR 0.09999) => LSC_loss 1.74, Spatial_loss 4.19, Flat_loss 0.84, Train_acc 68.27, Test_acc 34.42
2025-02-14 22:10:56,239 [podnet.py] => Task 10, Epoch 2/160 (LR 0.09996) => LSC_loss 0.89, Spatial_loss 4.67, Flat_loss 0.74, Train_acc 76.41, Test_acc 35.37
2025-02-14 22:10:58,418 [podnet.py] => Task 10, Epoch 3/160 (LR 0.09991) => LSC_loss 0.78, Spatial_loss 4.48, Flat_loss 0.68, Train_acc 78.27, Test_acc 35.87
2025-02-14 22:11:00,684 [podnet.py] => Task 10, Epoch 4/160 (LR 0.09985) => LSC_loss 0.64, Spatial_loss 4.16, Flat_loss 0.56, Train_acc 83.23, Test_acc 41.88
2025-02-14 22:11:02,939 [podnet.py] => Task 10, Epoch 5/160 (LR 0.09976) => LSC_loss 0.56, Spatial_loss 3.82, Flat_loss 0.49, Train_acc 84.98, Test_acc 36.33
2025-02-14 22:11:05,187 [podnet.py] => Task 10, Epoch 6/160 (LR 0.09965) => LSC_loss 0.53, Spatial_loss 3.71, Flat_loss 0.45, Train_acc 86.14, Test_acc 41.55
2025-02-14 22:11:07,390 [podnet.py] => Task 10, Epoch 7/160 (LR 0.09953) => LSC_loss 0.51, Spatial_loss 3.69, Flat_loss 0.44, Train_acc 87.23, Test_acc 41.85
2025-02-14 22:11:09,539 [podnet.py] => Task 10, Epoch 8/160 (LR 0.09938) => LSC_loss 0.50, Spatial_loss 3.66, Flat_loss 0.43, Train_acc 86.98, Test_acc 39.85
2025-02-14 22:11:11,710 [podnet.py] => Task 10, Epoch 9/160 (LR 0.09922) => LSC_loss 0.50, Spatial_loss 3.58, Flat_loss 0.41, Train_acc 87.36, Test_acc 44.21
2025-02-14 22:11:14,020 [podnet.py] => Task 10, Epoch 10/160 (LR 0.09904) => LSC_loss 0.45, Spatial_loss 3.46, Flat_loss 0.39, Train_acc 89.02, Test_acc 44.26
2025-02-14 22:11:16,225 [podnet.py] => Task 10, Epoch 11/160 (LR 0.09884) => LSC_loss 0.45, Spatial_loss 3.42, Flat_loss 0.38, Train_acc 88.84, Test_acc 46.50
2025-02-14 22:11:18,388 [podnet.py] => Task 10, Epoch 12/160 (LR 0.09862) => LSC_loss 0.45, Spatial_loss 3.51, Flat_loss 0.38, Train_acc 88.84, Test_acc 40.46
2025-02-14 22:11:20,556 [podnet.py] => Task 10, Epoch 13/160 (LR 0.09838) => LSC_loss 0.41, Spatial_loss 3.29, Flat_loss 0.36, Train_acc 90.25, Test_acc 44.98
2025-02-14 22:11:22,774 [podnet.py] => Task 10, Epoch 14/160 (LR 0.09812) => LSC_loss 0.44, Spatial_loss 3.35, Flat_loss 0.37, Train_acc 89.11, Test_acc 43.38
2025-02-14 22:11:24,985 [podnet.py] => Task 10, Epoch 15/160 (LR 0.09785) => LSC_loss 0.45, Spatial_loss 3.41, Flat_loss 0.37, Train_acc 89.16, Test_acc 42.15
2025-02-14 22:11:27,203 [podnet.py] => Task 10, Epoch 16/160 (LR 0.09755) => LSC_loss 0.42, Spatial_loss 3.35, Flat_loss 0.36, Train_acc 90.36, Test_acc 37.40
2025-02-14 22:11:29,430 [podnet.py] => Task 10, Epoch 17/160 (LR 0.09724) => LSC_loss 0.42, Spatial_loss 3.37, Flat_loss 0.37, Train_acc 89.66, Test_acc 37.82
2025-02-14 22:11:31,686 [podnet.py] => Task 10, Epoch 18/160 (LR 0.09691) => LSC_loss 0.42, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 90.02, Test_acc 44.82
2025-02-14 22:11:33,869 [podnet.py] => Task 10, Epoch 19/160 (LR 0.09656) => LSC_loss 0.41, Spatial_loss 3.34, Flat_loss 0.36, Train_acc 89.95, Test_acc 41.55
2025-02-14 22:11:36,081 [podnet.py] => Task 10, Epoch 20/160 (LR 0.09619) => LSC_loss 0.42, Spatial_loss 3.34, Flat_loss 0.36, Train_acc 89.91, Test_acc 40.54
2025-02-14 22:11:38,300 [podnet.py] => Task 10, Epoch 21/160 (LR 0.09581) => LSC_loss 0.40, Spatial_loss 3.26, Flat_loss 0.37, Train_acc 90.52, Test_acc 45.27
2025-02-14 22:11:40,504 [podnet.py] => Task 10, Epoch 22/160 (LR 0.09541) => LSC_loss 0.40, Spatial_loss 3.23, Flat_loss 0.35, Train_acc 90.11, Test_acc 42.16
2025-02-14 22:11:42,726 [podnet.py] => Task 10, Epoch 23/160 (LR 0.09499) => LSC_loss 0.39, Spatial_loss 3.26, Flat_loss 0.35, Train_acc 90.18, Test_acc 41.69
2025-02-14 22:11:44,973 [podnet.py] => Task 10, Epoch 24/160 (LR 0.09455) => LSC_loss 0.40, Spatial_loss 3.26, Flat_loss 0.36, Train_acc 90.34, Test_acc 45.09
2025-02-14 22:11:47,168 [podnet.py] => Task 10, Epoch 25/160 (LR 0.09410) => LSC_loss 0.40, Spatial_loss 3.21, Flat_loss 0.35, Train_acc 90.25, Test_acc 40.26
2025-02-14 22:11:49,397 [podnet.py] => Task 10, Epoch 26/160 (LR 0.09362) => LSC_loss 0.42, Spatial_loss 3.40, Flat_loss 0.37, Train_acc 89.36, Test_acc 41.67
2025-02-14 22:11:51,638 [podnet.py] => Task 10, Epoch 27/160 (LR 0.09314) => LSC_loss 0.40, Spatial_loss 3.27, Flat_loss 0.35, Train_acc 90.34, Test_acc 41.84
2025-02-14 22:11:53,862 [podnet.py] => Task 10, Epoch 28/160 (LR 0.09263) => LSC_loss 0.39, Spatial_loss 3.24, Flat_loss 0.35, Train_acc 90.57, Test_acc 43.09
2025-02-14 22:11:56,137 [podnet.py] => Task 10, Epoch 29/160 (LR 0.09211) => LSC_loss 0.38, Spatial_loss 3.18, Flat_loss 0.34, Train_acc 90.86, Test_acc 38.88
2025-02-14 22:11:58,383 [podnet.py] => Task 10, Epoch 30/160 (LR 0.09157) => LSC_loss 0.36, Spatial_loss 3.01, Flat_loss 0.32, Train_acc 91.36, Test_acc 42.06
2025-02-14 22:12:00,637 [podnet.py] => Task 10, Epoch 31/160 (LR 0.09102) => LSC_loss 0.36, Spatial_loss 3.08, Flat_loss 0.32, Train_acc 91.50, Test_acc 42.24
2025-02-14 22:12:02,807 [podnet.py] => Task 10, Epoch 32/160 (LR 0.09045) => LSC_loss 0.36, Spatial_loss 3.07, Flat_loss 0.32, Train_acc 91.43, Test_acc 43.99
2025-02-14 22:12:05,067 [podnet.py] => Task 10, Epoch 33/160 (LR 0.08987) => LSC_loss 0.38, Spatial_loss 3.16, Flat_loss 0.33, Train_acc 90.84, Test_acc 41.99
2025-02-14 22:12:07,324 [podnet.py] => Task 10, Epoch 34/160 (LR 0.08927) => LSC_loss 0.38, Spatial_loss 3.10, Flat_loss 0.34, Train_acc 90.84, Test_acc 45.45
2025-02-14 22:12:09,591 [podnet.py] => Task 10, Epoch 35/160 (LR 0.08865) => LSC_loss 0.36, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 91.59, Test_acc 47.53
2025-02-14 22:12:11,820 [podnet.py] => Task 10, Epoch 36/160 (LR 0.08802) => LSC_loss 0.37, Spatial_loss 3.05, Flat_loss 0.32, Train_acc 91.32, Test_acc 44.47
2025-02-14 22:12:14,047 [podnet.py] => Task 10, Epoch 37/160 (LR 0.08738) => LSC_loss 0.35, Spatial_loss 3.01, Flat_loss 0.33, Train_acc 91.73, Test_acc 45.10
2025-02-14 22:12:16,267 [podnet.py] => Task 10, Epoch 38/160 (LR 0.08672) => LSC_loss 0.36, Spatial_loss 3.04, Flat_loss 0.32, Train_acc 91.82, Test_acc 42.99
2025-02-14 22:12:18,460 [podnet.py] => Task 10, Epoch 39/160 (LR 0.08604) => LSC_loss 0.37, Spatial_loss 3.09, Flat_loss 0.32, Train_acc 91.30, Test_acc 40.81
2025-02-14 22:12:20,670 [podnet.py] => Task 10, Epoch 40/160 (LR 0.08536) => LSC_loss 0.36, Spatial_loss 3.10, Flat_loss 0.33, Train_acc 91.07, Test_acc 44.93
2025-02-14 22:12:22,863 [podnet.py] => Task 10, Epoch 41/160 (LR 0.08465) => LSC_loss 0.35, Spatial_loss 3.07, Flat_loss 0.32, Train_acc 91.86, Test_acc 42.71
2025-02-14 22:12:25,065 [podnet.py] => Task 10, Epoch 42/160 (LR 0.08394) => LSC_loss 0.34, Spatial_loss 3.00, Flat_loss 0.31, Train_acc 92.39, Test_acc 43.84
2025-02-14 22:12:27,321 [podnet.py] => Task 10, Epoch 43/160 (LR 0.08321) => LSC_loss 0.33, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 93.16, Test_acc 44.37
2025-02-14 22:12:29,558 [podnet.py] => Task 10, Epoch 44/160 (LR 0.08247) => LSC_loss 0.35, Spatial_loss 2.96, Flat_loss 0.30, Train_acc 92.00, Test_acc 40.63
2025-02-14 22:12:31,827 [podnet.py] => Task 10, Epoch 45/160 (LR 0.08172) => LSC_loss 0.34, Spatial_loss 2.98, Flat_loss 0.31, Train_acc 92.36, Test_acc 43.13
2025-02-14 22:12:34,053 [podnet.py] => Task 10, Epoch 46/160 (LR 0.08095) => LSC_loss 0.36, Spatial_loss 3.03, Flat_loss 0.32, Train_acc 91.59, Test_acc 44.74
2025-02-14 22:12:36,235 [podnet.py] => Task 10, Epoch 47/160 (LR 0.08018) => LSC_loss 0.36, Spatial_loss 3.00, Flat_loss 0.32, Train_acc 91.50, Test_acc 45.77
2025-02-14 22:12:38,489 [podnet.py] => Task 10, Epoch 48/160 (LR 0.07939) => LSC_loss 0.36, Spatial_loss 2.98, Flat_loss 0.32, Train_acc 91.34, Test_acc 44.00
2025-02-14 22:12:40,763 [podnet.py] => Task 10, Epoch 49/160 (LR 0.07859) => LSC_loss 0.35, Spatial_loss 2.93, Flat_loss 0.31, Train_acc 92.02, Test_acc 43.79
2025-02-14 22:12:43,028 [podnet.py] => Task 10, Epoch 50/160 (LR 0.07778) => LSC_loss 0.36, Spatial_loss 2.96, Flat_loss 0.31, Train_acc 91.27, Test_acc 40.46
2025-02-14 22:12:45,250 [podnet.py] => Task 10, Epoch 51/160 (LR 0.07696) => LSC_loss 0.36, Spatial_loss 3.07, Flat_loss 0.33, Train_acc 91.18, Test_acc 45.47
2025-02-14 22:12:47,531 [podnet.py] => Task 10, Epoch 52/160 (LR 0.07612) => LSC_loss 0.33, Spatial_loss 3.03, Flat_loss 0.30, Train_acc 92.64, Test_acc 44.24
2025-02-14 22:12:49,715 [podnet.py] => Task 10, Epoch 53/160 (LR 0.07528) => LSC_loss 0.34, Spatial_loss 2.98, Flat_loss 0.30, Train_acc 91.64, Test_acc 46.82
2025-02-14 22:12:51,898 [podnet.py] => Task 10, Epoch 54/160 (LR 0.07443) => LSC_loss 0.35, Spatial_loss 2.97, Flat_loss 0.31, Train_acc 91.52, Test_acc 42.57
2025-02-14 22:12:54,098 [podnet.py] => Task 10, Epoch 55/160 (LR 0.07357) => LSC_loss 0.35, Spatial_loss 2.91, Flat_loss 0.31, Train_acc 92.09, Test_acc 44.74
2025-02-14 22:12:56,301 [podnet.py] => Task 10, Epoch 56/160 (LR 0.07270) => LSC_loss 0.34, Spatial_loss 2.90, Flat_loss 0.30, Train_acc 92.05, Test_acc 43.15
2025-02-14 22:12:58,489 [podnet.py] => Task 10, Epoch 57/160 (LR 0.07182) => LSC_loss 0.33, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 92.52, Test_acc 46.80
2025-02-14 22:13:00,776 [podnet.py] => Task 10, Epoch 58/160 (LR 0.07093) => LSC_loss 0.34, Spatial_loss 2.95, Flat_loss 0.29, Train_acc 92.16, Test_acc 43.42
2025-02-14 22:13:03,034 [podnet.py] => Task 10, Epoch 59/160 (LR 0.07004) => LSC_loss 0.31, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.84, Test_acc 40.49
2025-02-14 22:13:05,225 [podnet.py] => Task 10, Epoch 60/160 (LR 0.06913) => LSC_loss 0.33, Spatial_loss 2.84, Flat_loss 0.30, Train_acc 92.23, Test_acc 43.95
2025-02-14 22:13:07,442 [podnet.py] => Task 10, Epoch 61/160 (LR 0.06822) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 93.14, Test_acc 47.12
2025-02-14 22:13:09,728 [podnet.py] => Task 10, Epoch 62/160 (LR 0.06731) => LSC_loss 0.33, Spatial_loss 2.74, Flat_loss 0.28, Train_acc 92.68, Test_acc 45.90
2025-02-14 22:13:11,938 [podnet.py] => Task 10, Epoch 63/160 (LR 0.06638) => LSC_loss 0.32, Spatial_loss 2.82, Flat_loss 0.29, Train_acc 92.93, Test_acc 44.37
2025-02-14 22:13:14,114 [podnet.py] => Task 10, Epoch 64/160 (LR 0.06545) => LSC_loss 0.34, Spatial_loss 2.81, Flat_loss 0.29, Train_acc 92.00, Test_acc 43.75
2025-02-14 22:13:16,374 [podnet.py] => Task 10, Epoch 65/160 (LR 0.06451) => LSC_loss 0.31, Spatial_loss 2.73, Flat_loss 0.27, Train_acc 93.20, Test_acc 44.11
2025-02-14 22:13:18,655 [podnet.py] => Task 10, Epoch 66/160 (LR 0.06357) => LSC_loss 0.33, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.39, Test_acc 46.29
2025-02-14 22:13:20,876 [podnet.py] => Task 10, Epoch 67/160 (LR 0.06262) => LSC_loss 0.31, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 92.86, Test_acc 46.93
2025-02-14 22:13:23,126 [podnet.py] => Task 10, Epoch 68/160 (LR 0.06167) => LSC_loss 0.32, Spatial_loss 2.72, Flat_loss 0.28, Train_acc 92.73, Test_acc 44.73
2025-02-14 22:13:25,415 [podnet.py] => Task 10, Epoch 69/160 (LR 0.06072) => LSC_loss 0.33, Spatial_loss 2.75, Flat_loss 0.28, Train_acc 92.98, Test_acc 46.16
2025-02-14 22:13:27,675 [podnet.py] => Task 10, Epoch 70/160 (LR 0.05975) => LSC_loss 0.31, Spatial_loss 2.68, Flat_loss 0.27, Train_acc 93.30, Test_acc 44.79
2025-02-14 22:13:29,898 [podnet.py] => Task 10, Epoch 71/160 (LR 0.05879) => LSC_loss 0.33, Spatial_loss 2.82, Flat_loss 0.28, Train_acc 92.14, Test_acc 46.22
2025-02-14 22:13:32,105 [podnet.py] => Task 10, Epoch 72/160 (LR 0.05782) => LSC_loss 0.31, Spatial_loss 2.74, Flat_loss 0.27, Train_acc 92.84, Test_acc 46.32
2025-02-14 22:13:34,255 [podnet.py] => Task 10, Epoch 73/160 (LR 0.05685) => LSC_loss 0.31, Spatial_loss 2.72, Flat_loss 0.27, Train_acc 92.98, Test_acc 46.52
2025-02-14 22:13:36,551 [podnet.py] => Task 10, Epoch 74/160 (LR 0.05588) => LSC_loss 0.30, Spatial_loss 2.61, Flat_loss 0.26, Train_acc 93.16, Test_acc 46.48
2025-02-14 22:13:38,774 [podnet.py] => Task 10, Epoch 75/160 (LR 0.05490) => LSC_loss 0.31, Spatial_loss 2.65, Flat_loss 0.27, Train_acc 93.05, Test_acc 45.60
2025-02-14 22:13:41,019 [podnet.py] => Task 10, Epoch 76/160 (LR 0.05392) => LSC_loss 0.32, Spatial_loss 2.69, Flat_loss 0.27, Train_acc 92.86, Test_acc 47.60
2025-02-14 22:13:43,183 [podnet.py] => Task 10, Epoch 77/160 (LR 0.05294) => LSC_loss 0.32, Spatial_loss 2.57, Flat_loss 0.25, Train_acc 93.02, Test_acc 46.29
2025-02-14 22:13:45,447 [podnet.py] => Task 10, Epoch 78/160 (LR 0.05196) => LSC_loss 0.32, Spatial_loss 2.60, Flat_loss 0.26, Train_acc 92.57, Test_acc 42.00
2025-02-14 22:13:47,637 [podnet.py] => Task 10, Epoch 79/160 (LR 0.05098) => LSC_loss 0.32, Spatial_loss 2.58, Flat_loss 0.26, Train_acc 92.57, Test_acc 46.32
2025-02-14 22:13:49,853 [podnet.py] => Task 10, Epoch 80/160 (LR 0.05000) => LSC_loss 0.29, Spatial_loss 2.54, Flat_loss 0.25, Train_acc 93.91, Test_acc 45.37
2025-02-14 22:13:52,096 [podnet.py] => Task 10, Epoch 81/160 (LR 0.04902) => LSC_loss 0.29, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 93.55, Test_acc 46.22
2025-02-14 22:13:54,261 [podnet.py] => Task 10, Epoch 82/160 (LR 0.04804) => LSC_loss 0.30, Spatial_loss 2.39, Flat_loss 0.23, Train_acc 93.61, Test_acc 46.89
2025-02-14 22:13:56,461 [podnet.py] => Task 10, Epoch 83/160 (LR 0.04706) => LSC_loss 0.30, Spatial_loss 2.49, Flat_loss 0.24, Train_acc 93.16, Test_acc 44.32
2025-02-14 22:13:58,703 [podnet.py] => Task 10, Epoch 84/160 (LR 0.04608) => LSC_loss 0.29, Spatial_loss 2.41, Flat_loss 0.24, Train_acc 93.68, Test_acc 47.55
2025-02-14 22:14:00,994 [podnet.py] => Task 10, Epoch 85/160 (LR 0.04510) => LSC_loss 0.30, Spatial_loss 2.42, Flat_loss 0.23, Train_acc 93.91, Test_acc 46.12
2025-02-14 22:14:03,236 [podnet.py] => Task 10, Epoch 86/160 (LR 0.04412) => LSC_loss 0.31, Spatial_loss 2.43, Flat_loss 0.24, Train_acc 93.41, Test_acc 44.38
2025-02-14 22:14:05,510 [podnet.py] => Task 10, Epoch 87/160 (LR 0.04315) => LSC_loss 0.30, Spatial_loss 2.43, Flat_loss 0.25, Train_acc 93.43, Test_acc 48.46
2025-02-14 22:14:07,713 [podnet.py] => Task 10, Epoch 88/160 (LR 0.04218) => LSC_loss 0.30, Spatial_loss 2.47, Flat_loss 0.24, Train_acc 93.59, Test_acc 45.85
2025-02-14 22:14:09,970 [podnet.py] => Task 10, Epoch 89/160 (LR 0.04121) => LSC_loss 0.28, Spatial_loss 2.41, Flat_loss 0.23, Train_acc 93.86, Test_acc 46.75
2025-02-14 22:14:12,167 [podnet.py] => Task 10, Epoch 90/160 (LR 0.04025) => LSC_loss 0.30, Spatial_loss 2.42, Flat_loss 0.23, Train_acc 93.59, Test_acc 44.74
2025-02-14 22:14:14,318 [podnet.py] => Task 10, Epoch 91/160 (LR 0.03928) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 93.66, Test_acc 46.72
2025-02-14 22:14:16,568 [podnet.py] => Task 10, Epoch 92/160 (LR 0.03833) => LSC_loss 0.29, Spatial_loss 2.34, Flat_loss 0.23, Train_acc 93.89, Test_acc 47.58
2025-02-14 22:14:18,848 [podnet.py] => Task 10, Epoch 93/160 (LR 0.03738) => LSC_loss 0.29, Spatial_loss 2.28, Flat_loss 0.22, Train_acc 93.86, Test_acc 47.94
2025-02-14 22:14:21,103 [podnet.py] => Task 10, Epoch 94/160 (LR 0.03643) => LSC_loss 0.30, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 93.36, Test_acc 44.17
2025-02-14 22:14:23,343 [podnet.py] => Task 10, Epoch 95/160 (LR 0.03549) => LSC_loss 0.30, Spatial_loss 2.30, Flat_loss 0.23, Train_acc 94.02, Test_acc 47.81
2025-02-14 22:14:25,570 [podnet.py] => Task 10, Epoch 96/160 (LR 0.03455) => LSC_loss 0.28, Spatial_loss 2.29, Flat_loss 0.22, Train_acc 94.09, Test_acc 44.94
2025-02-14 22:14:27,784 [podnet.py] => Task 10, Epoch 97/160 (LR 0.03362) => LSC_loss 0.29, Spatial_loss 2.27, Flat_loss 0.22, Train_acc 93.80, Test_acc 46.94
2025-02-14 22:14:30,040 [podnet.py] => Task 10, Epoch 98/160 (LR 0.03269) => LSC_loss 0.28, Spatial_loss 2.24, Flat_loss 0.22, Train_acc 94.16, Test_acc 48.73
2025-02-14 22:14:32,288 [podnet.py] => Task 10, Epoch 99/160 (LR 0.03178) => LSC_loss 0.27, Spatial_loss 2.14, Flat_loss 0.21, Train_acc 94.48, Test_acc 46.41
2025-02-14 22:14:34,479 [podnet.py] => Task 10, Epoch 100/160 (LR 0.03087) => LSC_loss 0.29, Spatial_loss 2.22, Flat_loss 0.21, Train_acc 93.52, Test_acc 49.43
2025-02-14 22:14:36,694 [podnet.py] => Task 10, Epoch 101/160 (LR 0.02996) => LSC_loss 0.28, Spatial_loss 2.18, Flat_loss 0.21, Train_acc 94.36, Test_acc 47.95
2025-02-14 22:14:38,883 [podnet.py] => Task 10, Epoch 102/160 (LR 0.02907) => LSC_loss 0.29, Spatial_loss 2.13, Flat_loss 0.21, Train_acc 93.77, Test_acc 48.91
2025-02-14 22:14:41,119 [podnet.py] => Task 10, Epoch 103/160 (LR 0.02818) => LSC_loss 0.28, Spatial_loss 2.15, Flat_loss 0.20, Train_acc 94.41, Test_acc 47.19
2025-02-14 22:14:43,360 [podnet.py] => Task 10, Epoch 104/160 (LR 0.02730) => LSC_loss 0.28, Spatial_loss 2.17, Flat_loss 0.21, Train_acc 93.86, Test_acc 48.20
2025-02-14 22:14:45,567 [podnet.py] => Task 10, Epoch 105/160 (LR 0.02643) => LSC_loss 0.28, Spatial_loss 2.03, Flat_loss 0.20, Train_acc 94.14, Test_acc 48.09
2025-02-14 22:14:47,807 [podnet.py] => Task 10, Epoch 106/160 (LR 0.02557) => LSC_loss 0.28, Spatial_loss 2.14, Flat_loss 0.20, Train_acc 93.64, Test_acc 48.42
2025-02-14 22:14:50,042 [podnet.py] => Task 10, Epoch 107/160 (LR 0.02472) => LSC_loss 0.28, Spatial_loss 2.07, Flat_loss 0.20, Train_acc 94.11, Test_acc 48.40
2025-02-14 22:14:52,228 [podnet.py] => Task 10, Epoch 108/160 (LR 0.02388) => LSC_loss 0.27, Spatial_loss 2.04, Flat_loss 0.20, Train_acc 94.84, Test_acc 47.09
2025-02-14 22:14:54,486 [podnet.py] => Task 10, Epoch 109/160 (LR 0.02304) => LSC_loss 0.28, Spatial_loss 2.12, Flat_loss 0.20, Train_acc 93.77, Test_acc 46.98
2025-02-14 22:14:56,739 [podnet.py] => Task 10, Epoch 110/160 (LR 0.02222) => LSC_loss 0.28, Spatial_loss 2.11, Flat_loss 0.20, Train_acc 94.45, Test_acc 48.65
2025-02-14 22:14:58,926 [podnet.py] => Task 10, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 2.01, Flat_loss 0.19, Train_acc 94.09, Test_acc 47.55
2025-02-14 22:15:01,129 [podnet.py] => Task 10, Epoch 112/160 (LR 0.02061) => LSC_loss 0.28, Spatial_loss 1.99, Flat_loss 0.19, Train_acc 94.14, Test_acc 48.74
2025-02-14 22:15:03,352 [podnet.py] => Task 10, Epoch 113/160 (LR 0.01982) => LSC_loss 0.28, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 93.95, Test_acc 47.91
2025-02-14 22:15:05,628 [podnet.py] => Task 10, Epoch 114/160 (LR 0.01905) => LSC_loss 0.30, Spatial_loss 1.93, Flat_loss 0.19, Train_acc 93.59, Test_acc 47.18
2025-02-14 22:15:07,893 [podnet.py] => Task 10, Epoch 115/160 (LR 0.01828) => LSC_loss 0.27, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 94.41, Test_acc 49.42
2025-02-14 22:15:10,082 [podnet.py] => Task 10, Epoch 116/160 (LR 0.01753) => LSC_loss 0.28, Spatial_loss 1.97, Flat_loss 0.19, Train_acc 94.39, Test_acc 50.54
2025-02-14 22:15:12,270 [podnet.py] => Task 10, Epoch 117/160 (LR 0.01679) => LSC_loss 0.28, Spatial_loss 1.91, Flat_loss 0.18, Train_acc 94.11, Test_acc 49.62
2025-02-14 22:15:14,523 [podnet.py] => Task 10, Epoch 118/160 (LR 0.01606) => LSC_loss 0.26, Spatial_loss 1.89, Flat_loss 0.18, Train_acc 94.36, Test_acc 46.88
2025-02-14 22:15:16,741 [podnet.py] => Task 10, Epoch 119/160 (LR 0.01535) => LSC_loss 0.27, Spatial_loss 1.87, Flat_loss 0.18, Train_acc 94.23, Test_acc 48.28
2025-02-14 22:15:18,984 [podnet.py] => Task 10, Epoch 120/160 (LR 0.01464) => LSC_loss 0.28, Spatial_loss 1.90, Flat_loss 0.18, Train_acc 94.18, Test_acc 50.21
2025-02-14 22:15:21,183 [podnet.py] => Task 10, Epoch 121/160 (LR 0.01396) => LSC_loss 0.27, Spatial_loss 1.88, Flat_loss 0.18, Train_acc 94.34, Test_acc 49.33
2025-02-14 22:15:23,406 [podnet.py] => Task 10, Epoch 122/160 (LR 0.01328) => LSC_loss 0.26, Spatial_loss 1.85, Flat_loss 0.18, Train_acc 94.75, Test_acc 49.43
2025-02-14 22:15:25,596 [podnet.py] => Task 10, Epoch 123/160 (LR 0.01262) => LSC_loss 0.27, Spatial_loss 1.77, Flat_loss 0.17, Train_acc 94.70, Test_acc 47.79
2025-02-14 22:15:27,856 [podnet.py] => Task 10, Epoch 124/160 (LR 0.01198) => LSC_loss 0.27, Spatial_loss 1.72, Flat_loss 0.17, Train_acc 94.32, Test_acc 49.10
2025-02-14 22:15:30,058 [podnet.py] => Task 10, Epoch 125/160 (LR 0.01135) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.17, Train_acc 94.36, Test_acc 48.60
2025-02-14 22:15:32,256 [podnet.py] => Task 10, Epoch 126/160 (LR 0.01073) => LSC_loss 0.27, Spatial_loss 1.74, Flat_loss 0.17, Train_acc 94.66, Test_acc 48.89
2025-02-14 22:15:34,523 [podnet.py] => Task 10, Epoch 127/160 (LR 0.01013) => LSC_loss 0.28, Spatial_loss 1.79, Flat_loss 0.17, Train_acc 93.93, Test_acc 48.52
2025-02-14 22:15:36,765 [podnet.py] => Task 10, Epoch 128/160 (LR 0.00955) => LSC_loss 0.27, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 94.55, Test_acc 47.81
2025-02-14 22:15:38,982 [podnet.py] => Task 10, Epoch 129/160 (LR 0.00898) => LSC_loss 0.27, Spatial_loss 1.73, Flat_loss 0.17, Train_acc 93.93, Test_acc 48.51
2025-02-14 22:15:41,230 [podnet.py] => Task 10, Epoch 130/160 (LR 0.00843) => LSC_loss 0.27, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 94.25, Test_acc 48.96
2025-02-14 22:15:43,524 [podnet.py] => Task 10, Epoch 131/160 (LR 0.00789) => LSC_loss 0.27, Spatial_loss 1.76, Flat_loss 0.17, Train_acc 94.84, Test_acc 49.71
2025-02-14 22:15:45,786 [podnet.py] => Task 10, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.71, Flat_loss 0.17, Train_acc 94.59, Test_acc 50.17
2025-02-14 22:15:48,026 [podnet.py] => Task 10, Epoch 133/160 (LR 0.00686) => LSC_loss 0.28, Spatial_loss 1.71, Flat_loss 0.16, Train_acc 93.93, Test_acc 50.17
2025-02-14 22:15:50,275 [podnet.py] => Task 10, Epoch 134/160 (LR 0.00638) => LSC_loss 0.26, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 95.00, Test_acc 49.28
2025-02-14 22:15:52,460 [podnet.py] => Task 10, Epoch 135/160 (LR 0.00590) => LSC_loss 0.27, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 94.98, Test_acc 50.80
2025-02-14 22:15:54,729 [podnet.py] => Task 10, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 95.18, Test_acc 49.07
2025-02-14 22:15:56,925 [podnet.py] => Task 10, Epoch 137/160 (LR 0.00501) => LSC_loss 0.27, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 94.59, Test_acc 49.80
2025-02-14 22:15:59,193 [podnet.py] => Task 10, Epoch 138/160 (LR 0.00459) => LSC_loss 0.26, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 94.64, Test_acc 49.78
2025-02-14 22:16:01,432 [podnet.py] => Task 10, Epoch 139/160 (LR 0.00419) => LSC_loss 0.27, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 94.84, Test_acc 49.87
2025-02-14 22:16:03,662 [podnet.py] => Task 10, Epoch 140/160 (LR 0.00381) => LSC_loss 0.27, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 94.18, Test_acc 49.90
2025-02-14 22:16:05,977 [podnet.py] => Task 10, Epoch 141/160 (LR 0.00344) => LSC_loss 0.27, Spatial_loss 1.57, Flat_loss 0.15, Train_acc 94.36, Test_acc 50.02
2025-02-14 22:16:08,196 [podnet.py] => Task 10, Epoch 142/160 (LR 0.00309) => LSC_loss 0.27, Spatial_loss 1.60, Flat_loss 0.16, Train_acc 94.41, Test_acc 50.24
2025-02-14 22:16:10,378 [podnet.py] => Task 10, Epoch 143/160 (LR 0.00276) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.45, Test_acc 50.36
2025-02-14 22:16:12,591 [podnet.py] => Task 10, Epoch 144/160 (LR 0.00245) => LSC_loss 0.26, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 95.11, Test_acc 50.11
2025-02-14 22:16:14,805 [podnet.py] => Task 10, Epoch 145/160 (LR 0.00215) => LSC_loss 0.27, Spatial_loss 1.54, Flat_loss 0.16, Train_acc 94.61, Test_acc 50.58
2025-02-14 22:16:17,037 [podnet.py] => Task 10, Epoch 146/160 (LR 0.00188) => LSC_loss 0.27, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.50, Test_acc 50.25
2025-02-14 22:16:19,239 [podnet.py] => Task 10, Epoch 147/160 (LR 0.00162) => LSC_loss 0.26, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.93, Test_acc 49.84
2025-02-14 22:16:21,444 [podnet.py] => Task 10, Epoch 148/160 (LR 0.00138) => LSC_loss 0.26, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.66, Test_acc 49.85
2025-02-14 22:16:23,670 [podnet.py] => Task 10, Epoch 149/160 (LR 0.00116) => LSC_loss 0.27, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 94.75, Test_acc 50.07
2025-02-14 22:16:25,911 [podnet.py] => Task 10, Epoch 150/160 (LR 0.00096) => LSC_loss 0.27, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 94.30, Test_acc 49.83
2025-02-14 22:16:28,179 [podnet.py] => Task 10, Epoch 151/160 (LR 0.00078) => LSC_loss 0.26, Spatial_loss 1.47, Flat_loss 0.15, Train_acc 95.11, Test_acc 49.91
2025-02-14 22:16:30,423 [podnet.py] => Task 10, Epoch 152/160 (LR 0.00062) => LSC_loss 0.26, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 94.91, Test_acc 50.23
2025-02-14 22:16:32,716 [podnet.py] => Task 10, Epoch 153/160 (LR 0.00047) => LSC_loss 0.27, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 94.52, Test_acc 49.70
2025-02-14 22:16:34,962 [podnet.py] => Task 10, Epoch 154/160 (LR 0.00035) => LSC_loss 0.26, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 94.80, Test_acc 50.09
2025-02-14 22:16:37,158 [podnet.py] => Task 10, Epoch 155/160 (LR 0.00024) => LSC_loss 0.27, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.52, Test_acc 49.71
2025-02-14 22:16:39,424 [podnet.py] => Task 10, Epoch 156/160 (LR 0.00015) => LSC_loss 0.26, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 95.07, Test_acc 49.68
2025-02-14 22:16:41,624 [podnet.py] => Task 10, Epoch 157/160 (LR 0.00009) => LSC_loss 0.26, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 94.95, Test_acc 50.23
2025-02-14 22:16:43,900 [podnet.py] => Task 10, Epoch 158/160 (LR 0.00004) => LSC_loss 0.26, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 94.89, Test_acc 50.23
2025-02-14 22:16:46,095 [podnet.py] => Task 10, Epoch 159/160 (LR 0.00001) => LSC_loss 0.27, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 94.25, Test_acc 49.95
2025-02-14 22:16:48,359 [podnet.py] => Task 10, Epoch 160/160 (LR 0.00000) => LSC_loss 0.26, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 94.80, Test_acc 49.80
2025-02-14 22:16:48,360 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-02-14 22:16:48,360 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:17:22,712 [podnet.py] => The size of finetune dataset: 2000
2025-02-14 22:17:24,429 [podnet.py] => Task 10, Epoch 1/20 (LR 0.00497) => LSC_loss 0.21, Spatial_loss 1.71, Flat_loss 0.14, Train_acc 96.45, Test_acc 50.14
2025-02-14 22:17:26,064 [podnet.py] => Task 10, Epoch 2/20 (LR 0.00488) => LSC_loss 0.14, Spatial_loss 1.57, Flat_loss 0.08, Train_acc 98.45, Test_acc 51.96
2025-02-14 22:17:27,743 [podnet.py] => Task 10, Epoch 3/20 (LR 0.00473) => LSC_loss 0.13, Spatial_loss 1.44, Flat_loss 0.07, Train_acc 98.80, Test_acc 51.92
2025-02-14 22:17:29,404 [podnet.py] => Task 10, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.07, Train_acc 98.35, Test_acc 51.30
2025-02-14 22:17:31,128 [podnet.py] => Task 10, Epoch 5/20 (LR 0.00427) => LSC_loss 0.13, Spatial_loss 1.46, Flat_loss 0.06, Train_acc 98.05, Test_acc 52.10
2025-02-14 22:17:32,762 [podnet.py] => Task 10, Epoch 6/20 (LR 0.00397) => LSC_loss 0.13, Spatial_loss 1.48, Flat_loss 0.06, Train_acc 98.70, Test_acc 51.94
2025-02-14 22:17:34,463 [podnet.py] => Task 10, Epoch 7/20 (LR 0.00363) => LSC_loss 0.13, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.30, Test_acc 51.72
2025-02-14 22:17:36,151 [podnet.py] => Task 10, Epoch 8/20 (LR 0.00327) => LSC_loss 0.13, Spatial_loss 1.42, Flat_loss 0.07, Train_acc 98.50, Test_acc 52.00
2025-02-14 22:17:37,872 [podnet.py] => Task 10, Epoch 9/20 (LR 0.00289) => LSC_loss 0.13, Spatial_loss 1.43, Flat_loss 0.06, Train_acc 98.60, Test_acc 51.62
2025-02-14 22:17:39,554 [podnet.py] => Task 10, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 1.37, Flat_loss 0.06, Train_acc 98.55, Test_acc 52.08
2025-02-14 22:17:41,233 [podnet.py] => Task 10, Epoch 11/20 (LR 0.00211) => LSC_loss 0.13, Spatial_loss 1.41, Flat_loss 0.06, Train_acc 98.85, Test_acc 51.48
2025-02-14 22:17:42,920 [podnet.py] => Task 10, Epoch 12/20 (LR 0.00173) => LSC_loss 0.13, Spatial_loss 1.36, Flat_loss 0.06, Train_acc 98.30, Test_acc 52.22
2025-02-14 22:17:44,615 [podnet.py] => Task 10, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.90, Test_acc 51.77
2025-02-14 22:17:46,316 [podnet.py] => Task 10, Epoch 14/20 (LR 0.00103) => LSC_loss 0.11, Spatial_loss 1.34, Flat_loss 0.06, Train_acc 98.95, Test_acc 52.17
2025-02-14 22:17:47,965 [podnet.py] => Task 10, Epoch 15/20 (LR 0.00073) => LSC_loss 0.12, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.85, Test_acc 52.07
2025-02-14 22:17:49,639 [podnet.py] => Task 10, Epoch 16/20 (LR 0.00048) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.60, Test_acc 52.16
2025-02-14 22:17:51,320 [podnet.py] => Task 10, Epoch 17/20 (LR 0.00027) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.06, Train_acc 98.80, Test_acc 52.16
2025-02-14 22:17:52,953 [podnet.py] => Task 10, Epoch 18/20 (LR 0.00012) => LSC_loss 0.13, Spatial_loss 1.33, Flat_loss 0.06, Train_acc 98.20, Test_acc 52.06
2025-02-14 22:17:54,666 [podnet.py] => Task 10, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 1.31, Flat_loss 0.05, Train_acc 98.45, Test_acc 52.15
2025-02-14 22:17:56,311 [podnet.py] => Task 10, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 1.28, Flat_loss 0.05, Train_acc 98.55, Test_acc 52.11
2025-02-14 22:17:56,312 [base.py] => Constructing exemplars for new classes...(20 per classes)
2025-02-14 22:18:32,768 [podnet.py] => Exemplar size: 2000
2025-02-14 22:18:32,769 [trainer.py] => CNN: {'total': 52.11, '00-09': 59.3, '10-19': 43.9, '20-29': 60.6, '30-39': 49.7, '40-49': 59.5, '50-59': 38.6, '60-69': 47.6, '70-79': 46.8, '80-89': 56.7, '90-99': 58.4, 'old': 51.58, 'new': 62.2}
2025-02-14 22:18:32,769 [trainer.py] => NME: {'total': 52.55, '00-09': 66.0, '10-19': 48.8, '20-29': 63.6, '30-39': 53.6, '40-49': 61.0, '50-59': 35.6, '60-69': 45.3, '70-79': 44.6, '80-89': 53.5, '90-99': 53.5, 'old': 52.35, 'new': 56.4}
2025-02-14 22:18:32,769 [trainer.py] => CNN top1 curve: [77.7, 73.69, 69.58, 66.06, 63.51, 60.75, 58.02, 56.36, 54.7, 53.21, 52.11]
2025-02-14 22:18:32,769 [trainer.py] => CNN top5 curve: [94.08, 93.33, 91.27, 88.49, 86.67, 84.55, 83.08, 83.08, 81.5, 80.15, 79.14]
2025-02-14 22:18:32,769 [trainer.py] => NME top1 curve: [77.44, 73.36, 69.3, 66.14, 64.09, 61.48, 59.02, 57.14, 55.1, 53.65, 52.55]
2025-02-14 22:18:32,769 [trainer.py] => NME top5 curve: [93.98, 93.18, 91.18, 89.22, 88.2, 86.52, 85.16, 84.48, 82.73, 81.57, 80.19]

2025-02-14 22:18:32,769 [trainer.py] => Average Accuracy (CNN): 62.33545454545455
2025-02-14 22:18:32,769 [trainer.py] => Average Accuracy (NME): 62.66090909090909
